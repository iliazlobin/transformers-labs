{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video LLaVA\n",
    "\n",
    "VQA with a video of making coffee\n",
    "\n",
    "Model Card:\n",
    "* https://huggingface.co/LanguageBind/Video-LLaVA-7B-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "!pip install av\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0a01958c1b4a26be8855579c766448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "from transformers import VideoLlavaProcessor, VideoLlavaForConditionalGeneration\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", device_map=0)\n",
    "processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", device_map=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making-coffee_low-quality.mp4, the lowest quality, 2Mb, 352x240, 503kbps, 30fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "video_path = \"videos/making-coffee_low-quality.mp4\"\n",
    "container = av.open(video_path)\n",
    "\n",
    "# sample uniformly 8 frames from the video\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip = read_video_pyav(container, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Describe the process in the video in detail. ASSISTANT: In the video, a person is seen pouring milk into a cup using a coffee maker. The person starts by pouring milk into the cup, and then proceeds to add coffee to the cup. The person then uses a spoon to stir the mixture, ensuring that the coffee and milk are well combined. The cup is then placed on a countertop, and the person takes a sip of the beverage. The video showcases the process of making a coffee and milk drink, highlighting the steps involved in preparing the beverage. The person in the video demonstrates the importance of stirring the mixture to ensure that the coffee and milk are well combined, resulting in a smooth and enjoyable drink.\n",
      "total/used/cuda/res/ram (Gb): 79.15/36.12/27.46/34.76/5.71\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <video>Describe the process in the video in detail. ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# generate_ids = model.generate(**inputs, max_length=80)\n",
    "generate_ids = model.generate(**inputs, max_length=800)\n",
    "# generate_ids = model.generate(**inputs, max_length=8000)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making-coffee_20.mp4, higher quality, 20Mb, 720x480, 5273kbps, 15fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "video_path = \"videos/making-coffee_20.mp4\"\n",
    "container = av.open(video_path)\n",
    "\n",
    "# sample uniformly 8 frames from the video\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip = read_video_pyav(container, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What the person did after turning on a coffee machine? ASSISTANT: After turning on the coffee machine, the person poured milk into a cup.Ъ\n",
      "total/used/cuda/res/ram (Gb): 79.15/34.23/27.46/32.87/6.05\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <video>What the person did after turning on a coffee machine? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# generate_ids = model.generate(**inputs, max_length=80)\n",
    "generate_ids = model.generate(**inputs, max_length=800)\n",
    "# generate_ids = model.generate(**inputs, max_length=8000)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "# >>> 'USER:  Why is this video funny? ASSISTANT: The video is funny because the baby is sitting on the bed and reading a book, which is an unusual and amusing sight.'\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Does a person take a sip of beferage and when? ASSISTANT: Yes, a person takes a sip of beverage from the cup after the coffee is done brewing.Ъ\n",
      "total/used/cuda/res/ram (Gb): 79.15/34.23/27.46/32.87/6.01\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <video>Does a person take a sip of beferage and when? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# generate_ids = model.generate(**inputs, max_length=80)\n",
    "generate_ids = model.generate(**inputs, max_length=800)\n",
    "# generate_ids = model.generate(**inputs, max_length=8000)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "# >>> 'USER:  Why is this video funny? ASSISTANT: The video is funny because the baby is sitting on the bed and reading a book, which is an unusual and amusing sight.'\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What was the boiler's pressure during the procedure? ASSISTANT: The boiler's pressure was 1 bar during the procedure.Ъ\n",
      "total/used/cuda/res/ram (Gb): 79.15/34.23/27.46/32.87/6.03\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <video>What was the boiler's pressure during the procedure? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# generate_ids = model.generate(**inputs, max_length=80)\n",
    "generate_ids = model.generate(**inputs, max_length=800)\n",
    "# generate_ids = model.generate(**inputs, max_length=8000)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "# >>> 'USER:  Why is this video funny? ASSISTANT: The video is funny because the baby is sitting on the bed and reading a book, which is an unusual and amusing sight.'\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What was the number on a display of the coffee machine the camera has captured? ASSISTANT: The camera captured a display of the coffee machine that shows the number 100.Ъ\n",
      "total/used/cuda/res/ram (Gb): 79.15/34.23/27.46/32.87/6.09\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <video>What was the number on a display of the coffee machine the camera has captured? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# generate_ids = model.generate(**inputs, max_length=80)\n",
    "generate_ids = model.generate(**inputs, max_length=800)\n",
    "# generate_ids = model.generate(**inputs, max_length=8000)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "# >>> 'USER:  Why is this video funny? ASSISTANT: The video is funny because the baby is sitting on the bed and reading a book, which is an unusual and amusing sight.'\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What is the brand of coffee machine and the model name? ASSISTANT: The brand of the coffee machine is Breville, and the model name is BES870XL.Ъ\n",
      "total/used/cuda/res/ram (Gb): 79.15/34.23/27.46/32.87/6.08\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <video>What is the brand of coffee machine and the model name? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# generate_ids = model.generate(**inputs, max_length=80)\n",
    "generate_ids = model.generate(**inputs, max_length=800)\n",
    "# generate_ids = model.generate(**inputs, max_length=8000)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "# >>> 'USER:  Why is this video funny? ASSISTANT: The video is funny because the baby is sitting on the bed and reading a book, which is an unusual and amusing sight.'\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: How much does this coffee machine cost? ASSISTANT: The coffee machine in the video is a Breville coffee machine, and it costs $199.Ъ\n",
      "total/used/cuda/res/ram (Gb): 79.15/34.23/27.46/32.87/6.16\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <video>How much does this coffee machine cost? ASSISTANT:\"\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# generate_ids = model.generate(**inputs, max_length=80)\n",
    "generate_ids = model.generate(**inputs, max_length=800)\n",
    "# generate_ids = model.generate(**inputs, max_length=8000)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "# >>> 'USER:  Why is this video funny? ASSISTANT: The video is funny because the baby is sitting on the bed and reading a book, which is an unusual and amusing sight.'\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
