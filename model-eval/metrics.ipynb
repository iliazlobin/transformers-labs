{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install transformers evaluate\n",
    "%pip install nltk absl-py rouge_score\n",
    "%pip install bleu sacrebleu\n",
    "%pip install sacremoses\n",
    "%pip install scipy\n",
    "%pip install sentencepiece\n",
    "%pip install optimum auto-gptq\n",
    "%pip install scikit-learn\n",
    "%pip install einops\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading custom gpt2 / gpt2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large,model_id: openai-community/gpt2-large,model_path: openai-community_gpt2-large\n",
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "trained_model_name: gpt2-large-bnb8-coedit,trained_model_id: iliazlobin/gpt2-large-bnb8-coedit,trained_model_path: iliazlobin_gpt2-large-bnb8-coedit\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"openai-community/gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 1280)\n",
      "        (wpe): Embedding(1024, 1280)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-35): 36 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=3840, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "model_name: gpt2-large-bnb8-coedit,model_id: iliazlobin/gpt2-large-bnb8-coedit,model_path: iliazlobin_gpt2-large-bnb8-coedit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "model_repo = f\"openai-community\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "# print(model.config)\n",
    "# print(model)\n",
    "\n",
    "trained_model_name = \"gpt2-large-bnb8-coedit\"\n",
    "trained_model_repo = f\"iliazlobin\"\n",
    "trained_model_id = f\"{trained_model_repo}/{trained_model_name}\"\n",
    "trained_model_checkpoint = f\"{trained_model_repo}/{trained_model_name}\"\n",
    "trained_model_path = f\"{trained_model_repo}_{trained_model_name}\"\n",
    "print(\n",
    "    f\"trained_model_name: {trained_model_name},\"\n",
    "    f\"trained_model_id: {trained_model_id},\"\n",
    "    f\"trained_model_path: {trained_model_path}\"\n",
    ")\n",
    "\n",
    "\n",
    "adapters_path = f\"../model-train/model-{trained_model_repo}_{trained_model_name}\"\n",
    "peft_model = PeftModel.from_pretrained(model, adapters_path)\n",
    "\n",
    "print(type(peft_model))\n",
    "print(peft_model.config)\n",
    "print(peft_model)\n",
    "\n",
    "origin_model = model\n",
    "model = peft_model\n",
    "model_name = trained_model_name\n",
    "model_repo = trained_model_repo\n",
    "model_id = trained_model_id\n",
    "model_checkpoint = trained_model_checkpoint\n",
    "model_path = trained_model_path\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BART\n",
    "* https://huggingface.co/facebook/bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# model = BartModel.from_pretrained(model_name)\n",
    "# model = BartForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "BartConfig {\n",
      "  \"_name_or_path\": \"iliazlobin/bart-grammarly\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"iliazlobin/bart-grammarly\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# model = BartModel.from_pretrained(model_name)\n",
    "# model = BartForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading t5\n",
    "* https://huggingface.co/google-t5/t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name, model_max_length=512)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading coedit / flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading grammarly/coedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"grammarly/coedit-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"grammarly/coedit-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/gemma-2b-it\"\n",
    "# model_name = \"google/gemma-7b-it\"\n",
    "# model_name = \"google/gemma-7b\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # revision=\"float16\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Phi-2\n",
    "* https://huggingface.co/microsoft/phi-2\n",
    "* https://huggingface.co/TheBloke/phi-2-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token: <|endoftext|>\n",
      "PhiConfig {\n",
      "  \"_name_or_path\": \"TheBloke/phi-2-GPTQ\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"TheBloke/phi-2-GPTQ--configuration_phi.PhiConfig\",\n",
      "    \"AutoModelForCausalLM\": \"TheBloke/phi-2-GPTQ--modeling_phi.PhiForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"flash_attn\": false,\n",
      "  \"flash_rotary\": false,\n",
      "  \"fused_dense\": false,\n",
      "  \"img_processor\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"phi-msft\",\n",
      "  \"n_embd\": 2560,\n",
      "  \"n_head\": 32,\n",
      "  \"n_head_kv\": null,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 32,\n",
      "  \"n_positions\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": true,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'microsoft/phi-2'\n",
    "model_name = 'TheBloke/phi-2-GPTQ'\n",
    "model_alias = model_name.replace('/', '_')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\n",
    "# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # revision=\"float16\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading mixtral-8x7B\n",
    "* https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "* https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ\n",
    "\n",
    "Quantization\n",
    "* https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from transformers import BitsAndBytesConfig\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     29\u001b[0m     model_name,\n\u001b[1;32m     30\u001b[0m     use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# tokenizer.pad_token = tokenizer.eos_token\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3039\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3039\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3043\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/quantizers/auto.py:153\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m AutoQuantizationConfig\u001b[38;5;241m.\u001b[39mfrom_dict(quantization_config)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig)) \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ config collision\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m \u001b[43mquantization_config_from_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loading_attributes\u001b[49m()\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m loading_attr_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(quantization_config, attr, val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'"
     ]
    }
   ],
   "source": [
    "# model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "model_name = 'TheBloke/Mixtral-8x7B-v0.1-GPTQ'\n",
    "model_alias = model_name.replace('/', '_')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=0,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # load_in_4bit=True,\n",
    "#     # quantization_config=quantization_config,\n",
    "# )\n",
    "\n",
    "# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\n",
    "# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# text = 'Hello my name is'\n",
    "# inputs = tokenizer(text, return_tensors='pt')\n",
    "# outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"TheBloke/Llama-2-7B-GPTQ\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": false,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_name = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)\n",
    "\n",
    "# from auto_gptq import exllama_set_max_input_length\n",
    "# model = exllama_set_max_input_length(model, max_input_length=2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring memory (VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total/trainable params: 785826560/0\n",
      "{'total_memory': 10736893952, 'memory_used': 5864493056, 'cuda_allocated': 3257204736, 'cuda_reserved': 3391094784, 'ram_usage': 15431069696}\n",
      "{'total_memory': '10.00', 'memory_used': '5.46', 'cuda_allocated': '3.03', 'cuda_reserved': '3.16', 'ram_usage': '14.37'}\n",
      "total/used/cuda/res/ram(Gb): 10.00/5.46/3.03/3.16/14.37\n",
      "Total/used/available memory (Gb): 10.00/{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\n",
      "Recommended/actual fraction: 0.45/0.90\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total/trainable params: {total_params}/{total_trainable_params}')\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "print(utilization)\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(utilization_str)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram(Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "actual_fraction = 0.90\n",
    "available_memory = utilization['total_memory'] - utilization['memory_used']\n",
    "recommended_fraction = available_memory / utilization['total_memory']\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"Total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    \"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f'Recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}')\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.empty(utilization['total_memory'] // 2, dtype=torch.int8, device='cuda')\n",
    "# print_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs.\n"
     ]
    }
   ],
   "source": [
    "# input_text = \"Fix grammatical errors in this sentence: I goes to school every day.\"\n",
    "input_text = \"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\"\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "# outputs = model.generate(\n",
    "#     **input_ids,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=False,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
      "          1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
      "           430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
      "          1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
      "           198, 31077,    25],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 22743, 23491,    25,   679,  3708,   257,  4998,  1097,    13,\n",
      "           198, 31077,    25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "[' Countries with a lot of deserts can terraform their deserts to increase their habitable land and using irrigation to provide clean water to the desert.', ' He drives an amazing car.']\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\n",
    "    f\"\"\"Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "Response:\n",
    "\"\"\".strip(),\n",
    "    f\"\"\"Fix grammar: He drive a amazing car.\n",
    "Response:\n",
    "\"\"\".strip(),\n",
    "]\n",
    "# input_texts = [\n",
    "#     \"Fix grammar:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\",\n",
    "#     \"Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian Hall, still a vibrant community center/beer garden started by Czech immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, continuously occupied Latin music store in New York City,  as census sites to the National Register of Historic Places.\",\n",
    "# ]\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "print(inputs)\n",
    "\n",
    "model.config.use_cache = False\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=True,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "trimmed_outputs = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "print(trimmed_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 69071\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1712\n",
      "    })\n",
      "})\n",
      "{'validation', 'train'}\n",
      "{'clarity', 'coherence', 'simplification', 'gec', 'neutralize', 'paraphrase'}\n"
     ]
    }
   ],
   "source": [
    "# api = HfApi()\n",
    "# coedit_info = api.dataset_info(\"grammarly/coedit\")\n",
    "# pprint(coedit_info)\n",
    "\n",
    "grammarly_dataset = load_dataset(\"grammarly/coedit\")\n",
    "pprint(grammarly_dataset)\n",
    "\n",
    "unique_categories = set(grammarly_dataset)\n",
    "pprint(unique_categories)\n",
    "\n",
    "unique_tasks = set(grammarly_dataset[\"train\"][\"task\"])\n",
    "pprint(unique_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "[gec] Fix grammaticality in this sentence\n",
      "src: Dear friends, I hope you should correctly but I can gives you some opinion, I guess that is a good idea if you go to a small schools, under you can met a lot on people and there are more closed friend of course you cannot like that opcion if you like the biggest once, so in that ways you can go from the other school.\n",
      "tgt: Dear friend, I hope you choose correctly but I can give you my opinion. I guess that it's a good idea if you go to a small school, because you can meet a lot of people and make more close friends of course you won't like that option if you like the bigger one, so in that case you should go to the other school.\n"
     ]
    }
   ],
   "source": [
    "def get_samples(dataset, category=\"validation\", task=\"gec\", num_samples=1, seed=42):\n",
    "    return dataset[category].shuffle(seed=seed).filter(lambda item: item[\"task\"] == task).select(range(num_samples))\n",
    "\n",
    "def print_samples(samples) -> None:\n",
    "    for item in samples:\n",
    "        pfx, src = item[\"src\"].split(\": \", 1)\n",
    "        print(f\"[{item['task']}] {pfx}\")\n",
    "        print(f\"src: {src}\")\n",
    "        print(f\"tgt: {item['tgt']}\")\n",
    "\n",
    "\n",
    "print_samples(get_samples(grammarly_dataset, num_samples=2))\n",
    "\n",
    "# input_ids = tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "# outputs = model.generate(input_ids, max_length=256)\n",
    "# corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# return {\"processed\": corrected}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'rouge1': 0.7150747966331101,\n",
      " 'rouge2': 0.5167050375929942,\n",
      " 'rougeL': 0.7005677840072502,\n",
      " 'rougeLsum': 0.7007083564381789}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = rouge_metric.compute(\n",
    "    predictions=samples['src'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _GLUE metric_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "tensor([14269, 19519,    16,    48,  7142,    10,   493,  6195,   388,    55,\n",
      "            1])\n",
      "tensor([ 493, 6195,    6,  388,   55,    1])\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=2)\n",
    "pprint(object=samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "src_input_ids = tokenizer(samples[\"src\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "tgt_input_ids = tokenizer(samples[\"tgt\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "pprint(src_input_ids[0])\n",
    "pprint(tgt_input_ids[0])\n",
    "\n",
    "# score = glue_metric.compute(predictions=src_input_ids[0], references=tgt_input_ids[0])\n",
    "# score = glue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "# pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SacreBLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'bp': 1.0,\n",
      " 'counts': [3886, 2743, 1965, 1419],\n",
      " 'precisions': [70.79613773000547,\n",
      "                50.899981443681575,\n",
      "                37.152580828133864,\n",
      "                27.346309500867218],\n",
      " 'ref_len': 5090,\n",
      " 'score': 43.74251258938969,\n",
      " 'sys_len': 5489,\n",
      " 'totals': [5489, 5389, 5289, 5189]}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = sacreblue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARI metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mget_samples\u001b[49m(grammarly_dataset, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgec\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      2\u001b[0m pprint(samples)\n\u001b[1;32m      3\u001b[0m print_samples([samples[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_samples' is not defined"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "new_samples = samples.map(lambda item: {\"tgts\": [item[\"tgt\"]]})\n",
    "new_samples[\"tgts\"][:5]\n",
    "\n",
    "# sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "# predictions=[\"About 95 you now get in.\",\"About 95 you now get in.\"]\n",
    "# references=[[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"],[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"]]\n",
    "\n",
    "score = sari_metric.compute(\n",
    "  sources=new_samples['src'],\n",
    "  predictions=new_samples['src'],\n",
    "  references=new_samples['tgts']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact match (EM) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mget_samples\u001b[49m(grammarly_dataset, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgec\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      2\u001b[0m pprint(samples)\n\u001b[1;32m      3\u001b[0m print_samples([samples[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_samples' is not defined"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=samples['tgt'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Datasets\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_v2\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_full_sent\n",
    "* https://huggingface.co/datasets/grammarly/coedit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterater dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "max_samples: 5078, num_samples: 100, selected: 100\n",
      "max_samples: 5106, num_samples: 100, selected: 100\n",
      "max_samples: 1676, num_samples: 100, selected: 100\n",
      "{'fluency': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "}), 'clarity': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "}), 'coherence': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "})}\n",
      "Fix all grammatical errors:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      " We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      " We don't have enough good Open Source games — it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      "Fix grammatical errors in this sentence:  In 2001, they successfully nominated Bohemian Hall, still a vibrant community center/beer garden started by Czech immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, continuously occupied Latin music store in New York City,  as census sites to the National Register of Historic Places.\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 10\n",
    "total_samples = 100\n",
    "samples_map = {\n",
    "    \"fluency\": get_iterater_samples_simplified(label=\"fluency\", num_samples=total_samples),\n",
    "    \"clarity\": get_iterater_samples_simplified(label=\"clarity\", num_samples=total_samples),\n",
    "    \"coherence\": get_iterater_samples_simplified(label=\"coherence\", num_samples=total_samples),\n",
    "}\n",
    "print(samples_map)\n",
    "print(samples_map[\"fluency\"][\"task\"][0])\n",
    "print(samples_map[\"fluency\"][\"source\"][0])\n",
    "print(samples_map[\"fluency\"][\"reference\"][0])\n",
    "print(samples_map[\"fluency\"][\"task\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly/coedit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 70783\n",
      "})\n",
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 63703\n",
      "})\n",
      "train set {'clarity', 'coherence', 'simplification', 'gec', 'neutralize', 'paraphrase'}\n",
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 7080\n",
      "})\n",
      "test set {'clarity', 'coherence', 'simplification', 'gec', 'neutralize', 'paraphrase'}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references'],\n",
      "        num_rows: 63703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references'],\n",
      "        num_rows: 7080\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "gec_dataset_split = int(0.9 * len(gec_dataset))\n",
    "train_gec_dataset = gec_dataset.select(range(0, gec_dataset_split))\n",
    "test_gec_dataset = gec_dataset.select(range(gec_dataset_split, len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "simplification_dataset_split = int(0.9 * len(simplification_dataset))\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, simplification_dataset_split))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(simplification_dataset_split, len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "clarity_dataset_split = int(0.9 * len(clarity_dataset))\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, clarity_dataset_split))\n",
    "test_clarity_dataset = clarity_dataset.select(range(clarity_dataset_split, len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "coherence_dataset_split = int(0.9 * len(coherence_dataset))\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, coherence_dataset_split))\n",
    "test_coherence_dataset = coherence_dataset.select(range(coherence_dataset_split, len(coherence_dataset)))\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "paraphrase_dataset_split = int(0.9 * len(paraphrase_dataset))\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, paraphrase_dataset_split))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(range(paraphrase_dataset_split, len(paraphrase_dataset)))\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(0.9 * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, neutralize_dataset_split))\n",
    "test_neutralize_dataset = neutralize_dataset.select(range(neutralize_dataset_split, len(neutralize_dataset)))\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "print(train_dataset)\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "print(test_dataset)\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "dataset = dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_length: 171\n"
     ]
    }
   ],
   "source": [
    "# find the longest sequence in the dataset\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"train\"])\n",
    "print(f\"max_input_length: {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clarity - Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 126\n",
      "})\n",
      "coherence - Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 1062\n",
      "})\n",
      "simplification - Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 1144\n",
      "})\n",
      "gec - Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 2031\n",
      "})\n",
      "neutralize - Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 1127\n",
      "})\n",
      "paraphrase - Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 1590\n",
      "})\n",
      "test/clarity: 126\n",
      "test/coherence: 1062\n",
      "test/simplification: 1144\n",
      "test/gec: 2031\n",
      "test/neutralize: 1127\n",
      "test/paraphrase: 1590\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    test_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    test_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "test_dataset_map = {}\n",
    "for task, list in test_lists_map.items():\n",
    "    test_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(test_dataset_map)\n",
    "\n",
    "test_dataset_dict = DatasetDict(test_dataset_map)\n",
    "# print(test_dataset_dict)\n",
    "\n",
    "# for task, ds in test_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"test/{task}: {len(test_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval encoder-decoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "total/used/cuda/res/ram (Gb): 10.00/7.00/3.04/4.46/20.02\n",
      "Processing 126 samples for clarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd9ae06c83445a2b36ebfe455d22094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/126 | total/used/cuda/res/ram (Gb): 10.00/7.02/3.04/4.46/20.05 | batch/sps: 5/3.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-9/126 | total/used/cuda/res/ram (Gb): 10.00/7.01/3.04/4.46/20.01 | batch/sps: 5/1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-14/126 | total/used/cuda/res/ram (Gb): 10.00/6.93/3.04/4.46/20.02 | batch/sps: 5/1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15-19/126 | total/used/cuda/res/ram (Gb): 10.00/6.90/3.04/4.46/20.01 | batch/sps: 5/2.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-24/126 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.04/4.46/20.02 | batch/sps: 5/1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-29/126 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.04/4.46/20.01 | batch/sps: 5/1.32\n",
      "30-34/126 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.04/4.46/20.01 | batch/sps: 5/123.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35-39/126 | total/used/cuda/res/ram (Gb): 10.00/6.83/3.04/4.46/20.02 | batch/sps: 5/1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40-44/126 | total/used/cuda/res/ram (Gb): 10.00/6.83/3.04/4.46/20.01 | batch/sps: 5/3.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45-49/126 | total/used/cuda/res/ram (Gb): 10.00/6.83/3.04/4.46/20.02 | batch/sps: 5/4.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50-54/126 | total/used/cuda/res/ram (Gb): 10.00/6.78/3.04/4.46/20.07 | batch/sps: 5/1.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55-59/126 | total/used/cuda/res/ram (Gb): 10.00/6.80/3.04/4.46/20.08 | batch/sps: 5/10.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60-64/126 | total/used/cuda/res/ram (Gb): 10.00/6.82/3.04/4.46/20.06 | batch/sps: 5/3.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65-69/126 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.04/4.46/20.06 | batch/sps: 5/5.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70-74/126 | total/used/cuda/res/ram (Gb): 10.00/6.86/3.04/4.46/20.06 | batch/sps: 5/1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75-79/126 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.04/4.46/20.07 | batch/sps: 5/1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80-84/126 | total/used/cuda/res/ram (Gb): 10.00/6.90/3.04/4.46/20.07 | batch/sps: 5/1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85-89/126 | total/used/cuda/res/ram (Gb): 10.00/6.85/3.04/4.46/20.09 | batch/sps: 5/3.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90-94/126 | total/used/cuda/res/ram (Gb): 10.00/6.85/3.04/4.46/20.06 | batch/sps: 5/3.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95-99/126 | total/used/cuda/res/ram (Gb): 10.00/6.85/3.04/4.46/20.06 | batch/sps: 5/5.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100-104/126 | total/used/cuda/res/ram (Gb): 10.00/6.88/3.04/4.46/20.06 | batch/sps: 5/1.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105-109/126 | total/used/cuda/res/ram (Gb): 10.00/6.90/3.04/4.46/20.06 | batch/sps: 5/3.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110-114/126 | total/used/cuda/res/ram (Gb): 10.00/6.88/3.04/4.46/20.06 | batch/sps: 5/1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115-119/126 | total/used/cuda/res/ram (Gb): 10.00/6.80/3.04/4.46/20.06 | batch/sps: 5/1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120-124/126 | total/used/cuda/res/ram (Gb): 10.00/6.82/3.04/4.46/20.11 | batch/sps: 5/1.34\n",
      "125-125/126 | total/used/cuda/res/ram (Gb): 10.00/6.82/3.04/4.46/20.11 | batch/sps: 1/43.74\n",
      "Finished processing 126 samples for clarity.\n",
      "126 | total/used/cuda/res/ram (Gb): 10.00/6.82/3.04/4.46/20.11 | sps: 1.82\n",
      "Processing 1062 samples for coherence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953218de427747dd8f149aa437122ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/1062 | total/used/cuda/res/ram (Gb): 10.00/6.80/3.04/4.46/20.14 | batch/sps: 5/2.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-9/1062 | total/used/cuda/res/ram (Gb): 10.00/6.86/3.04/4.46/20.14 | batch/sps: 5/1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-14/1062 | total/used/cuda/res/ram (Gb): 10.00/6.86/3.04/4.46/20.14 | batch/sps: 5/5.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15-19/1062 | total/used/cuda/res/ram (Gb): 10.00/6.80/3.04/4.46/20.14 | batch/sps: 5/1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-24/1062 | total/used/cuda/res/ram (Gb): 10.00/6.78/3.04/4.46/20.14 | batch/sps: 5/3.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-29/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.04/4.46/20.14 | batch/sps: 5/2.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-34/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.04/4.46/20.16 | batch/sps: 5/3.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35-39/1062 | total/used/cuda/res/ram (Gb): 10.00/6.80/3.04/4.46/20.48 | batch/sps: 5/3.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40-44/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.04/4.46/20.39 | batch/sps: 5/2.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45-49/1062 | total/used/cuda/res/ram (Gb): 10.00/6.81/3.04/4.46/20.45 | batch/sps: 5/5.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50-54/1062 | total/used/cuda/res/ram (Gb): 10.00/6.80/3.04/4.46/20.45 | batch/sps: 5/5.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55-59/1062 | total/used/cuda/res/ram (Gb): 10.00/6.80/3.04/4.46/20.45 | batch/sps: 5/3.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60-64/1062 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.04/4.46/20.45 | batch/sps: 5/2.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65-69/1062 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.04/4.46/20.43 | batch/sps: 5/1.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70-74/1062 | total/used/cuda/res/ram (Gb): 10.00/6.85/3.04/4.46/20.44 | batch/sps: 5/1.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75-79/1062 | total/used/cuda/res/ram (Gb): 10.00/6.86/3.04/4.46/20.47 | batch/sps: 5/4.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80-84/1062 | total/used/cuda/res/ram (Gb): 10.00/6.86/3.04/4.46/20.44 | batch/sps: 5/4.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85-89/1062 | total/used/cuda/res/ram (Gb): 10.00/6.83/3.04/4.46/20.43 | batch/sps: 5/1.18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:57\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "File \u001b[0;32m<timed exec>:18\u001b[0m, in \u001b[0;36mmodel_process\u001b[0;34m(batch, idx, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1510\u001b[0m         input_ids,\n\u001b[1;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:183\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attn\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, value, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 183\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_weights:\n\u001b[1;32m    186\u001b[0m         attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m    187\u001b[0m             [], value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    188\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 5\n",
    "max_length = 256\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"input\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    input_ids = tokenizer(batch[\"input\"], padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(batch['task'], return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(item['task'], return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(input_ids, max_length=512)\n",
    "    outputs = model.generate(input_ids, max_length=max_length)\n",
    "    # print(f\"outputs: {outputs}\")\n",
    "    processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for task, samples in test_dataset_dict.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {task}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {task}.\")\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[task] = {\n",
    "        # \"task\": task,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"sps\": sps,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_fluency_samples = processed_samples_map[\"fluency\"][\"samples\"]\n",
    "\n",
    "pprint(processed_fluency_samples)\n",
    "pprint(processed_fluency_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval decoder-only models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "total/used/cuda/res/ram (Gb): 10.00/6.80/3.31/4.46/20.26\n",
      "Processing 126 samples for clarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'fn_kwargs'={'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 1280)\n",
      "        (wpe): Embedding(1024, 1280)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-35): 36 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=3840, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      "), 'tokenizer': GPT2TokenizerFast(name_or_path='openai-community/gpt2-large', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}, 'total_samples': 126} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb50f91560e64855af9e61beb79a4ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/126 | total/used/cuda/res/ram (Gb): 10.00/6.81/3.31/4.46/20.27 | batch/sps: 5/22.75\n",
      "5-9/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.26 | batch/sps: 5/6.75\n",
      "10-14/126 | total/used/cuda/res/ram (Gb): 10.00/6.80/3.31/4.46/20.28 | batch/sps: 5/8.73\n",
      "15-19/126 | total/used/cuda/res/ram (Gb): 10.00/6.78/3.31/4.46/20.32 | batch/sps: 5/7.71\n",
      "20-24/126 | total/used/cuda/res/ram (Gb): 10.00/6.78/3.31/4.46/20.33 | batch/sps: 5/3.66\n",
      "25-29/126 | total/used/cuda/res/ram (Gb): 10.00/6.78/3.31/4.46/20.33 | batch/sps: 5/6.32\n",
      "30-34/126 | total/used/cuda/res/ram (Gb): 10.00/6.78/3.31/4.46/20.33 | batch/sps: 5/115.38\n",
      "35-39/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.33 | batch/sps: 5/94.47\n",
      "40-44/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.33 | batch/sps: 5/7.65\n",
      "45-49/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.33 | batch/sps: 5/119.70\n",
      "50-54/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.33 | batch/sps: 5/5.03\n",
      "55-59/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.45 | batch/sps: 5/73.14\n",
      "60-64/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.33 | batch/sps: 5/86.57\n",
      "65-69/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.24 | batch/sps: 5/107.80\n",
      "70-74/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.25 | batch/sps: 5/9.38\n",
      "75-79/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.24 | batch/sps: 5/68.62\n",
      "80-84/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.24 | batch/sps: 5/72.40\n",
      "85-89/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.24 | batch/sps: 5/7.21\n",
      "90-94/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.24 | batch/sps: 5/97.24\n",
      "95-99/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.24 | batch/sps: 5/84.16\n",
      "100-104/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.25 | batch/sps: 5/98.16\n",
      "105-109/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.26 | batch/sps: 5/15.52\n",
      "110-114/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.20 | batch/sps: 5/49.22\n",
      "115-119/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.20 | batch/sps: 5/98.69\n",
      "120-124/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.20 | batch/sps: 5/67.46\n",
      "125-125/126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.20 | batch/sps: 1/44.88\n",
      "Finished processing 126 samples for clarity.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'processed'],\n",
      "    num_rows: 126\n",
      "})\n",
      "Canals are waterway channels, or artificial waterways, for water conveyance, or to service water transport vehicles.\n",
      "\n",
      "126 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.20 | sps: 11.96\n",
      "Processing 1062 samples for coherence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f0c8c0c62b493697ff35efd4a5b34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.19 | batch/sps: 5/3.78\n",
      "5-9/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.27 | batch/sps: 5/18.27\n",
      "10-14/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.21 | batch/sps: 5/10.30\n",
      "15-19/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.20 | batch/sps: 5/91.49\n",
      "20-24/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.20 | batch/sps: 5/5.73\n",
      "25-29/1062 | total/used/cuda/res/ram (Gb): 10.00/6.79/3.31/4.46/20.20 | batch/sps: 5/104.25\n",
      "30-34/1062 | total/used/cuda/res/ram (Gb): 10.00/6.82/3.31/4.46/20.22 | batch/sps: 5/4.27\n",
      "35-39/1062 | total/used/cuda/res/ram (Gb): 10.00/6.82/3.31/4.46/20.20 | batch/sps: 5/4.85\n",
      "40-44/1062 | total/used/cuda/res/ram (Gb): 10.00/6.83/3.31/4.46/20.20 | batch/sps: 5/12.24\n",
      "45-49/1062 | total/used/cuda/res/ram (Gb): 10.00/6.83/3.31/4.46/20.20 | batch/sps: 5/8.38\n",
      "50-54/1062 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.31/4.46/20.20 | batch/sps: 5/12.83\n",
      "55-59/1062 | total/used/cuda/res/ram (Gb): 10.00/6.84/3.31/4.46/20.21 | batch/sps: 5/7.29\n",
      "60-64/1062 | total/used/cuda/res/ram (Gb): 10.00/6.85/3.31/4.46/20.22 | batch/sps: 5/9.64\n",
      "65-69/1062 | total/used/cuda/res/ram (Gb): 10.00/6.85/3.31/4.46/20.22 | batch/sps: 5/12.22\n",
      "70-74/1062 | total/used/cuda/res/ram (Gb): 10.00/6.83/3.31/4.46/20.22 | batch/sps: 5/5.63\n",
      "75-79/1062 | total/used/cuda/res/ram (Gb): 10.00/6.83/3.31/4.46/20.22 | batch/sps: 5/8.93\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 20\n",
    "max_length = 256\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"input\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    # print(batch[\"input\"])\n",
    "    inputs = tokenizer(batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    # print(inputs)\n",
    "    # inputs = tokenizer(batch[\"input\"], return_tensors=\"pt\").inputs.to(device)\n",
    "    # inputs = tokenizer(item[\"input\"], return_tensors=\"pt\").inputs\n",
    "\n",
    "    # outputs = model.generate(inputs, max_length=512)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    # print(outputs)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    processed = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    # print(processed)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for task, samples in test_dataset_dict.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {task}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {task}.\")\n",
    "    print(processed_samples)\n",
    "    print(processed_samples[\"reference\"][0])\n",
    "    print(processed_samples[\"processed\"][0])\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[task] = {\n",
    "        # \"task\": task,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"sps\": sps,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_fluency_samples = processed_samples_map[\"fluency\"][\"samples\"]\n",
    "\n",
    "pprint(processed_fluency_samples)\n",
    "pprint(processed_fluency_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving eval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large-bnb8-coedit,model_id: iliazlobin/gpt2-large-bnb8-coedit,model_path: iliazlobin_gpt2-large-bnb8-coedit\n",
      "Total/trainable params: 785826560/0\n",
      "{'model': 'gpt2-large-bnb8-coedit', 'hardware': 'HomeDesktop (RTX3080)', 'total_params': 785826560, 'fluency': {'category': 'fluency', 'total_samples': 10, 'sps': 1.4038752382088802, 'scores': {'rouge': {'rouge1': 0.6285879338544873, 'rouge2': 0.5650044282777548, 'rougeL': 0.6110223542727786, 'rougeLsum': 0.6128952302178108}, 'sacreblue': {'score': 36.89335999040467, 'counts': [182, 160, 143, 126], 'totals': [219, 211, 203, 195], 'precisions': [83.10502283105023, 75.82938388625593, 70.44334975369458, 64.61538461538461], 'bp': 0.5041247574853277, 'sys_len': 219, 'ref_len': 369}, 'sari': {'sari': 29.490016265428093}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8572403712, 'cuda_allocated': 3266646528, 'cuda_reserved': 6719275008, 'ram_usage': 24726765568}}, 'clarity': {'category': 'clarity', 'total_samples': 10, 'sps': 1.7353276763710268, 'scores': {'rouge': {'rouge1': 0.5205313727336301, 'rouge2': 0.4646924483958938, 'rougeL': 0.5219687140377878, 'rougeLsum': 0.5150080820771559}, 'sacreblue': {'score': 36.583936895453114, 'counts': [148, 121, 106, 91], 'totals': [194, 187, 180, 173], 'precisions': [76.28865979381443, 64.70588235294117, 58.888888888888886, 52.60115606936416], 'bp': 0.585035661238133, 'sys_len': 194, 'ref_len': 298}, 'sari': {'sari': 30.125629339456868}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8573517824, 'cuda_allocated': 3266646528, 'cuda_reserved': 6719275008, 'ram_usage': 24736018432}}, 'coherence': {'category': 'coherence', 'total_samples': 10, 'sps': 2.026729630862352, 'scores': {'rouge': {'rouge1': 0.44030142943186423, 'rouge2': 0.42604477611940295, 'rougeL': 0.42729582729582727, 'rougeLsum': 0.43961437874481346}, 'sacreblue': {'score': 29.15762288192054, 'counts': [125, 115, 108, 101], 'totals': [198, 191, 184, 177], 'precisions': [63.13131313131313, 60.20942408376963, 58.69565217391305, 57.06214689265537], 'bp': 0.4881308776541762, 'sys_len': 198, 'ref_len': 340}, 'sari': {'sari': 42.85178443420447}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8574566400, 'cuda_allocated': 3266646528, 'cuda_reserved': 6719275008, 'ram_usage': 24651825152}}}\n"
     ]
    }
   ],
   "source": [
    "hardware = \"HomeDesktop (RTX3080)\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "all_flats = []\n",
    "all_scores = []\n",
    "if os.path.exists(\"results/all-scores.csv\"):\n",
    "    all_scores = pd.read_csv(\"results/all-scores.csv\").to_dict(\"records\")\n",
    "\n",
    "all_fulls = []\n",
    "\n",
    "for split, obj in processed_samples_map.items():\n",
    "    samples = obj[\"samples\"]\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    all_saved_samples = samples.remove_columns([\"references\"])\n",
    "    saved_samples = all_saved_samples[:100] if len(all_saved_samples) > 100 else all_saved_samples\n",
    "    flats_frame = pd.DataFrame.from_records(saved_samples)\n",
    "    flats_frame.to_json(f\"samples/{model_path}_{split}.json\", orient=\"records\")\n",
    "\n",
    "    scores = calculate_scores(samples)\n",
    "    # pprint(scores)\n",
    "\n",
    "    score_paths = [\n",
    "        \"rouge.rouge1\",\n",
    "        # \"rouge.rouge2\",\n",
    "        # \"rouge.rougeL\",\n",
    "        # \"rouge.rougeLsum\",\n",
    "        \"sacreblue.score\",\n",
    "        \"sari.sari\",\n",
    "        \"em.exact_match\",\n",
    "    ]\n",
    "\n",
    "    normalized_scores = {}\n",
    "    for task, ds in scores.items():\n",
    "        for k2, v2 in ds.items():\n",
    "            if not isinstance(v2, list):\n",
    "                # normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "                path = f\"{task}.{k2}\"\n",
    "                if path in score_paths:\n",
    "                    normalized_scores[f\"score.{task}.{k2}\"] = v2\n",
    "    # pprint(normalized_scores)\n",
    "\n",
    "    normalized_utilization = {}\n",
    "    for task, ds in obj[\"utilization\"].items():\n",
    "        if not isinstance(ds, list):\n",
    "            normalized_utilization[f\"utilization.{task}\"] = ds\n",
    "    # print(normalized_utilization)\n",
    "\n",
    "    flat_dict = {\n",
    "        \"model\": model_name,\n",
    "        \"hardware\": hardware,\n",
    "        \"total_params\": total_params,\n",
    "        \"category\": split,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"sps\": obj[\"sps\"],\n",
    "    }\n",
    "    flat_dict.update(normalized_scores)\n",
    "    flat_dict.update(normalized_utilization)\n",
    "    # pprint(frame)\n",
    "\n",
    "    all_flats.append(flat_dict)\n",
    "    all_scores.append(flat_dict)\n",
    "\n",
    "    fulls_frame = {\n",
    "        \"category\": split,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"sps\": obj[\"sps\"],\n",
    "    }\n",
    "    fulls_frame.update(\n",
    "        {\n",
    "            \"scores\": scores,\n",
    "            \"utilization\": obj[\"utilization\"],\n",
    "        }\n",
    "    )\n",
    "    all_fulls.append(fulls_frame)\n",
    "\n",
    "flats_frame = pd.DataFrame.from_records(all_flats)\n",
    "flats_frame.to_csv(f\"results/{model_path}.csv\", index=False)\n",
    "\n",
    "scores_frame = pd.DataFrame.from_records(all_scores)\n",
    "scores_frame.to_csv(f\"results/all-scores.csv\", index=False)\n",
    "\n",
    "fulls_dict = {\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": hardware,\n",
    "    \"total_params\": total_params,\n",
    "}\n",
    "for full in all_fulls:\n",
    "    fulls_dict[full[\"category\"]] = full\n",
    "\n",
    "print(fulls_dict)\n",
    "fulls_frame = pd.DataFrame.from_records([fulls_dict])\n",
    "fulls_frame.to_json(f\"results/{model_path}.json\", orient=\"records\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
