{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install transformers evaluate\n",
    "%pip install nltk absl-py rouge_score\n",
    "%pip install bleu sacrebleu\n",
    "%pip install sacremoses\n",
    "%pip install scipy\n",
    "%pip install sentencepiece\n",
    "%pip install optimum auto-gptq\n",
    "%pip install scikit-learn\n",
    "%pip install einops\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading custom gpt2 / gpt2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large,model_id: openai-community/gpt2-large,model_path: openai-community_gpt2-large\n",
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "trained_model_name: gpt2-large-bnb8-coedit,trained_model_id: iliazlobin/gpt2-large-bnb8-coedit,trained_model_path: iliazlobin_gpt2-large-bnb8-coedit\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"openai-community/gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 1280)\n",
      "        (wpe): Embedding(1024, 1280)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-35): 36 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=3840, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "model_name: gpt2-large-bnb8-coedit,model_id: iliazlobin/gpt2-large-bnb8-coedit,model_path: iliazlobin_gpt2-large-bnb8-coedit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "model_repo = f\"openai-community\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "# print(model.config)\n",
    "# print(model)\n",
    "\n",
    "trained_model_name = \"gpt2-large-bnb8-coedit\"\n",
    "trained_model_repo = f\"iliazlobin\"\n",
    "trained_model_id = f\"{trained_model_repo}/{trained_model_name}\"\n",
    "trained_model_checkpoint = f\"{trained_model_repo}/{trained_model_name}\"\n",
    "trained_model_path = f\"{trained_model_repo}_{trained_model_name}\"\n",
    "print(\n",
    "    f\"trained_model_name: {trained_model_name},\"\n",
    "    f\"trained_model_id: {trained_model_id},\"\n",
    "    f\"trained_model_path: {trained_model_path}\"\n",
    ")\n",
    "\n",
    "\n",
    "adapters_path = f\"../model-train/model-{trained_model_repo}_{trained_model_name}\"\n",
    "peft_model = PeftModel.from_pretrained(model, adapters_path)\n",
    "\n",
    "print(type(peft_model))\n",
    "print(peft_model.config)\n",
    "print(peft_model)\n",
    "\n",
    "origin_model = model\n",
    "model = peft_model\n",
    "model_name = trained_model_name\n",
    "model_repo = trained_model_repo\n",
    "model_id = trained_model_id\n",
    "model_checkpoint = trained_model_checkpoint\n",
    "model_path = trained_model_path\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/izlobin/ws/transformers-labs/model-train/model-gpt2-large-bnb4bit-test does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/izlobin/ws/transformers-labs/model-train/model-gpt2-large-bnb4bit-test/tree/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/izlobin/ws/transformers-labs/model-train/model-gpt2-large-bnb4bit-test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n",
      "\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n",
      "\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m---> 12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\u001b[39;00m\n",
      "\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, quantization_config\u001b[38;5;241m=\u001b[39mbnb_config, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:794\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n",
      "\u001b[0;32m--> 794\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    795\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    797\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n",
      "\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1138\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1135\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;32m   1136\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;32m-> 1138\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1139\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;32m   1140\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n",
      "\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n",
      "\u001b[0;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n",
      "\u001b[1;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n",
      "\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n",
      "\u001b[0;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n",
      "\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n",
      "\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:369\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n",
      "\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n",
      "\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[0;32m--> 369\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n",
      "\u001b[1;32m    370\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    372\u001b[0m         )\n",
      "\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mOSError\u001b[0m: /home/izlobin/ws/transformers-labs/model-train/model-gpt2-large-bnb4bit-test does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/izlobin/ws/transformers-labs/model-train/model-gpt2-large-bnb4bit-test/tree/None' for available files."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"/home/izlobin/ws/transformers-labs/model-train/model-gpt2-large-bnb4bit-test\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "print(type(model))\n",
    "print(type(tokenizer))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BART\n",
    "* https://huggingface.co/facebook/bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# model = BartModel.from_pretrained(model_name)\n",
    "# model = BartForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "BartConfig {\n",
      "  \"_name_or_path\": \"iliazlobin/bart-grammarly\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"iliazlobin/bart-grammarly\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# model = BartModel.from_pretrained(model_name)\n",
    "# model = BartForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading t5\n",
    "* https://huggingface.co/google-t5/t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name, model_max_length=512)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading coedit / flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading grammarly/coedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"grammarly/coedit-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"grammarly/coedit-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/gemma-2b-it\"\n",
    "# model_name = \"google/gemma-7b-it\"\n",
    "# model_name = \"google/gemma-7b\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # revision=\"float16\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Phi-2\n",
    "* https://huggingface.co/microsoft/phi-2\n",
    "* https://huggingface.co/TheBloke/phi-2-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token: <|endoftext|>\n",
      "PhiConfig {\n",
      "  \"_name_or_path\": \"TheBloke/phi-2-GPTQ\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"TheBloke/phi-2-GPTQ--configuration_phi.PhiConfig\",\n",
      "    \"AutoModelForCausalLM\": \"TheBloke/phi-2-GPTQ--modeling_phi.PhiForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"flash_attn\": false,\n",
      "  \"flash_rotary\": false,\n",
      "  \"fused_dense\": false,\n",
      "  \"img_processor\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"phi-msft\",\n",
      "  \"n_embd\": 2560,\n",
      "  \"n_head\": 32,\n",
      "  \"n_head_kv\": null,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 32,\n",
      "  \"n_positions\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": true,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'microsoft/phi-2'\n",
    "model_name = 'TheBloke/phi-2-GPTQ'\n",
    "model_alias = model_name.replace('/', '_')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\n",
    "# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # revision=\"float16\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading mixtral-8x7B\n",
    "* https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "* https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ\n",
    "\n",
    "Quantization\n",
    "* https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from transformers import BitsAndBytesConfig\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     29\u001b[0m     model_name,\n\u001b[1;32m     30\u001b[0m     use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# tokenizer.pad_token = tokenizer.eos_token\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3039\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3039\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3043\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/quantizers/auto.py:153\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m AutoQuantizationConfig\u001b[38;5;241m.\u001b[39mfrom_dict(quantization_config)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig)) \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ config collision\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m \u001b[43mquantization_config_from_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loading_attributes\u001b[49m()\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m loading_attr_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(quantization_config, attr, val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'"
     ]
    }
   ],
   "source": [
    "# model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "model_name = 'TheBloke/Mixtral-8x7B-v0.1-GPTQ'\n",
    "model_alias = model_name.replace('/', '_')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=0,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # load_in_4bit=True,\n",
    "#     # quantization_config=quantization_config,\n",
    "# )\n",
    "\n",
    "# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\n",
    "# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# text = 'Hello my name is'\n",
    "# inputs = tokenizer(text, return_tensors='pt')\n",
    "# outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"TheBloke/Llama-2-7B-GPTQ\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": false,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_name = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)\n",
    "\n",
    "# from auto_gptq import exllama_set_max_input_length\n",
    "# model = exllama_set_max_input_length(model, max_input_length=2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring memory (VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total/trainable params: 262410240/262410240\n",
      "{'total_memory': 10736893952, 'memory_used': 5595607040, 'cuda_allocated': 4104217088, 'cuda_reserved': 4374659072, 'ram_usage': 15024398336}\n",
      "{'total_memory': '10.00', 'memory_used': '5.21', 'cuda_allocated': '3.82', 'cuda_reserved': '4.07', 'ram_usage': '13.99'}\n",
      "total/used/cuda/res/ram(Gb): 10.00/5.21/3.82/4.07/13.99\n",
      "Total/used/available memory (Gb): 10.00/{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\n",
      "Recommended/actual fraction: 0.48/0.90\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total/trainable params: {total_params}/{total_trainable_params}')\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "print(utilization)\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(utilization_str)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram(Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "actual_fraction = 0.90\n",
    "available_memory = utilization['total_memory'] - utilization['memory_used']\n",
    "recommended_fraction = available_memory / utilization['total_memory']\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"Total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    \"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f'Recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}')\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.empty(utilization['total_memory'] // 2, dtype=torch.int8, device='cuda')\n",
    "# print_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\n",
      " nobody can deny that the best way to get a good job is to study hard.\n",
      "nobody can deny that the best way to get a good job is to study hard.\n",
      "nobody can deny that the best way to get a good job is to study hard.\n",
      "nobody can deny that the best way to get a good job is to study hard.\n",
      "nobody can deny that the best way to get a good job is to\n"
     ]
    }
   ],
   "source": [
    "# input_text = \"Fix grammatical errors in this sentence: I goes to school every day.\"\n",
    "input_text = \"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\"\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "# outputs = model.generate(\n",
    "#     **input_ids,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=False,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
      "          1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
      "           430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
      "          1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
      "           198, 31077,    25],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 22743, 23491,    25,   679,  3708,   257,  4998,  1097,    13,\n",
      "           198, 31077,    25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "[' Countries with a lot of deserts can terraform their deserts to increase their habitable land and using irrigation to provide clean water to the desert.', ' He drives an amazing car.']\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\n",
    "    f\"\"\"Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "Response:\n",
    "\"\"\".strip(),\n",
    "    f\"\"\"Fix grammar: He drive a amazing car.\n",
    "Response:\n",
    "\"\"\".strip(),\n",
    "]\n",
    "# input_texts = [\n",
    "#     \"Fix grammar:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\",\n",
    "#     \"Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian Hall, still a vibrant community center/beer garden started by Czech immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, continuously occupied Latin music store in New York City,  as census sites to the National Register of Historic Places.\",\n",
    "# ]\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "print(inputs)\n",
    "\n",
    "model.config.use_cache = False\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=True,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "trimmed_outputs = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "print(trimmed_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 69071\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1712\n",
      "    })\n",
      "})\n",
      "{'validation', 'train'}\n",
      "{'gec', 'simplification', 'clarity', 'coherence', 'paraphrase', 'neutralize'}\n"
     ]
    }
   ],
   "source": [
    "# api = HfApi()\n",
    "# coedit_info = api.dataset_info(\"grammarly/coedit\")\n",
    "# pprint(coedit_info)\n",
    "\n",
    "grammarly_dataset = load_dataset(\"grammarly/coedit\")\n",
    "pprint(grammarly_dataset)\n",
    "\n",
    "unique_categories = set(grammarly_dataset)\n",
    "pprint(unique_categories)\n",
    "\n",
    "unique_tasks = set(grammarly_dataset[\"train\"][\"task\"])\n",
    "pprint(unique_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "[gec] Fix grammaticality in this sentence\n",
      "src: Dear friends, I hope you should correctly but I can gives you some opinion, I guess that is a good idea if you go to a small schools, under you can met a lot on people and there are more closed friend of course you cannot like that opcion if you like the biggest once, so in that ways you can go from the other school.\n",
      "tgt: Dear friend, I hope you choose correctly but I can give you my opinion. I guess that it's a good idea if you go to a small school, because you can meet a lot of people and make more close friends of course you won't like that option if you like the bigger one, so in that case you should go to the other school.\n"
     ]
    }
   ],
   "source": [
    "def get_samples(dataset, category=\"validation\", task=\"gec\", num_samples=1, seed=42):\n",
    "    return dataset[category].shuffle(seed=seed).filter(lambda item: item[\"task\"] == task).select(range(num_samples))\n",
    "\n",
    "def print_samples(samples) -> None:\n",
    "    for item in samples:\n",
    "        pfx, src = item[\"src\"].split(\": \", 1)\n",
    "        print(f\"[{item['task']}] {pfx}\")\n",
    "        print(f\"src: {src}\")\n",
    "        print(f\"tgt: {item['tgt']}\")\n",
    "\n",
    "\n",
    "print_samples(get_samples(grammarly_dataset, num_samples=2))\n",
    "\n",
    "# input_ids = tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "# outputs = model.generate(input_ids, max_length=256)\n",
    "# corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# return {\"processed\": corrected}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'rouge1': 0.7150747966331101,\n",
      " 'rouge2': 0.5167050375929942,\n",
      " 'rougeL': 0.7005677840072502,\n",
      " 'rougeLsum': 0.7007083564381789}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = rouge_metric.compute(\n",
    "    predictions=samples['src'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _GLUE metric_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "tensor([14269, 19519,    16,    48,  7142,    10,   493,  6195,   388,    55,\n",
      "            1])\n",
      "tensor([ 493, 6195,    6,  388,   55,    1])\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=2)\n",
    "pprint(object=samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "src_input_ids = tokenizer(samples[\"src\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "tgt_input_ids = tokenizer(samples[\"tgt\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "pprint(src_input_ids[0])\n",
    "pprint(tgt_input_ids[0])\n",
    "\n",
    "# score = glue_metric.compute(predictions=src_input_ids[0], references=tgt_input_ids[0])\n",
    "# score = glue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "# pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SacreBLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'bp': 1.0,\n",
      " 'counts': [3886, 2743, 1965, 1419],\n",
      " 'precisions': [70.79613773000547,\n",
      "                50.899981443681575,\n",
      "                37.152580828133864,\n",
      "                27.346309500867218],\n",
      " 'ref_len': 5090,\n",
      " 'score': 43.74251258938969,\n",
      " 'sys_len': 5489,\n",
      " 'totals': [5489, 5389, 5289, 5189]}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = sacreblue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARI metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mget_samples\u001b[49m(grammarly_dataset, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgec\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      2\u001b[0m pprint(samples)\n\u001b[1;32m      3\u001b[0m print_samples([samples[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_samples' is not defined"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "new_samples = samples.map(lambda item: {\"tgts\": [item[\"tgt\"]]})\n",
    "new_samples[\"tgts\"][:5]\n",
    "\n",
    "# sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "# predictions=[\"About 95 you now get in.\",\"About 95 you now get in.\"]\n",
    "# references=[[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"],[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"]]\n",
    "\n",
    "score = sari_metric.compute(\n",
    "  sources=new_samples['src'],\n",
    "  predictions=new_samples['src'],\n",
    "  references=new_samples['tgts']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact match (EM) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mget_samples\u001b[49m(grammarly_dataset, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgec\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      2\u001b[0m pprint(samples)\n\u001b[1;32m      3\u001b[0m print_samples([samples[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_samples' is not defined"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=samples['tgt'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Datasets\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_v2\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_full_sent\n",
    "* https://huggingface.co/datasets/grammarly/coedit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterater dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "max_samples: 5078, num_samples: 100, selected: 100\n",
      "max_samples: 5106, num_samples: 100, selected: 100\n",
      "max_samples: 1676, num_samples: 100, selected: 100\n",
      "{'fluency': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "}), 'clarity': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "}), 'coherence': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "})}\n",
      "Fix all grammatical errors:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      " We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      " We don't have enough good Open Source games — it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      "Fix grammatical errors in this sentence:  In 2001, they successfully nominated Bohemian Hall, still a vibrant community center/beer garden started by Czech immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, continuously occupied Latin music store in New York City,  as census sites to the National Register of Historic Places.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 10\n",
    "total_samples = 100\n",
    "samples_map = {\n",
    "    \"fluency\": get_iterater_samples_simplified(label=\"fluency\", num_samples=total_samples),\n",
    "    \"clarity\": get_iterater_samples_simplified(label=\"clarity\", num_samples=total_samples),\n",
    "    \"coherence\": get_iterater_samples_simplified(label=\"coherence\", num_samples=total_samples),\n",
    "}\n",
    "print(samples_map)\n",
    "print(samples_map[\"fluency\"][\"task\"][0])\n",
    "print(samples_map[\"fluency\"][\"source\"][0])\n",
    "print(samples_map[\"fluency\"][\"reference\"][0])\n",
    "print(samples_map[\"fluency\"][\"task\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly/coedit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 70783\n",
      "})\n",
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 63703\n",
      "})\n",
      "train set {'gec', 'clarity', 'simplification', 'coherence', 'paraphrase', 'neutralize'}\n",
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 7080\n",
      "})\n",
      "test set {'gec', 'clarity', 'simplification', 'coherence', 'paraphrase', 'neutralize'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a282f7fd749340118d333c89ae10f6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6d963a14f2496a9cedf655a8142360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references'],\n",
      "        num_rows: 63703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references'],\n",
      "        num_rows: 7080\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "gec_dataset_split = int(0.9 * len(gec_dataset))\n",
    "train_gec_dataset = gec_dataset.select(range(0, gec_dataset_split))\n",
    "test_gec_dataset = gec_dataset.select(range(gec_dataset_split, len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "simplification_dataset_split = int(0.9 * len(simplification_dataset))\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, simplification_dataset_split))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(simplification_dataset_split, len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "clarity_dataset_split = int(0.9 * len(clarity_dataset))\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, clarity_dataset_split))\n",
    "test_clarity_dataset = clarity_dataset.select(range(clarity_dataset_split, len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "coherence_dataset_split = int(0.9 * len(coherence_dataset))\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, coherence_dataset_split))\n",
    "test_coherence_dataset = coherence_dataset.select(range(coherence_dataset_split, len(coherence_dataset)))\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "paraphrase_dataset_split = int(0.9 * len(paraphrase_dataset))\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, paraphrase_dataset_split))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(range(paraphrase_dataset_split, len(paraphrase_dataset)))\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(0.9 * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, neutralize_dataset_split))\n",
    "test_neutralize_dataset = neutralize_dataset.select(range(neutralize_dataset_split, len(neutralize_dataset)))\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "print(train_dataset)\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "print(test_dataset)\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "dataset = dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval encoder-decoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 5\n",
    "total_samples = 10\n",
    "samples_map = {\n",
    "    \"fluency\": get_iterater_samples_simplified(label=\"fluency\", num_samples=total_samples),\n",
    "    \"clarity\": get_iterater_samples_simplified(label=\"clarity\", num_samples=total_samples),\n",
    "    \"coherence\": get_iterater_samples_simplified(label=\"coherence\", num_samples=total_samples),\n",
    "}\n",
    "# {'gec', 'clarity', 'simplification', 'coherence', 'paraphrase', 'neutralize'}\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"task\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    input_ids = tokenizer(batch[\"task\"], padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(batch['task'], return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(item['task'], return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(input_ids, max_length=512)\n",
    "    outputs = model.generate(input_ids, max_length=312)\n",
    "    # print(f\"outputs: {outputs}\")\n",
    "    processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for category, samples in samples_map.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {category}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {category}.\")\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[category] = {\n",
    "        # \"category\": category,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"sps\": sps,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_fluency_samples = processed_samples_map[\"fluency\"][\"samples\"]\n",
    "\n",
    "pprint(processed_fluency_samples)\n",
    "pprint(processed_fluency_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval decoder-only models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "max_samples: 5078, num_samples: 10, selected: 10\n",
      "max_samples: 5106, num_samples: 10, selected: 10\n",
      "max_samples: 1676, num_samples: 10, selected: 10\n",
      "total/used/cuda/res/ram (Gb): 10.00/8.04/3.04/6.26/23.11\n",
      "Processing 10 samples for fluency\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Fix grammar errors:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre? Response: \n",
      " We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbc7157da674a7db5ae10283c36898e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/10 | total/used/cuda/res/ram (Gb): 10.00/8.02/3.04/6.26/23.02 | batch/sps: 5/3.14\n",
      "5-9/10 | total/used/cuda/res/ram (Gb): 10.00/7.98/3.04/6.26/23.03 | batch/sps: 5/3.88\n",
      "Finished processing 10 samples for fluency.\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'processed'],\n",
      "    num_rows: 10\n",
      "})\n",
      " We don't have enough good Open Source games — it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      " We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :)\n",
      "10 | total/used/cuda/res/ram (Gb): 10.00/7.98/3.04/6.26/23.03 | sps: 1.40\n",
      "Processing 10 samples for clarity\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Make this sentence more readable:  Dr. Farley Stillwell appears in Spider-Man: The Animated  , voiced by Michael Rye. Response: \n",
      " Dr. Farley Stillwell appears in Spider-Man: The Animated  , voiced by Michael Rye.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5479fed04e24208b25dc668aa116582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/10 | total/used/cuda/res/ram (Gb): 10.00/7.98/3.04/6.26/23.04 | batch/sps: 5/3.80\n",
      "5-9/10 | total/used/cuda/res/ram (Gb): 10.00/7.98/3.04/6.26/23.04 | batch/sps: 5/5.42\n",
      "Finished processing 10 samples for clarity.\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'processed'],\n",
      "    num_rows: 10\n",
      "})\n",
      " Dr. Farley Stillwell appears in Spider-Man: The Animated Series , voiced by Michael Rye.\n",
      " Dr. Farley Stillwell appears in Spider-Man: The Animated, voiced by Michael Rye.\n",
      "10 | total/used/cuda/res/ram (Gb): 10.00/7.98/3.04/6.26/23.04 | sps: 1.74\n",
      "Processing 10 samples for coherence\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Improve the coherence of the text:  The study also exhibits three methodological challenges with respect to assessing of interventions : a) the estimation of true infection dates , b) the usage of several indicators and c) the influence of test volume . In conclusion, the effectiveness of most German interventions remains questionable . Response: \n",
      " The study also exhibits three methodological challenges with respect to assessing of interventions : a) the estimation of true infection dates , b) the usage of several indicators and c) the influence of test volume . In conclusion, the effectiveness of most German interventions remains questionable .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d299345081449688a060b0eba13441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/10 | total/used/cuda/res/ram (Gb): 10.00/7.99/3.04/6.26/22.96 | batch/sps: 5/5.59\n",
      "5-9/10 | total/used/cuda/res/ram (Gb): 10.00/7.99/3.04/6.26/22.96 | batch/sps: 5/4.02\n",
      "Finished processing 10 samples for coherence.\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'processed'],\n",
      "    num_rows: 10\n",
      "})\n",
      " The study also exhibits three methodological challenges with respect to assessing of interventions : a) the estimation of true infection dates , b) the usage of several indicators and c) the influence of test volume .\n",
      "\n",
      "10 | total/used/cuda/res/ram (Gb): 10.00/7.99/3.04/6.26/22.96 | sps: 2.03\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'processed'],\n",
      "    num_rows: 10\n",
      "})\n",
      "[\" We don't have enough good Open Source games -- it's a waste to pour all the \"\n",
      " 'resources we have into one. :)',\n",
      " '']\n",
      "CPU times: user 14.2 s, sys: 3.61 s, total: 17.9 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 5\n",
    "total_samples = 10\n",
    "samples_map = {\n",
    "    \"fluency\": get_iterater_samples_with_instruction(label=\"fluency\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "    \"clarity\": get_iterater_samples_with_instruction(label=\"clarity\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "    \"coherence\": get_iterater_samples_with_instruction(label=\"coherence\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "}\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"task\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    # print(batch[\"task\"])\n",
    "    inputs = tokenizer(batch[\"task\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    # print(inputs)\n",
    "    # inputs = tokenizer(batch['task'], return_tensors=\"pt\").inputs.to(device)\n",
    "    # inputs = tokenizer(item['task'], return_tensors=\"pt\").inputs\n",
    "\n",
    "    # outputs = model.generate(inputs, max_length=512)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=128,\n",
    "    )\n",
    "    # print(outputs)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    processed = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    # print(processed)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for category, samples in samples_map.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {category}\")\n",
    "    print(samples)\n",
    "    print(samples[\"task\"][0])\n",
    "    print(samples[\"source\"][0])\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {category}.\")\n",
    "    print(processed_samples)\n",
    "    print(processed_samples[\"reference\"][0])\n",
    "    print(processed_samples[\"processed\"][0])\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[category] = {\n",
    "        # \"category\": category,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"sps\": sps,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_fluency_samples = processed_samples_map[\"fluency\"][\"samples\"]\n",
    "\n",
    "pprint(processed_fluency_samples)\n",
    "pprint(processed_fluency_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving eval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large-bnb8-coedit,model_id: iliazlobin/gpt2-large-bnb8-coedit,model_path: iliazlobin_gpt2-large-bnb8-coedit\n",
      "Total/trainable params: 785826560/0\n",
      "{'model': 'gpt2-large-bnb8-coedit', 'hardware': 'HomeDesktop (RTX3080)', 'total_params': 785826560, 'fluency': {'category': 'fluency', 'total_samples': 10, 'sps': 1.4038752382088802, 'scores': {'rouge': {'rouge1': 0.6285879338544873, 'rouge2': 0.5650044282777548, 'rougeL': 0.6110223542727786, 'rougeLsum': 0.6128952302178108}, 'sacreblue': {'score': 36.89335999040467, 'counts': [182, 160, 143, 126], 'totals': [219, 211, 203, 195], 'precisions': [83.10502283105023, 75.82938388625593, 70.44334975369458, 64.61538461538461], 'bp': 0.5041247574853277, 'sys_len': 219, 'ref_len': 369}, 'sari': {'sari': 29.490016265428093}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8572403712, 'cuda_allocated': 3266646528, 'cuda_reserved': 6719275008, 'ram_usage': 24726765568}}, 'clarity': {'category': 'clarity', 'total_samples': 10, 'sps': 1.7353276763710268, 'scores': {'rouge': {'rouge1': 0.5205313727336301, 'rouge2': 0.4646924483958938, 'rougeL': 0.5219687140377878, 'rougeLsum': 0.5150080820771559}, 'sacreblue': {'score': 36.583936895453114, 'counts': [148, 121, 106, 91], 'totals': [194, 187, 180, 173], 'precisions': [76.28865979381443, 64.70588235294117, 58.888888888888886, 52.60115606936416], 'bp': 0.585035661238133, 'sys_len': 194, 'ref_len': 298}, 'sari': {'sari': 30.125629339456868}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8573517824, 'cuda_allocated': 3266646528, 'cuda_reserved': 6719275008, 'ram_usage': 24736018432}}, 'coherence': {'category': 'coherence', 'total_samples': 10, 'sps': 2.026729630862352, 'scores': {'rouge': {'rouge1': 0.44030142943186423, 'rouge2': 0.42604477611940295, 'rougeL': 0.42729582729582727, 'rougeLsum': 0.43961437874481346}, 'sacreblue': {'score': 29.15762288192054, 'counts': [125, 115, 108, 101], 'totals': [198, 191, 184, 177], 'precisions': [63.13131313131313, 60.20942408376963, 58.69565217391305, 57.06214689265537], 'bp': 0.4881308776541762, 'sys_len': 198, 'ref_len': 340}, 'sari': {'sari': 42.85178443420447}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8574566400, 'cuda_allocated': 3266646528, 'cuda_reserved': 6719275008, 'ram_usage': 24651825152}}}\n"
     ]
    }
   ],
   "source": [
    "hardware = \"HomeDesktop (RTX3080)\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "all_flats = []\n",
    "all_scores = []\n",
    "if os.path.exists(\"results/all-scores.csv\"):\n",
    "    all_scores = pd.read_csv(\"results/all-scores.csv\").to_dict(\"records\")\n",
    "\n",
    "all_fulls = []\n",
    "\n",
    "for split, obj in processed_samples_map.items():\n",
    "    samples = obj[\"samples\"]\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    all_saved_samples = samples.remove_columns([\"references\"])\n",
    "    saved_samples = all_saved_samples[:100] if len(all_saved_samples) > 100 else all_saved_samples\n",
    "    flats_frame = pd.DataFrame.from_records(saved_samples)\n",
    "    flats_frame.to_json(f\"samples/{model_path}_{split}.json\", orient=\"records\")\n",
    "\n",
    "    scores = calculate_scores(samples)\n",
    "    # pprint(scores)\n",
    "\n",
    "    score_paths = [\n",
    "        \"rouge.rouge1\",\n",
    "        # \"rouge.rouge2\",\n",
    "        # \"rouge.rougeL\",\n",
    "        # \"rouge.rougeLsum\",\n",
    "        \"sacreblue.score\",\n",
    "        \"sari.sari\",\n",
    "        \"em.exact_match\",\n",
    "    ]\n",
    "\n",
    "    normalized_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        for k2, v2 in v.items():\n",
    "            if not isinstance(v2, list):\n",
    "                # normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "                path = f\"{k}.{k2}\"\n",
    "                if path in score_paths:\n",
    "                    normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "    # pprint(normalized_scores)\n",
    "\n",
    "    normalized_utilization = {}\n",
    "    for k, v in obj[\"utilization\"].items():\n",
    "        if not isinstance(v, list):\n",
    "            normalized_utilization[f\"utilization.{k}\"] = v\n",
    "    # print(normalized_utilization)\n",
    "\n",
    "    flat_dict = {\n",
    "        \"model\": model_name,\n",
    "        \"hardware\": hardware,\n",
    "        \"total_params\": total_params,\n",
    "        \"category\": split,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"sps\": obj[\"sps\"],\n",
    "    }\n",
    "    flat_dict.update(normalized_scores)\n",
    "    flat_dict.update(normalized_utilization)\n",
    "    # pprint(frame)\n",
    "\n",
    "    all_flats.append(flat_dict)\n",
    "    all_scores.append(flat_dict)\n",
    "\n",
    "    fulls_frame = {\n",
    "        \"category\": split,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"sps\": obj[\"sps\"],\n",
    "    }\n",
    "    fulls_frame.update(\n",
    "        {\n",
    "            \"scores\": scores,\n",
    "            \"utilization\": obj[\"utilization\"],\n",
    "        }\n",
    "    )\n",
    "    all_fulls.append(fulls_frame)\n",
    "\n",
    "flats_frame = pd.DataFrame.from_records(all_flats)\n",
    "flats_frame.to_csv(f\"results/{model_path}.csv\", index=False)\n",
    "\n",
    "scores_frame = pd.DataFrame.from_records(all_scores)\n",
    "scores_frame.to_csv(f\"results/all-scores.csv\", index=False)\n",
    "\n",
    "fulls_dict = {\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": hardware,\n",
    "    \"total_params\": total_params,\n",
    "}\n",
    "for full in all_fulls:\n",
    "    fulls_dict[full[\"category\"]] = full\n",
    "\n",
    "print(fulls_dict)\n",
    "fulls_frame = pd.DataFrame.from_records([fulls_dict])\n",
    "fulls_frame.to_json(f\"results/{model_path}.json\", orient=\"records\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
