{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install huggingface_hub\n",
    "# %pip install transformers evaluate gradio datasets chardet cchardet librosa ipython sentencepiece plotly phonemizer\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading custom gpt2 / gpt2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large,model_id: openai-community/gpt2-large,model_path: openai-community_gpt2-large\n",
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "50256\n",
      "50256\n",
      "left\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"padding_side\": \"left\"\n",
      "}\n",
      "\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1280)\n",
      "    (wpe): Embedding(1024, 1280)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-35): 36 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "model_repo = f\"openai-community\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "model.generation_config.max_new_tokens = 0\n",
    "# model.generation_config.new_tokens = 350\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large-coedit,model_id: iliazlobin/gpt2-large-coedit,model_path: iliazlobin_gpt2-large-coedit\n",
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "50256\n",
      "50256\n",
      "left\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"iliazlobin/gpt2-large-coedit\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_new_tokens\": 350,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"padding_side\": \"left\"\n",
      "}\n",
      "\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1280)\n",
      "    (wpe): Embedding(1024, 1280)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-35): 36 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large-coedit\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "print(model.config)\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# model.generation_config.new_tokens = 350\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "model_repo = f\"openai-community\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "# print(model.config)\n",
    "# print(model)\n",
    "\n",
    "trained_model_name = \"gpt2-large-bnb8-coedit\"\n",
    "trained_model_repo = f\"iliazlobin\"\n",
    "trained_model_id = f\"{trained_model_repo}/{trained_model_name}\"\n",
    "trained_model_checkpoint = f\"{trained_model_repo}/{trained_model_name}\"\n",
    "trained_model_path = f\"{trained_model_repo}_{trained_model_name}\"\n",
    "print(\n",
    "    f\"trained_model_name: {trained_model_name},\"\n",
    "    f\"trained_model_id: {trained_model_id},\"\n",
    "    f\"trained_model_path: {trained_model_path}\"\n",
    ")\n",
    "\n",
    "\n",
    "adapters_path = f\"../model-train/model-{trained_model_repo}_{trained_model_name}\"\n",
    "peft_model = PeftModel.from_pretrained(model, adapters_path)\n",
    "\n",
    "print(type(peft_model))\n",
    "print(peft_model.config)\n",
    "print(peft_model)\n",
    "\n",
    "origin_model = model\n",
    "model = peft_model\n",
    "model_name = trained_model_name\n",
    "model_repo = trained_model_repo\n",
    "model_id = trained_model_id\n",
    "model_checkpoint = trained_model_checkpoint\n",
    "model_path = trained_model_path\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BART\n",
    "* https://huggingface.co/facebook/bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: bart-large,model_id: facebook/bart-large,model_path: facebook_bart-large\n",
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BartForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"bart-large\"\n",
    "model_repo = f\"facebook\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_id)\n",
    "tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: bart-large-coedit,model_id: iliazlobin/bart-large-coedit,model_path: iliazlobin_bart-large-coedit\n",
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BartForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"bart-large-coedit\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_id)\n",
    "tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0, max_memory={0: \"20GIB\"})\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "BartConfig {\n",
      "  \"_name_or_path\": \"iliazlobin/bart-grammarly\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"iliazlobin/bart-grammarly\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# model = BartModel.from_pretrained(model_name)\n",
    "# model = BartForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading google/t5-large\n",
    "* https://huggingface.co/google-t5/t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: t5-large,model_id: google-t5/t5-large,model_path: google-t5_t5-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-large\"\n",
    "model_repo = f\"google-t5\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_id)\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0, max_memory={0: \"20GIB\"})\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=0)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: t5-large-coedit,model_id: iliazlobin/t5-large-coedit,model_path: iliazlobin_t5-large-coedit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d1641060de4b21b6ffd8714d1576f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3494c15e859343efa132b97df1c9479b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abdf9823b054fdd9d3832e8b6065a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfd8804484b44d898f8a1f5ff4350ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa52c30d7304e318359a2a1ce419279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fde836e6aee49f0aec0f56b3b924902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a3a6b108af4149a978ef8ec7b39521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"iliazlobin/t5-large-coedit\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-large-coedit\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_id)\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=0)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading grammarly/coedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: coedit-large,model_id: grammarly/coedit-large,model_path: grammarly_coedit-large\n",
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"grammarly/coedit-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"coedit-large\"\n",
    "model_repo = f\"grammarly\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(type(tokenizer))\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gemma-2b\"\n",
    "# model_name = \"gemma-7b-it\"\n",
    "# model_name = \"gemma-7b\"\n",
    "model_repo = f\"google\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gemma-2b-coedit,model_id: iliazlobin/gemma-2b-coedit,model_path: iliazlobin_gemma-2b-coedit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "1\n",
      "0\n",
      "left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c958100cb84d138f313964a2c0621f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.modeling_gemma.GemmaForCausalLM'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from AutoGPTQ.auto_gptq import quantization\n",
    "\n",
    "model_name = \"gemma-2b-coedit\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Phi-2\n",
    "* https://huggingface.co/microsoft/phi-2\n",
    "* https://huggingface.co/TheBloke/phi-2-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: phi-2,model_id: microsoft/phi-2,model_path: microsoft_phi-2\n",
      "<class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>\n",
      "50256\n",
      "50256\n",
      "right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d3a8a3b00b449f8c4dcd8407ddc652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.phi.modeling_phi.PhiForCausalLM'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_new_tokens\": 350,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"padding_side\": \"left\"\n",
      "}\n",
      "\n",
      "PhiConfig {\n",
      "  \"_name_or_path\": \"microsoft/phi-2\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-2--configuration_phi.PhiConfig\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-2--modeling_phi.PhiForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10240,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"phi\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"partial_rotary_factor\": 0.4,\n",
      "  \"qk_layernorm\": false,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": false,\n",
      "    \"_load_in_8bit\": true,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"load_in_8bit\": true,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      ")\n",
      "Total/trainable params: 2779683840/262364160\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"phi-2\"\n",
    "model_repo = f\"microsoft\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model.config)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: phi-2-coedit,model_id: iliazlobin/phi-2-coedit,model_path: iliazlobin_phi-2-coedit\n",
      "<class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>\n",
      "50256\n",
      "50256\n",
      "left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beef24445e6453a98766c1ec88c27a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.phi.modeling_phi.PhiForCausalLM'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_new_tokens\": 350,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"padding_side\": \"left\"\n",
      "}\n",
      "\n",
      "PhiConfig {\n",
      "  \"_name_or_path\": \"iliazlobin/phi-2-coedit\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-2--configuration_phi.PhiConfig\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-2--modeling_phi.PhiForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10240,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"phi\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"partial_rotary_factor\": 0.4,\n",
      "  \"qk_layernorm\": false,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": false,\n",
      "    \"_load_in_8bit\": true,\n",
      "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"load_in_8bit\": true,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      ")\n",
      "Total/trainable params: 2779683840/262364160\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"phi-2-coedit\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, torch_dtype=torch.bfloat16, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model.config)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading mixtral-8x7B\n",
    "* https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "* https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ\n",
    "\n",
    "Quantization\n",
    "* https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from transformers import BitsAndBytesConfig\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     29\u001b[0m     model_name,\n\u001b[1;32m     30\u001b[0m     use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# tokenizer.pad_token = tokenizer.eos_token\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3039\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3039\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3043\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/quantizers/auto.py:153\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m AutoQuantizationConfig\u001b[38;5;241m.\u001b[39mfrom_dict(quantization_config)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig)) \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ config collision\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m \u001b[43mquantization_config_from_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loading_attributes\u001b[49m()\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m loading_attr_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(quantization_config, attr, val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'"
     ]
    }
   ],
   "source": [
    "# model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "model_name = 'TheBloke/Mixtral-8x7B-v0.1-GPTQ'\n",
    "model_alias = model_name.replace('/', '_')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=0,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # load_in_4bit=True,\n",
    "#     # quantization_config=quantization_config,\n",
    "# )\n",
    "\n",
    "# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\n",
    "# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# text = 'Hello my name is'\n",
    "# inputs = tokenizer(text, return_tensors='pt')\n",
    "# outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"TheBloke/Llama-2-7B-GPTQ\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": false,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_name = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)\n",
    "\n",
    "# from auto_gptq import exllama_set_max_input_length\n",
    "# model = exllama_set_max_input_length(model, max_input_length=2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly/coedit dataset\n",
    "* https://huggingface.co/datasets/grammarly/coedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set {'neutralize', 'simplification', 'paraphrase', 'coherence', 'clarity', 'gec'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 63703\n",
      "})\n",
      "test set {'neutralize', 'simplification', 'paraphrase', 'coherence', 'clarity', 'gec'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 7080\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 63703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 7080\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'], 'request': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'prompt': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "# print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "# train_ratio = 0.01\n",
    "# test_ratio = 0.001\n",
    "# train_ratio = 0.1\n",
    "# test_ratio = 0.01\n",
    "train_ratio = 0.9\n",
    "test_ratio = 0.1\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "train_gec_dataset = gec_dataset.select(range(0, int(train_ratio * len(gec_dataset))))\n",
    "test_gec_dataset = gec_dataset.select(range(int((1 - test_ratio) * len(gec_dataset)), len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, int(train_ratio * len(simplification_dataset))))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(simplification_dataset)), len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, int(train_ratio * len(clarity_dataset))))\n",
    "test_clarity_dataset = clarity_dataset.select(range(int((1 - test_ratio) * len(clarity_dataset)), len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, int(train_ratio * len(coherence_dataset))))\n",
    "test_coherence_dataset = coherence_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(coherence_dataset)), len(coherence_dataset))\n",
    ")\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, int(train_ratio * len(paraphrase_dataset))))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(paraphrase_dataset)), len(paraphrase_dataset))\n",
    ")\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(train_ratio * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, int(train_ratio * len(neutralize_dataset))))\n",
    "test_neutralize_dataset = neutralize_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(neutralize_dataset)), len(neutralize_dataset))\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "print(train_dataset)\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "def add_prompt(item):\n",
    "    return {\n",
    "        \"request\": f\"{item['input']}\\nResponse:\",\n",
    "        \"prompt\": f\"{item['input']}\\nResponse:{item['reference']}\",\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "dataset = dataset.map(add_prompt)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_length train: 171\n",
      "max_input_length test: 239\n"
     ]
    }
   ],
   "source": [
    "# find the longest sequence in the dataset\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"train\"])\n",
    "print(f\"max_input_length train: {max_input_length}\")\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"test\"])\n",
    "print(f\"max_input_length test: {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/neutralize: 10143\n",
      "train/simplification: 10296\n",
      "train/paraphrase: 14307\n",
      "train/coherence: 9554\n",
      "train/clarity: 1126\n",
      "train/gec: 18277\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    train_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    train_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "train_dataset_map = {}\n",
    "for task, l in train_lists_map.items():\n",
    "    train_dataset_map[task] = Dataset.from_list(l)\n",
    "# print(train_dataset_map)\n",
    "\n",
    "train_dataset_dict = DatasetDict(train_dataset_map)\n",
    "# print(train_dataset_dict)\n",
    "\n",
    "# for task, ds in train_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"train/{task}: {len(train_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/neutralize: 1127\n",
      "test/simplification: 1144\n",
      "test/paraphrase: 1590\n",
      "test/coherence: 1062\n",
      "test/clarity: 126\n",
      "test/gec: 2031\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    test_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    test_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "test_dataset_map = {}\n",
    "for task, l in test_lists_map.items():\n",
    "    test_dataset_map[task] = Dataset.from_list(l)\n",
    "# print(test_dataset_map)\n",
    "\n",
    "test_dataset_dict = DatasetDict(test_dataset_map)\n",
    "# print(test_dataset_dict)\n",
    "\n",
    "# for task, ds in test_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"test/{task}: {len(test_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gec\n",
      "Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.\n",
      "Despite the strict Japanese society, I feel happy when I have dinner with my family.\n",
      "Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.\n",
      "Response:Despite the strict Japanese society, I feel happy when I have dinner with my family.\n",
      "Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][0][\"task\"])\n",
    "print(dataset[\"test\"][0][\"input\"])\n",
    "print(dataset[\"test\"][0][\"reference\"])\n",
    "print(dataset[\"test\"][0][\"prompt\"])\n",
    "print(dataset[\"test\"][0][\"request\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> neutralize\n",
      "input: ['Remove non-neutral POV: new moon received poor reviews from critics.', 'Remove POVs in this text: rhonda shear (born 1954), american television personality, comedienne, and actress']\n",
      "result: ['new moon received poor reviews from critics.', 'rhonda shear (born 1954), american television personality, comedian, and actress']\n"
     ]
    }
   ],
   "source": [
    "max_batch = 2\n",
    "max_length = 350\n",
    "max_new_tokens = 100\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"input: {input_batch['input']}\")\n",
    "\n",
    "    input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    # outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "    outputs = model.generate(input.input_ids, max_new_tokens=max_new_tokens)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> neutralize\n",
      "request: ['Remove non-neutral POV: new moon received poor reviews from critics.\\nResponse:', 'Remove POVs in this text: rhonda shear (born 1954), american television personality, comedienne, and actress\\nResponse:']\n",
      "reference: ['new moon received negative reviews from critics.', 'rhonda shear (born 1954), american television personality, comedian, and actress']\n",
      "result: ['new moon received mixed reviews from critics.', 'rhonda shear (born 1954), american television personality, comedian, and actress']\n"
     ]
    }
   ],
   "source": [
    "max_batch = 2\n",
    "max_length = 350\n",
    "max_new_tokens = 350\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"request: {input_batch['request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch['request'], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    # outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    outputs = model.generate(**inputs, num_return_sequences=1)\n",
    "    # outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=10,\n",
    "    #     num_return_sequences=1,\n",
    "    #     pad_token_id=tokenizer.eos_token_id,\n",
    "    #     # return_attention_mask=True,\n",
    "    #     max_length=256,\n",
    "    # )\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total/trainable params: 737668096/737668096\n",
      "{'total_memory': 10736893952, 'memory_used': 5470830592, 'cuda_allocated': 2959717888, 'cuda_reserved': 3001024512, 'ram_usage': 15106895872}\n",
      "{'total_memory': '10.00', 'memory_used': '5.10', 'cuda_allocated': '2.76', 'cuda_reserved': '2.79', 'ram_usage': '14.07'}\n",
      "total/used/cuda/res/ram(Gb): 10.00/5.10/2.76/2.79/14.07\n",
      "Total/used/available memory (Gb): 10.00/{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\n",
      "Recommended/actual fraction: 0.49/0.95\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total/trainable params: {total_params}/{total_trainable_params}')\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "print(utilization)\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(utilization_str)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram(Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "actual_fraction = 0.95\n",
    "available_memory = utilization['total_memory'] - utilization['memory_used']\n",
    "recommended_fraction = available_memory / utilization['total_memory']\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"Total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    \"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f'Recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}')\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.empty(utilization['total_memory'] // 2, dtype=torch.int8, device='cuda')\n",
    "# print_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval encoder-decoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "total/used/cuda/res/ram (Gb): 10.00/5.10/2.76/2.77/14.08\n",
      "Processing 1127 samples for neutralize\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d72f998e8146309a9d3b28175a7aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-19/1127 | total/used/cuda/res/ram (Gb): 10.00/5.57/2.76/3.25/16.72 | batch/sps: 20/20.54\n",
      "20-39/1127 | total/used/cuda/res/ram (Gb): 10.00/5.65/2.76/3.33/16.72 | batch/sps: 20/27.30\n",
      "40-59/1127 | total/used/cuda/res/ram (Gb): 10.00/5.65/2.76/3.33/16.72 | batch/sps: 20/25.34\n",
      "60-79/1127 | total/used/cuda/res/ram (Gb): 10.00/5.65/2.76/3.33/16.72 | batch/sps: 20/32.37\n",
      "80-99/1127 | total/used/cuda/res/ram (Gb): 10.00/5.65/2.76/3.33/16.72 | batch/sps: 20/30.04\n",
      "100-119/1127 | total/used/cuda/res/ram (Gb): 10.00/5.66/2.76/3.33/16.73 | batch/sps: 20/26.21\n",
      "120-139/1127 | total/used/cuda/res/ram (Gb): 10.00/5.66/2.76/3.33/16.73 | batch/sps: 20/28.13\n",
      "140-159/1127 | total/used/cuda/res/ram (Gb): 10.00/5.66/2.76/3.33/16.73 | batch/sps: 20/31.43\n",
      "160-179/1127 | total/used/cuda/res/ram (Gb): 10.00/5.66/2.76/3.33/16.74 | batch/sps: 20/32.94\n",
      "180-199/1127 | total/used/cuda/res/ram (Gb): 10.00/5.66/2.76/3.33/16.73 | batch/sps: 20/32.22\n",
      "200-219/1127 | total/used/cuda/res/ram (Gb): 10.00/5.66/2.76/3.33/16.73 | batch/sps: 20/26.65\n",
      "220-239/1127 | total/used/cuda/res/ram (Gb): 10.00/5.66/2.76/3.33/16.73 | batch/sps: 20/32.69\n",
      "240-259/1127 | total/used/cuda/res/ram (Gb): 10.00/5.81/2.76/3.50/16.73 | batch/sps: 20/20.41\n",
      "260-279/1127 | total/used/cuda/res/ram (Gb): 10.00/6.30/2.76/3.99/16.73 | batch/sps: 20/8.58\n",
      "280-299/1127 | total/used/cuda/res/ram (Gb): 10.00/6.29/2.76/3.99/16.74 | batch/sps: 20/26.55\n",
      "300-319/1127 | total/used/cuda/res/ram (Gb): 10.00/6.29/2.76/3.99/16.74 | batch/sps: 20/28.80\n",
      "320-339/1127 | total/used/cuda/res/ram (Gb): 10.00/6.29/2.76/3.99/16.74 | batch/sps: 20/32.93\n",
      "340-359/1127 | total/used/cuda/res/ram (Gb): 10.00/6.32/2.76/3.99/16.74 | batch/sps: 20/29.19\n",
      "360-379/1127 | total/used/cuda/res/ram (Gb): 10.00/6.32/2.76/3.99/16.74 | batch/sps: 20/31.75\n",
      "380-399/1127 | total/used/cuda/res/ram (Gb): 10.00/6.34/2.76/3.99/16.74 | batch/sps: 20/26.34\n",
      "400-419/1127 | total/used/cuda/res/ram (Gb): 10.00/6.34/2.76/3.99/16.74 | batch/sps: 20/25.79\n",
      "420-439/1127 | total/used/cuda/res/ram (Gb): 10.00/6.40/2.76/4.04/16.74 | batch/sps: 20/11.93\n",
      "440-459/1127 | total/used/cuda/res/ram (Gb): 10.00/6.37/2.76/4.04/16.74 | batch/sps: 20/12.82\n",
      "460-479/1127 | total/used/cuda/res/ram (Gb): 10.00/6.61/2.76/4.32/16.74 | batch/sps: 20/8.90\n",
      "480-499/1127 | total/used/cuda/res/ram (Gb): 10.00/6.61/2.76/4.32/16.74 | batch/sps: 20/11.17\n",
      "500-519/1127 | total/used/cuda/res/ram (Gb): 10.00/6.67/2.76/4.32/16.74 | batch/sps: 20/13.44\n",
      "520-539/1127 | total/used/cuda/res/ram (Gb): 10.00/6.57/2.76/4.32/16.74 | batch/sps: 20/10.12\n",
      "540-559/1127 | total/used/cuda/res/ram (Gb): 10.00/6.57/2.76/4.32/16.75 | batch/sps: 20/11.79\n",
      "560-579/1127 | total/used/cuda/res/ram (Gb): 10.00/6.65/2.76/4.32/16.74 | batch/sps: 20/9.32\n",
      "580-599/1127 | total/used/cuda/res/ram (Gb): 10.00/6.57/2.76/4.32/16.75 | batch/sps: 20/11.47\n",
      "600-619/1127 | total/used/cuda/res/ram (Gb): 10.00/6.64/2.76/4.32/16.75 | batch/sps: 20/9.46\n",
      "620-639/1127 | total/used/cuda/res/ram (Gb): 10.00/6.69/2.76/4.32/16.76 | batch/sps: 20/9.28\n",
      "640-659/1127 | total/used/cuda/res/ram (Gb): 10.00/6.78/2.76/4.40/16.78 | batch/sps: 20/8.48\n",
      "660-679/1127 | total/used/cuda/res/ram (Gb): 10.00/6.78/2.76/4.40/16.76 | batch/sps: 20/8.01\n",
      "680-699/1127 | total/used/cuda/res/ram (Gb): 10.00/6.78/2.76/4.40/16.76 | batch/sps: 20/10.84\n",
      "700-719/1127 | total/used/cuda/res/ram (Gb): 10.00/6.90/2.76/4.40/16.78 | batch/sps: 20/10.08\n",
      "720-739/1127 | total/used/cuda/res/ram (Gb): 10.00/6.90/2.76/4.40/16.76 | batch/sps: 20/7.32\n",
      "740-759/1127 | total/used/cuda/res/ram (Gb): 10.00/6.90/2.76/4.40/16.76 | batch/sps: 20/10.00\n",
      "760-779/1127 | total/used/cuda/res/ram (Gb): 10.00/6.98/2.76/4.40/16.76 | batch/sps: 20/7.59\n",
      "780-799/1127 | total/used/cuda/res/ram (Gb): 10.00/7.02/2.76/4.40/16.76 | batch/sps: 20/9.70\n",
      "800-819/1127 | total/used/cuda/res/ram (Gb): 10.00/7.55/2.76/4.90/16.75 | batch/sps: 20/8.22\n",
      "820-839/1127 | total/used/cuda/res/ram (Gb): 10.00/7.59/2.76/4.90/16.75 | batch/sps: 20/8.74\n",
      "840-859/1127 | total/used/cuda/res/ram (Gb): 10.00/7.59/2.76/4.90/16.75 | batch/sps: 20/12.28\n",
      "860-879/1127 | total/used/cuda/res/ram (Gb): 10.00/7.63/2.76/4.90/16.74 | batch/sps: 20/8.28\n",
      "880-899/1127 | total/used/cuda/res/ram (Gb): 10.00/7.67/2.76/4.90/16.77 | batch/sps: 20/8.04\n",
      "900-919/1127 | total/used/cuda/res/ram (Gb): 10.00/7.74/2.76/4.90/16.75 | batch/sps: 20/7.81\n",
      "920-939/1127 | total/used/cuda/res/ram (Gb): 10.00/7.75/2.76/4.90/16.75 | batch/sps: 20/8.42\n",
      "940-959/1127 | total/used/cuda/res/ram (Gb): 10.00/7.74/2.76/4.90/16.75 | batch/sps: 20/11.26\n",
      "960-979/1127 | total/used/cuda/res/ram (Gb): 10.00/7.74/2.76/4.90/16.74 | batch/sps: 20/10.56\n",
      "980-999/1127 | total/used/cuda/res/ram (Gb): 10.00/7.74/2.76/4.90/16.74 | batch/sps: 20/13.94\n",
      "1000-1019/1127 | total/used/cuda/res/ram (Gb): 10.00/7.74/2.76/4.90/16.75 | batch/sps: 20/12.46\n",
      "1020-1039/1127 | total/used/cuda/res/ram (Gb): 10.00/7.74/2.76/4.90/16.75 | batch/sps: 20/10.00\n",
      "1040-1059/1127 | total/used/cuda/res/ram (Gb): 10.00/7.79/2.76/4.90/16.75 | batch/sps: 20/8.50\n",
      "1060-1079/1127 | total/used/cuda/res/ram (Gb): 10.00/7.80/2.76/4.90/16.75 | batch/sps: 20/8.76\n",
      "1080-1099/1127 | total/used/cuda/res/ram (Gb): 10.00/7.84/2.76/4.90/16.75 | batch/sps: 20/9.37\n",
      "1100-1119/1127 | total/used/cuda/res/ram (Gb): 10.00/7.87/2.76/4.90/16.75 | batch/sps: 20/13.57\n",
      "1120-1126/1127 | total/used/cuda/res/ram (Gb): 10.00/7.88/2.76/4.90/16.74 | batch/sps: 7/4.24\n",
      "Finished processing 1127 samples for neutralize.\n",
      "1127 | total/used/cuda/res/ram (Gb): 10.00/7.88/2.76/4.90/15.19 | sps: 11.32\n",
      "Processing 1144 samples for simplification\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc883ffc8f7c49e08ccd53650510d767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-19/1144 | total/used/cuda/res/ram (Gb): 10.00/7.84/2.76/4.90/19.29 | batch/sps: 20/11.24\n",
      "20-39/1144 | total/used/cuda/res/ram (Gb): 10.00/7.80/2.76/4.90/19.31 | batch/sps: 20/14.74\n",
      "40-59/1144 | total/used/cuda/res/ram (Gb): 10.00/7.79/2.76/4.90/19.30 | batch/sps: 20/15.45\n",
      "60-79/1144 | total/used/cuda/res/ram (Gb): 10.00/7.81/2.76/4.90/19.30 | batch/sps: 20/14.41\n",
      "80-99/1144 | total/used/cuda/res/ram (Gb): 10.00/7.87/2.76/4.90/19.30 | batch/sps: 20/15.20\n",
      "100-119/1144 | total/used/cuda/res/ram (Gb): 10.00/7.87/2.76/4.90/19.30 | batch/sps: 20/18.12\n",
      "120-139/1144 | total/used/cuda/res/ram (Gb): 10.00/7.87/2.76/4.90/19.30 | batch/sps: 20/12.99\n",
      "140-159/1144 | total/used/cuda/res/ram (Gb): 10.00/7.83/2.76/4.90/19.30 | batch/sps: 20/17.44\n",
      "160-179/1144 | total/used/cuda/res/ram (Gb): 10.00/7.83/2.76/4.90/19.30 | batch/sps: 20/24.91\n",
      "180-199/1144 | total/used/cuda/res/ram (Gb): 10.00/7.83/2.76/4.90/19.30 | batch/sps: 20/22.61\n",
      "200-219/1144 | total/used/cuda/res/ram (Gb): 10.00/7.81/2.76/4.90/19.30 | batch/sps: 20/16.78\n",
      "220-239/1144 | total/used/cuda/res/ram (Gb): 10.00/7.82/2.76/4.90/19.30 | batch/sps: 20/21.28\n",
      "240-259/1144 | total/used/cuda/res/ram (Gb): 10.00/7.83/2.76/4.90/19.30 | batch/sps: 20/16.69\n",
      "260-279/1144 | total/used/cuda/res/ram (Gb): 10.00/7.85/2.76/4.90/19.30 | batch/sps: 20/12.74\n",
      "280-299/1144 | total/used/cuda/res/ram (Gb): 10.00/7.85/2.76/4.90/19.30 | batch/sps: 20/17.08\n",
      "300-319/1144 | total/used/cuda/res/ram (Gb): 10.00/7.85/2.76/4.90/19.30 | batch/sps: 20/20.14\n",
      "320-339/1144 | total/used/cuda/res/ram (Gb): 10.00/7.85/2.76/4.90/19.30 | batch/sps: 20/23.20\n",
      "340-359/1144 | total/used/cuda/res/ram (Gb): 10.00/7.85/2.76/4.90/19.31 | batch/sps: 20/15.37\n",
      "360-379/1144 | total/used/cuda/res/ram (Gb): 10.00/7.84/2.76/4.90/19.31 | batch/sps: 20/18.93\n",
      "380-399/1144 | total/used/cuda/res/ram (Gb): 10.00/7.84/2.76/4.90/19.32 | batch/sps: 20/19.87\n",
      "400-419/1144 | total/used/cuda/res/ram (Gb): 10.00/7.84/2.76/4.90/19.31 | batch/sps: 20/18.40\n",
      "420-439/1144 | total/used/cuda/res/ram (Gb): 10.00/7.84/2.76/4.90/19.31 | batch/sps: 20/11.22\n",
      "440-459/1144 | total/used/cuda/res/ram (Gb): 10.00/7.87/2.76/4.90/19.32 | batch/sps: 20/13.92\n",
      "460-479/1144 | total/used/cuda/res/ram (Gb): 10.00/8.08/2.76/5.06/19.31 | batch/sps: 20/7.54\n",
      "480-499/1144 | total/used/cuda/res/ram (Gb): 10.00/8.08/2.76/5.06/19.31 | batch/sps: 20/22.54\n",
      "500-519/1144 | total/used/cuda/res/ram (Gb): 10.00/8.05/2.76/5.06/19.31 | batch/sps: 20/14.92\n",
      "520-539/1144 | total/used/cuda/res/ram (Gb): 10.00/8.05/2.76/5.06/19.31 | batch/sps: 20/16.63\n",
      "540-559/1144 | total/used/cuda/res/ram (Gb): 10.00/8.05/2.76/5.06/19.30 | batch/sps: 20/19.64\n",
      "560-579/1144 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.06/19.32 | batch/sps: 20/15.01\n",
      "580-599/1144 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.06/19.31 | batch/sps: 20/29.99\n",
      "600-619/1144 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.06/19.32 | batch/sps: 20/17.32\n",
      "620-639/1144 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.06/19.31 | batch/sps: 20/20.15\n",
      "640-659/1144 | total/used/cuda/res/ram (Gb): 10.00/7.99/2.76/5.06/19.32 | batch/sps: 20/14.56\n",
      "660-679/1144 | total/used/cuda/res/ram (Gb): 10.00/7.99/2.76/5.06/19.31 | batch/sps: 20/19.04\n",
      "680-699/1144 | total/used/cuda/res/ram (Gb): 10.00/7.99/2.76/5.06/19.31 | batch/sps: 20/19.46\n",
      "700-719/1144 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.06/19.31 | batch/sps: 20/17.97\n",
      "720-739/1144 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.06/19.30 | batch/sps: 20/14.57\n",
      "740-759/1144 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.06/19.32 | batch/sps: 20/12.91\n",
      "760-779/1144 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.06/19.30 | batch/sps: 20/15.21\n",
      "780-799/1144 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.06/19.31 | batch/sps: 20/21.20\n",
      "800-819/1144 | total/used/cuda/res/ram (Gb): 10.00/7.99/2.76/5.06/19.31 | batch/sps: 20/17.78\n",
      "820-839/1144 | total/used/cuda/res/ram (Gb): 10.00/7.99/2.76/5.06/19.31 | batch/sps: 20/17.40\n",
      "840-859/1144 | total/used/cuda/res/ram (Gb): 10.00/7.99/2.76/5.06/19.31 | batch/sps: 20/16.28\n",
      "860-879/1144 | total/used/cuda/res/ram (Gb): 10.00/7.99/2.76/5.06/19.31 | batch/sps: 20/20.52\n",
      "880-899/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/26.09\n",
      "900-919/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.32 | batch/sps: 20/23.33\n",
      "920-939/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/23.58\n",
      "940-959/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.32 | batch/sps: 20/19.21\n",
      "960-979/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/20.78\n",
      "980-999/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/13.36\n",
      "1000-1019/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/22.42\n",
      "1020-1039/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/14.53\n",
      "1040-1059/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/22.30\n",
      "1060-1079/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/25.47\n",
      "1080-1099/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.31 | batch/sps: 20/13.99\n",
      "1100-1119/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.30 | batch/sps: 20/24.20\n",
      "1120-1139/1144 | total/used/cuda/res/ram (Gb): 10.00/7.98/2.76/5.06/19.30 | batch/sps: 20/19.89\n",
      "1140-1143/1144 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.35 | batch/sps: 4/4.15\n",
      "Finished processing 1144 samples for simplification.\n",
      "1144 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.35 | sps: 15.81\n",
      "Processing 1590 samples for paraphrase\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f42c07e577243709a0b88a6f3238ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-19/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/18.14\n",
      "20-39/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.31 | batch/sps: 20/21.11\n",
      "40-59/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.31 | batch/sps: 20/16.20\n",
      "60-79/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/13.87\n",
      "80-99/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/14.37\n",
      "100-119/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/19.97\n",
      "120-139/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.32 | batch/sps: 20/15.89\n",
      "140-159/1590 | total/used/cuda/res/ram (Gb): 10.00/8.07/2.76/5.10/19.30 | batch/sps: 20/19.68\n",
      "160-179/1590 | total/used/cuda/res/ram (Gb): 10.00/8.07/2.76/5.10/19.30 | batch/sps: 20/15.78\n",
      "180-199/1590 | total/used/cuda/res/ram (Gb): 10.00/8.10/2.76/5.10/19.29 | batch/sps: 20/22.55\n",
      "200-219/1590 | total/used/cuda/res/ram (Gb): 10.00/8.11/2.76/5.10/19.30 | batch/sps: 20/22.23\n",
      "220-239/1590 | total/used/cuda/res/ram (Gb): 10.00/8.11/2.76/5.10/19.29 | batch/sps: 20/15.77\n",
      "240-259/1590 | total/used/cuda/res/ram (Gb): 10.00/8.10/2.76/5.10/19.29 | batch/sps: 20/16.62\n",
      "260-279/1590 | total/used/cuda/res/ram (Gb): 10.00/8.11/2.76/5.10/19.30 | batch/sps: 20/17.27\n",
      "280-299/1590 | total/used/cuda/res/ram (Gb): 10.00/8.10/2.76/5.10/19.29 | batch/sps: 20/21.45\n",
      "300-319/1590 | total/used/cuda/res/ram (Gb): 10.00/8.10/2.76/5.10/19.29 | batch/sps: 20/16.06\n",
      "320-339/1590 | total/used/cuda/res/ram (Gb): 10.00/8.09/2.76/5.10/19.29 | batch/sps: 20/19.29\n",
      "340-359/1590 | total/used/cuda/res/ram (Gb): 10.00/8.09/2.76/5.10/19.29 | batch/sps: 20/24.83\n",
      "360-379/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.31 | batch/sps: 20/21.30\n",
      "380-399/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/17.55\n",
      "400-419/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/15.31\n",
      "420-439/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/17.35\n",
      "440-459/1590 | total/used/cuda/res/ram (Gb): 10.00/8.08/2.76/5.10/19.29 | batch/sps: 20/15.88\n",
      "460-479/1590 | total/used/cuda/res/ram (Gb): 10.00/8.09/2.76/5.10/19.29 | batch/sps: 20/16.87\n",
      "480-499/1590 | total/used/cuda/res/ram (Gb): 10.00/8.09/2.76/5.10/19.29 | batch/sps: 20/20.48\n",
      "500-519/1590 | total/used/cuda/res/ram (Gb): 10.00/8.09/2.76/5.10/19.29 | batch/sps: 20/18.31\n",
      "520-539/1590 | total/used/cuda/res/ram (Gb): 10.00/8.12/2.76/5.10/19.29 | batch/sps: 20/19.97\n",
      "540-559/1590 | total/used/cuda/res/ram (Gb): 10.00/8.12/2.76/5.10/19.29 | batch/sps: 20/17.36\n",
      "560-579/1590 | total/used/cuda/res/ram (Gb): 10.00/8.12/2.76/5.10/19.29 | batch/sps: 20/15.08\n",
      "580-599/1590 | total/used/cuda/res/ram (Gb): 10.00/8.14/2.76/5.10/19.30 | batch/sps: 20/14.97\n",
      "600-619/1590 | total/used/cuda/res/ram (Gb): 10.00/8.11/2.76/5.10/19.30 | batch/sps: 20/16.86\n",
      "620-639/1590 | total/used/cuda/res/ram (Gb): 10.00/8.11/2.76/5.10/19.31 | batch/sps: 20/19.59\n",
      "640-659/1590 | total/used/cuda/res/ram (Gb): 10.00/8.11/2.76/5.10/19.29 | batch/sps: 20/12.88\n",
      "660-679/1590 | total/used/cuda/res/ram (Gb): 10.00/8.05/2.76/5.10/19.29 | batch/sps: 20/13.00\n",
      "680-699/1590 | total/used/cuda/res/ram (Gb): 10.00/8.05/2.76/5.10/19.29 | batch/sps: 20/20.24\n",
      "700-719/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/16.14\n",
      "720-739/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/14.19\n",
      "740-759/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/13.24\n",
      "760-779/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.30 | batch/sps: 20/12.47\n",
      "780-799/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.30 | batch/sps: 20/17.75\n",
      "800-819/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.30 | batch/sps: 20/15.69\n",
      "820-839/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/15.50\n",
      "840-859/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/18.91\n",
      "860-879/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/13.62\n",
      "880-899/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.30 | batch/sps: 20/12.94\n",
      "900-919/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.30 | batch/sps: 20/12.82\n",
      "920-939/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.30 | batch/sps: 20/12.06\n",
      "940-959/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/13.29\n",
      "960-979/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/11.56\n",
      "980-999/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/8.83\n",
      "1000-1019/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.30 | batch/sps: 20/14.78\n",
      "1020-1039/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/10.77\n",
      "1040-1059/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/8.22\n",
      "1060-1079/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/14.29\n",
      "1080-1099/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/21.93\n",
      "1100-1119/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/36.61\n",
      "1120-1139/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/29.77\n",
      "1140-1159/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/25.02\n",
      "1160-1179/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/26.38\n",
      "1180-1199/1590 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.76/5.10/19.29 | batch/sps: 20/22.89\n",
      "1200-1219/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/29.67\n",
      "1220-1239/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/29.23\n",
      "1240-1259/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/25.66\n",
      "1260-1279/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/41.35\n",
      "1280-1299/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/28.01\n",
      "1300-1319/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/29.13\n",
      "1320-1339/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/34.92\n",
      "1340-1359/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/25.55\n",
      "1360-1379/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/40.69\n",
      "1380-1399/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/33.83\n",
      "1400-1419/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/22.51\n",
      "1420-1439/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/34.57\n",
      "1440-1459/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/25.47\n",
      "1460-1479/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/13.05\n",
      "1480-1499/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/35.47\n",
      "1500-1519/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/30.89\n",
      "1520-1539/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/22.03\n",
      "1540-1559/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/39.99\n",
      "1560-1579/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/25.33\n",
      "1580-1589/1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 10/15.25\n",
      "Finished processing 1590 samples for paraphrase.\n",
      "1590 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | sps: 17.14\n",
      "Processing 1062 samples for coherence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ae9a060d624210809269dabd0483b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-19/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/13.08\n",
      "20-39/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/15.56\n",
      "40-59/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/17.91\n",
      "60-79/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/12.71\n",
      "80-99/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/13.12\n",
      "100-119/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/11.31\n",
      "120-139/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/13.72\n",
      "140-159/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.31 | batch/sps: 20/13.91\n",
      "160-179/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.29 | batch/sps: 20/12.78\n",
      "180-199/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/15.90\n",
      "200-219/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/16.81\n",
      "220-239/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/11.88\n",
      "240-259/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/13.19\n",
      "260-279/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/12.71\n",
      "280-299/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/14.28\n",
      "300-319/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/14.17\n",
      "320-339/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/15.38\n",
      "340-359/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/14.84\n",
      "360-379/1062 | total/used/cuda/res/ram (Gb): 10.00/8.03/2.76/5.10/19.30 | batch/sps: 20/17.43\n",
      "380-399/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/14.61\n",
      "400-419/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/13.27\n",
      "420-439/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/12.22\n",
      "440-459/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/16.86\n",
      "460-479/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/14.14\n",
      "480-499/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/13.86\n",
      "500-519/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/16.04\n",
      "520-539/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/14.63\n",
      "540-559/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/15.48\n",
      "560-579/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/14.72\n",
      "580-599/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/13.19\n",
      "600-619/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.31 | batch/sps: 20/12.32\n",
      "620-639/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/16.70\n",
      "640-659/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/13.26\n",
      "660-679/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/15.01\n",
      "680-699/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/11.90\n",
      "700-719/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.32 | batch/sps: 20/15.28\n",
      "720-739/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/10.97\n",
      "740-759/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/11.82\n",
      "760-779/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/12.11\n",
      "780-799/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/13.82\n",
      "800-819/1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/12.61\n",
      "820-839/1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.31 | batch/sps: 20/13.80\n",
      "840-859/1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/16.49\n",
      "860-879/1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/13.46\n",
      "880-899/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/12.81\n",
      "900-919/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/14.93\n",
      "920-939/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.32 | batch/sps: 20/12.13\n",
      "940-959/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/15.51\n",
      "960-979/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/15.24\n",
      "980-999/1062 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/16.02\n",
      "1000-1019/1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/14.06\n",
      "1020-1039/1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/12.69\n",
      "1040-1059/1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/15.96\n",
      "1060-1061/1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 2/2.26\n",
      "Finished processing 1062 samples for coherence.\n",
      "1062 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | sps: 13.16\n",
      "Processing 126 samples for clarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc212cac03c4dcea5c83409acfdabca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-19/126 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/9.06\n",
      "20-39/126 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.28 | batch/sps: 20/8.71\n",
      "40-59/126 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/10.34\n",
      "60-79/126 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/9.08\n",
      "80-99/126 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/10.33\n",
      "100-119/126 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/9.76\n",
      "120-125/126 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.32 | batch/sps: 6/3.23\n",
      "Finished processing 126 samples for clarity.\n",
      "126 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.32 | sps: 6.98\n",
      "Processing 2031 samples for gec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f98b47e398342f192d23a0c8ba1d892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-19/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.31 | batch/sps: 20/20.47\n",
      "20-39/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/18.85\n",
      "40-59/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/23.79\n",
      "60-79/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/40.78\n",
      "80-99/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/36.09\n",
      "100-119/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.31 | batch/sps: 20/34.27\n",
      "120-139/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/27.51\n",
      "140-159/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/30.90\n",
      "160-179/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/25.17\n",
      "180-199/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/21.73\n",
      "200-219/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/29.97\n",
      "220-239/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/32.96\n",
      "240-259/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/30.51\n",
      "260-279/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/32.37\n",
      "280-299/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/24.43\n",
      "300-319/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/34.67\n",
      "320-339/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/33.76\n",
      "340-359/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/18.63\n",
      "360-379/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/29.47\n",
      "380-399/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/18.75\n",
      "400-419/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.30 | batch/sps: 20/28.19\n",
      "420-439/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/30.29\n",
      "440-459/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/8.80\n",
      "460-479/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/28.20\n",
      "480-499/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/34.03\n",
      "500-519/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/39.91\n",
      "520-539/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/25.42\n",
      "540-559/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.76/5.10/19.29 | batch/sps: 20/21.80\n",
      "560-579/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/29.65\n",
      "580-599/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/30.86\n",
      "600-619/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/19.09\n",
      "620-639/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.31 | batch/sps: 20/24.42\n",
      "640-659/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/23.12\n",
      "660-679/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/19.52\n",
      "680-699/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/24.47\n",
      "700-719/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/24.08\n",
      "720-739/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.31 | batch/sps: 20/32.14\n",
      "740-759/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/37.06\n",
      "760-779/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/30.10\n",
      "780-799/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.31 | batch/sps: 20/24.77\n",
      "800-819/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/28.73\n",
      "820-839/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/25.68\n",
      "840-859/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/26.66\n",
      "860-879/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/25.08\n",
      "880-899/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/20.60\n",
      "900-919/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/33.56\n",
      "920-939/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/32.53\n",
      "940-959/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/19.22\n",
      "960-979/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/29.62\n",
      "980-999/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/19.73\n",
      "1000-1019/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/24.14\n",
      "1020-1039/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/18.74\n",
      "1040-1059/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/38.83\n",
      "1060-1079/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/26.19\n",
      "1080-1099/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/24.42\n",
      "1100-1119/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/21.65\n",
      "1120-1139/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/21.93\n",
      "1140-1159/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/31.83\n",
      "1160-1179/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/22.08\n",
      "1180-1199/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/22.21\n",
      "1200-1219/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/36.95\n",
      "1220-1239/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/33.91\n",
      "1240-1259/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/37.20\n",
      "1260-1279/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/27.78\n",
      "1280-1299/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/31.59\n",
      "1300-1319/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.28 | batch/sps: 20/24.51\n",
      "1320-1339/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.30 | batch/sps: 20/28.12\n",
      "1340-1359/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/31.71\n",
      "1360-1379/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.29 | batch/sps: 20/26.93\n",
      "1380-1399/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.28 | batch/sps: 20/24.65\n",
      "1400-1419/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.28 | batch/sps: 20/33.12\n",
      "1420-1439/2031 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.10/19.28 | batch/sps: 20/25.06\n",
      "1440-1459/2031 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.10/19.29 | batch/sps: 20/25.54\n",
      "1460-1479/2031 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.10/19.29 | batch/sps: 20/23.86\n",
      "1480-1499/2031 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.10/19.29 | batch/sps: 20/25.52\n",
      "1500-1519/2031 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.10/19.29 | batch/sps: 20/26.09\n",
      "1520-1539/2031 | total/used/cuda/res/ram (Gb): 10.00/8.00/2.76/5.10/19.29 | batch/sps: 20/34.31\n",
      "1540-1559/2031 | total/used/cuda/res/ram (Gb): 10.00/8.01/2.76/5.10/19.32 | batch/sps: 20/8.03\n",
      "1560-1579/2031 | total/used/cuda/res/ram (Gb): 10.00/8.07/2.76/5.10/19.28 | batch/sps: 20/8.11\n",
      "1580-1599/2031 | total/used/cuda/res/ram (Gb): 10.00/8.07/2.76/5.10/19.31 | batch/sps: 20/7.80\n",
      "1600-1619/2031 | total/used/cuda/res/ram (Gb): 10.00/6.59/2.76/6.42/19.30 | batch/sps: 20/1.30\n",
      "1620-1639/2031 | total/used/cuda/res/ram (Gb): 10.00/7.24/2.76/6.51/19.29 | batch/sps: 20/7.81\n",
      "1640-1659/2031 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.76/6.51/19.32 | batch/sps: 20/7.86\n",
      "1660-1679/2031 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.76/6.51/19.36 | batch/sps: 20/8.98\n",
      "1680-1699/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.76/6.51/19.44 | batch/sps: 20/7.93\n",
      "1700-1719/2031 | total/used/cuda/res/ram (Gb): 10.00/7.43/2.76/6.51/19.43 | batch/sps: 20/8.20\n",
      "1720-1739/2031 | total/used/cuda/res/ram (Gb): 10.00/7.46/2.76/6.51/19.44 | batch/sps: 20/8.14\n",
      "1740-1759/2031 | total/used/cuda/res/ram (Gb): 10.00/7.51/2.76/6.51/19.36 | batch/sps: 20/7.79\n",
      "1760-1779/2031 | total/used/cuda/res/ram (Gb): 10.00/7.49/2.76/6.51/19.37 | batch/sps: 20/8.18\n",
      "1780-1799/2031 | total/used/cuda/res/ram (Gb): 10.00/7.50/2.76/6.51/19.37 | batch/sps: 20/8.28\n",
      "1800-1819/2031 | total/used/cuda/res/ram (Gb): 10.00/7.46/2.76/6.51/19.37 | batch/sps: 20/8.05\n",
      "1820-1839/2031 | total/used/cuda/res/ram (Gb): 10.00/8.67/2.76/6.51/19.37 | batch/sps: 20/10.55\n",
      "1840-1859/2031 | total/used/cuda/res/ram (Gb): 10.00/8.64/2.76/6.51/19.37 | batch/sps: 20/8.16\n",
      "1860-1879/2031 | total/used/cuda/res/ram (Gb): 10.00/8.63/2.76/6.51/19.37 | batch/sps: 20/8.39\n",
      "1880-1899/2031 | total/used/cuda/res/ram (Gb): 10.00/8.72/2.76/6.51/19.29 | batch/sps: 20/7.99\n",
      "1900-1919/2031 | total/used/cuda/res/ram (Gb): 10.00/8.79/2.76/6.51/19.30 | batch/sps: 20/7.97\n",
      "1920-1939/2031 | total/used/cuda/res/ram (Gb): 10.00/8.88/2.76/6.51/19.30 | batch/sps: 20/7.85\n",
      "1940-1959/2031 | total/used/cuda/res/ram (Gb): 10.00/8.89/2.76/6.51/19.30 | batch/sps: 20/8.15\n",
      "1960-1979/2031 | total/used/cuda/res/ram (Gb): 10.00/8.85/2.76/6.51/19.30 | batch/sps: 20/7.66\n",
      "1980-1999/2031 | total/used/cuda/res/ram (Gb): 10.00/9.12/2.76/6.79/19.31 | batch/sps: 20/7.44\n",
      "2000-2019/2031 | total/used/cuda/res/ram (Gb): 10.00/9.15/2.76/6.79/19.29 | batch/sps: 20/8.61\n",
      "2020-2030/2031 | total/used/cuda/res/ram (Gb): 10.00/9.14/2.76/6.79/19.29 | batch/sps: 11/4.55\n",
      "Finished processing 2031 samples for gec.\n",
      "2031 | total/used/cuda/res/ram (Gb): 10.00/9.14/2.76/6.79/19.29 | sps: 14.73\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 2031\n",
      "})\n",
      "['Despite strict japanese society, I feel happy when I had dinner with my '\n",
      " 'family.',\n",
      " 'They are increasing rapidly in Japan for a couple of years.']\n",
      "CPU times: user 7min 23s, sys: 58.3 s, total: 8min 21s\n",
      "Wall time: 8min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 20 # home: t5, grammarly/coedit\n",
    "# batch_size = 100\n",
    "max_length = 350\n",
    "\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"input\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    inputs = tokenizer(batch[\"input\"], max_length=max_length, padding=True, return_tensors=\"pt\").to(device)\n",
    "    # input_ids = tokenizer(batch['task'], return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(item['task'], return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(input_ids, max_length=512)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    # print(f\"outputs: {outputs}\")\n",
    "    processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    return {\"processed\": processed}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for task, samples in test_dataset_dict.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {task}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {task}.\")\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[task] = {\n",
    "        # \"task\": task,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"elapsed_time\": elapsed_time,\n",
    "        \"sps\": sps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_length\": max_length,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_gec_samples = processed_samples_map[\"gec\"][\"samples\"]\n",
    "\n",
    "pprint(processed_gec_samples)\n",
    "pprint(processed_gec_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval decoder-only models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'fn_kwargs'={'model': PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      "), 'tokenizer': CodeGenTokenizerFast(name_or_path='iliazlobin/phi-2-coedit', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50257: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50258: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50259: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50260: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50261: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50262: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50263: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50264: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50265: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50266: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50267: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50268: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50269: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50270: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50271: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50272: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50273: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50274: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50275: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50276: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50277: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50278: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50279: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50280: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50281: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50282: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50283: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50284: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50285: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50286: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50287: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50288: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50289: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50290: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50291: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50292: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50293: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50294: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "}, 'total_samples': 1127} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "total/used/cuda/res/ram (Gb): 10.00/4.82/2.92/3.02/13.85\n",
      "Processing 1127 samples for neutralize\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5dc09a11ad4360a53ecc0af36d802d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/1127 | total/used/cuda/res/ram (Gb): 10.00/5.20/2.92/3.38/13.85 | batch/sps: 10/2.54\n",
      "10-19/1127 | total/used/cuda/res/ram (Gb): 10.00/5.26/2.92/3.45/13.84 | batch/sps: 10/2.50\n",
      "20-29/1127 | total/used/cuda/res/ram (Gb): 10.00/5.24/2.92/3.45/13.84 | batch/sps: 10/3.08\n",
      "30-39/1127 | total/used/cuda/res/ram (Gb): 10.00/5.30/2.92/3.53/13.84 | batch/sps: 10/2.44\n",
      "40-49/1127 | total/used/cuda/res/ram (Gb): 10.00/5.25/2.92/3.53/13.84 | batch/sps: 10/2.27\n",
      "50-59/1127 | total/used/cuda/res/ram (Gb): 10.00/5.25/2.92/3.53/13.85 | batch/sps: 10/2.65\n",
      "60-69/1127 | total/used/cuda/res/ram (Gb): 10.00/5.24/2.92/3.53/13.84 | batch/sps: 10/2.82\n",
      "70-79/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.84 | batch/sps: 10/3.23\n",
      "80-89/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.84 | batch/sps: 10/2.52\n",
      "90-99/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.85 | batch/sps: 10/3.39\n",
      "100-109/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.84 | batch/sps: 10/3.47\n",
      "110-119/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.85 | batch/sps: 10/2.58\n",
      "120-129/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.85 | batch/sps: 10/2.69\n",
      "130-139/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.84 | batch/sps: 10/2.51\n",
      "140-149/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.85 | batch/sps: 10/3.42\n",
      "150-159/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.85 | batch/sps: 10/3.39\n",
      "160-169/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.85 | batch/sps: 10/3.74\n",
      "170-179/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.85 | batch/sps: 10/3.12\n",
      "180-189/1127 | total/used/cuda/res/ram (Gb): 10.00/5.22/2.92/3.53/13.85 | batch/sps: 10/2.81\n",
      "190-199/1127 | total/used/cuda/res/ram (Gb): 10.00/5.23/2.92/3.53/13.85 | batch/sps: 10/2.64\n",
      "200-209/1127 | total/used/cuda/res/ram (Gb): 10.00/5.23/2.92/3.53/13.85 | batch/sps: 10/2.41\n",
      "210-219/1127 | total/used/cuda/res/ram (Gb): 10.00/5.23/2.92/3.53/13.86 | batch/sps: 10/3.69\n",
      "220-229/1127 | total/used/cuda/res/ram (Gb): 10.00/5.23/2.92/3.53/13.86 | batch/sps: 10/2.57\n",
      "230-239/1127 | total/used/cuda/res/ram (Gb): 10.00/5.23/2.92/3.53/13.86 | batch/sps: 10/3.54\n",
      "240-249/1127 | total/used/cuda/res/ram (Gb): 10.00/5.23/2.92/3.53/13.85 | batch/sps: 10/2.47\n",
      "250-259/1127 | total/used/cuda/res/ram (Gb): 10.00/5.31/2.92/3.62/13.85 | batch/sps: 10/1.62\n",
      "260-269/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.86 | batch/sps: 10/2.54\n",
      "270-279/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.85 | batch/sps: 10/3.33\n",
      "280-289/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.85 | batch/sps: 10/2.96\n",
      "290-299/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.87 | batch/sps: 10/2.59\n",
      "300-309/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.84 | batch/sps: 10/2.64\n",
      "310-319/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.85 | batch/sps: 10/3.42\n",
      "320-329/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.84 | batch/sps: 10/3.02\n",
      "330-339/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.85 | batch/sps: 10/3.23\n",
      "340-349/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.84 | batch/sps: 10/2.57\n",
      "350-359/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.84 | batch/sps: 10/2.79\n",
      "360-369/1127 | total/used/cuda/res/ram (Gb): 10.00/5.32/2.92/3.62/13.85 | batch/sps: 10/2.47\n",
      "370-379/1127 | total/used/cuda/res/ram (Gb): 10.00/5.35/2.92/3.62/13.86 | batch/sps: 10/2.25\n",
      "380-389/1127 | total/used/cuda/res/ram (Gb): 10.00/5.36/2.92/3.62/13.85 | batch/sps: 10/2.61\n",
      "390-399/1127 | total/used/cuda/res/ram (Gb): 10.00/5.34/2.92/3.62/13.83 | batch/sps: 10/1.86\n",
      "400-409/1127 | total/used/cuda/res/ram (Gb): 10.00/5.39/2.92/3.62/13.87 | batch/sps: 10/2.44\n",
      "410-419/1127 | total/used/cuda/res/ram (Gb): 10.00/5.42/2.92/3.62/13.88 | batch/sps: 10/2.26\n",
      "420-429/1127 | total/used/cuda/res/ram (Gb): 10.00/5.68/2.92/3.89/13.88 | batch/sps: 10/0.95\n",
      "430-439/1127 | total/used/cuda/res/ram (Gb): 10.00/5.84/2.92/4.09/13.87 | batch/sps: 10/0.93\n",
      "440-449/1127 | total/used/cuda/res/ram (Gb): 10.00/5.88/2.92/4.09/13.86 | batch/sps: 10/1.28\n",
      "450-459/1127 | total/used/cuda/res/ram (Gb): 10.00/6.03/2.92/4.24/13.86 | batch/sps: 10/0.91\n",
      "460-469/1127 | total/used/cuda/res/ram (Gb): 10.00/6.23/2.92/4.43/13.83 | batch/sps: 10/0.73\n",
      "470-479/1127 | total/used/cuda/res/ram (Gb): 10.00/6.28/2.92/4.43/13.83 | batch/sps: 10/1.27\n",
      "480-489/1127 | total/used/cuda/res/ram (Gb): 10.00/6.26/2.92/4.43/13.83 | batch/sps: 10/0.92\n",
      "490-499/1127 | total/used/cuda/res/ram (Gb): 10.00/6.21/2.92/4.43/13.83 | batch/sps: 10/1.40\n",
      "500-509/1127 | total/used/cuda/res/ram (Gb): 10.00/6.20/2.92/4.43/13.83 | batch/sps: 10/1.31\n",
      "510-519/1127 | total/used/cuda/res/ram (Gb): 10.00/6.20/2.92/4.43/13.84 | batch/sps: 10/1.10\n",
      "520-529/1127 | total/used/cuda/res/ram (Gb): 10.00/6.23/2.92/4.43/13.83 | batch/sps: 10/0.88\n",
      "530-539/1127 | total/used/cuda/res/ram (Gb): 10.00/6.23/2.92/4.43/13.85 | batch/sps: 10/0.99\n",
      "540-549/1127 | total/used/cuda/res/ram (Gb): 10.00/6.23/2.92/4.43/13.83 | batch/sps: 10/1.16\n",
      "550-559/1127 | total/used/cuda/res/ram (Gb): 10.00/6.21/2.92/4.43/13.83 | batch/sps: 10/0.89\n",
      "560-569/1127 | total/used/cuda/res/ram (Gb): 10.00/6.21/2.92/4.43/13.83 | batch/sps: 10/0.67\n",
      "570-579/1127 | total/used/cuda/res/ram (Gb): 10.00/6.22/2.92/4.43/13.84 | batch/sps: 10/0.95\n",
      "580-589/1127 | total/used/cuda/res/ram (Gb): 10.00/6.22/2.92/4.43/13.81 | batch/sps: 10/0.98\n",
      "590-599/1127 | total/used/cuda/res/ram (Gb): 10.00/6.20/2.92/4.43/13.82 | batch/sps: 10/1.04\n",
      "600-609/1127 | total/used/cuda/res/ram (Gb): 10.00/6.15/2.92/4.43/13.81 | batch/sps: 10/0.77\n",
      "610-619/1127 | total/used/cuda/res/ram (Gb): 10.00/6.18/2.92/4.43/13.82 | batch/sps: 10/1.27\n",
      "620-629/1127 | total/used/cuda/res/ram (Gb): 10.00/6.24/2.92/4.43/13.80 | batch/sps: 10/1.18\n",
      "630-639/1127 | total/used/cuda/res/ram (Gb): 10.00/6.49/2.92/4.68/13.80 | batch/sps: 10/0.60\n",
      "640-649/1127 | total/used/cuda/res/ram (Gb): 10.00/6.65/2.92/4.91/13.79 | batch/sps: 10/0.57\n",
      "650-659/1127 | total/used/cuda/res/ram (Gb): 10.00/6.64/2.92/4.91/13.80 | batch/sps: 10/1.01\n",
      "660-669/1127 | total/used/cuda/res/ram (Gb): 10.00/6.65/2.92/4.91/13.80 | batch/sps: 10/1.03\n",
      "670-679/1127 | total/used/cuda/res/ram (Gb): 10.00/6.66/2.92/4.91/13.80 | batch/sps: 10/0.62\n",
      "680-689/1127 | total/used/cuda/res/ram (Gb): 10.00/6.74/2.92/4.91/13.80 | batch/sps: 10/0.97\n",
      "690-699/1127 | total/used/cuda/res/ram (Gb): 10.00/6.66/2.92/4.91/13.80 | batch/sps: 10/0.81\n",
      "700-709/1127 | total/used/cuda/res/ram (Gb): 10.00/6.72/2.92/4.91/13.81 | batch/sps: 10/1.27\n",
      "710-719/1127 | total/used/cuda/res/ram (Gb): 10.00/6.76/2.92/4.91/13.80 | batch/sps: 10/0.84\n",
      "720-729/1127 | total/used/cuda/res/ram (Gb): 10.00/6.76/2.92/4.91/13.81 | batch/sps: 10/0.92\n",
      "730-739/1127 | total/used/cuda/res/ram (Gb): 10.00/6.86/2.92/5.15/13.79 | batch/sps: 10/0.54\n",
      "740-749/1127 | total/used/cuda/res/ram (Gb): 10.00/6.85/2.92/5.15/13.84 | batch/sps: 10/0.98\n",
      "750-759/1127 | total/used/cuda/res/ram (Gb): 10.00/6.86/2.92/5.15/13.83 | batch/sps: 10/1.11\n",
      "760-769/1127 | total/used/cuda/res/ram (Gb): 10.00/6.86/2.92/5.15/13.81 | batch/sps: 10/0.62\n",
      "770-779/1127 | total/used/cuda/res/ram (Gb): 10.00/6.86/2.92/5.15/13.81 | batch/sps: 10/0.82\n",
      "780-789/1127 | total/used/cuda/res/ram (Gb): 10.00/6.87/2.92/5.15/13.83 | batch/sps: 10/0.81\n",
      "790-799/1127 | total/used/cuda/res/ram (Gb): 10.00/6.87/2.92/5.15/13.82 | batch/sps: 10/1.48\n",
      "800-809/1127 | total/used/cuda/res/ram (Gb): 10.00/6.88/2.92/5.15/13.83 | batch/sps: 10/1.00\n",
      "810-819/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.82 | batch/sps: 10/0.40\n",
      "820-829/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.82 | batch/sps: 10/0.73\n",
      "830-839/1127 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.83 | batch/sps: 10/0.75\n",
      "840-849/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.83 | batch/sps: 10/0.96\n",
      "850-859/1127 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.84 | batch/sps: 10/1.03\n",
      "860-869/1127 | total/used/cuda/res/ram (Gb): 10.00/7.18/2.92/5.46/13.83 | batch/sps: 10/0.47\n",
      "870-879/1127 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.84 | batch/sps: 10/0.81\n",
      "880-889/1127 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.46/13.85 | batch/sps: 10/0.57\n",
      "890-899/1127 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.46/13.83 | batch/sps: 10/0.76\n",
      "900-909/1127 | total/used/cuda/res/ram (Gb): 10.00/7.24/2.92/5.46/13.83 | batch/sps: 10/0.80\n",
      "910-919/1127 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.46/13.84 | batch/sps: 10/0.46\n",
      "920-929/1127 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.46/13.83 | batch/sps: 10/0.68\n",
      "930-939/1127 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.46/13.82 | batch/sps: 10/0.92\n",
      "940-949/1127 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.46/13.83 | batch/sps: 10/0.80\n",
      "950-959/1127 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.46/13.82 | batch/sps: 10/0.95\n",
      "960-969/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.83 | batch/sps: 10/0.86\n",
      "970-979/1127 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.46/13.83 | batch/sps: 10/0.81\n",
      "980-989/1127 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.46/13.83 | batch/sps: 10/1.55\n",
      "990-999/1127 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.83 | batch/sps: 10/1.21\n",
      "1000-1009/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.83 | batch/sps: 10/0.95\n",
      "1010-1019/1127 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.83 | batch/sps: 10/1.32\n",
      "1020-1029/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/0.85\n",
      "1030-1039/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/0.87\n",
      "1040-1049/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/0.67\n",
      "1050-1059/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.40\n",
      "1060-1069/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.26\n",
      "1070-1079/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/0.58\n",
      "1080-1089/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/0.99\n",
      "1090-1099/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/0.78\n",
      "1100-1109/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/1.48\n",
      "1110-1119/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/1.12\n",
      "1120-1126/1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 7/0.73\n",
      "Finished processing 1127 samples for neutralize.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 1127\n",
      "})\n",
      "new moon received negative reviews from critics.\n",
      "new moon received mixed reviews from critics.\n",
      "1127 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | sps: 1.15\n",
      "Processing 1144 samples for simplification\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae677f4e9af4ab7b14e3f6f2abd40f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.21\n",
      "10-19/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.30\n",
      "20-29/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/3.72\n",
      "30-39/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.58\n",
      "40-49/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/3.11\n",
      "50-59/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.28\n",
      "60-69/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.86 | batch/sps: 10/1.30\n",
      "70-79/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.90\n",
      "80-89/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.86 | batch/sps: 10/2.07\n",
      "90-99/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.08\n",
      "100-109/1144 | total/used/cuda/res/ram (Gb): 10.00/7.18/2.92/5.46/13.85 | batch/sps: 10/1.85\n",
      "110-119/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.87 | batch/sps: 10/1.67\n",
      "120-129/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.34\n",
      "130-139/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.85 | batch/sps: 10/2.16\n",
      "140-149/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.90 | batch/sps: 10/2.55\n",
      "150-159/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.87 | batch/sps: 10/1.96\n",
      "160-169/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.87 | batch/sps: 10/2.48\n",
      "170-179/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.87 | batch/sps: 10/1.99\n",
      "180-189/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.87 | batch/sps: 10/2.32\n",
      "190-199/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.98\n",
      "200-209/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.83\n",
      "210-219/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.15\n",
      "220-229/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.76\n",
      "230-239/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.11\n",
      "240-249/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.62\n",
      "250-259/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.97\n",
      "260-269/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.87\n",
      "270-279/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.84 | batch/sps: 10/0.93\n",
      "280-289/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.85 | batch/sps: 10/2.20\n",
      "290-299/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.59\n",
      "300-309/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.62\n",
      "310-319/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.26\n",
      "320-329/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.86 | batch/sps: 10/2.25\n",
      "330-339/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.40\n",
      "340-349/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/1.64\n",
      "350-359/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.89\n",
      "360-369/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.22\n",
      "370-379/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/3.72\n",
      "380-389/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.85 | batch/sps: 10/1.84\n",
      "390-399/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.01\n",
      "400-409/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.11\n",
      "410-419/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.40\n",
      "420-429/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.86 | batch/sps: 10/2.87\n",
      "430-439/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.87 | batch/sps: 10/1.22\n",
      "440-449/1144 | total/used/cuda/res/ram (Gb): 10.00/7.24/2.92/5.46/13.86 | batch/sps: 10/1.54\n",
      "450-459/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.86 | batch/sps: 10/1.78\n",
      "460-469/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.86 | batch/sps: 10/2.06\n",
      "470-479/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.86 | batch/sps: 10/1.77\n",
      "480-489/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.86 | batch/sps: 10/3.58\n",
      "490-499/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.85 | batch/sps: 10/2.00\n",
      "500-509/1144 | total/used/cuda/res/ram (Gb): 10.00/7.18/2.92/5.46/13.89 | batch/sps: 10/1.76\n",
      "510-519/1144 | total/used/cuda/res/ram (Gb): 10.00/7.18/2.92/5.46/13.92 | batch/sps: 10/2.06\n",
      "520-529/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.91 | batch/sps: 10/3.13\n",
      "530-539/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.86 | batch/sps: 10/2.30\n",
      "540-549/1144 | total/used/cuda/res/ram (Gb): 10.00/7.18/2.92/5.46/13.86 | batch/sps: 10/3.84\n",
      "550-559/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.87 | batch/sps: 10/3.14\n",
      "560-569/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/1.75\n",
      "570-579/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.41\n",
      "580-589/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.46\n",
      "590-599/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.64\n",
      "600-609/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.85 | batch/sps: 10/2.13\n",
      "610-619/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.20\n",
      "620-629/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.85 | batch/sps: 10/2.84\n",
      "630-639/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.35\n",
      "640-649/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/1.94\n",
      "650-659/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/1.23\n",
      "660-669/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.87 | batch/sps: 10/1.88\n",
      "670-679/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.88 | batch/sps: 10/2.61\n",
      "680-689/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/2.46\n",
      "690-699/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.86 | batch/sps: 10/1.69\n",
      "700-709/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.87 | batch/sps: 10/1.90\n",
      "710-719/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.86 | batch/sps: 10/1.87\n",
      "720-729/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.86 | batch/sps: 10/1.91\n",
      "730-739/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.87 | batch/sps: 10/3.38\n",
      "740-749/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.87 | batch/sps: 10/1.94\n",
      "750-759/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.88 | batch/sps: 10/2.79\n",
      "760-769/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.87 | batch/sps: 10/2.22\n",
      "770-779/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.87 | batch/sps: 10/1.65\n",
      "780-789/1144 | total/used/cuda/res/ram (Gb): 10.00/7.24/2.92/5.46/13.88 | batch/sps: 10/2.39\n",
      "790-799/1144 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.46/13.87 | batch/sps: 10/2.42\n",
      "800-809/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.87 | batch/sps: 10/1.63\n",
      "810-819/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.87 | batch/sps: 10/2.22\n",
      "820-829/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.87 | batch/sps: 10/1.46\n",
      "830-839/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.87 | batch/sps: 10/2.26\n",
      "840-849/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.87 | batch/sps: 10/2.56\n",
      "850-859/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.87 | batch/sps: 10/2.43\n",
      "860-869/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.88 | batch/sps: 10/2.61\n",
      "870-879/1144 | total/used/cuda/res/ram (Gb): 10.00/7.24/2.92/5.46/13.89 | batch/sps: 10/2.08\n",
      "880-889/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.89 | batch/sps: 10/2.35\n",
      "890-899/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.91 | batch/sps: 10/1.81\n",
      "900-909/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.93 | batch/sps: 10/2.16\n",
      "910-919/1144 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.46/13.92 | batch/sps: 10/2.82\n",
      "920-929/1144 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.46/13.92 | batch/sps: 10/1.95\n",
      "930-939/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.92 | batch/sps: 10/2.63\n",
      "940-949/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.92 | batch/sps: 10/1.36\n",
      "950-959/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.92 | batch/sps: 10/1.80\n",
      "960-969/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.90 | batch/sps: 10/3.15\n",
      "970-979/1144 | total/used/cuda/res/ram (Gb): 10.00/7.20/2.92/5.46/13.90 | batch/sps: 10/1.96\n",
      "980-989/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.90 | batch/sps: 10/2.21\n",
      "990-999/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.90 | batch/sps: 10/1.76\n",
      "1000-1009/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.91 | batch/sps: 10/2.95\n",
      "1010-1019/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.90 | batch/sps: 10/1.75\n",
      "1020-1029/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.90 | batch/sps: 10/2.55\n",
      "1030-1039/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.91 | batch/sps: 10/1.59\n",
      "1040-1049/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.91 | batch/sps: 10/2.24\n",
      "1050-1059/1144 | total/used/cuda/res/ram (Gb): 10.00/7.19/2.92/5.46/13.91 | batch/sps: 10/2.44\n",
      "1060-1069/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.91 | batch/sps: 10/3.28\n",
      "1070-1079/1144 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.46/13.91 | batch/sps: 10/1.89\n",
      "1080-1089/1144 | total/used/cuda/res/ram (Gb): 10.00/7.22/2.92/5.46/13.91 | batch/sps: 10/2.61\n",
      "1090-1099/1144 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.46/13.91 | batch/sps: 10/1.89\n",
      "1100-1109/1144 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.46/13.91 | batch/sps: 10/3.21\n",
      "1110-1119/1144 | total/used/cuda/res/ram (Gb): 10.00/7.24/2.92/5.46/13.91 | batch/sps: 10/2.49\n",
      "1120-1129/1144 | total/used/cuda/res/ram (Gb): 10.00/7.23/2.92/5.46/13.91 | batch/sps: 10/2.60\n",
      "1130-1139/1144 | total/used/cuda/res/ram (Gb): 10.00/7.21/2.92/5.46/13.91 | batch/sps: 10/2.12\n",
      "1140-1143/1144 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.92 | batch/sps: 4/1.46\n",
      "Finished processing 1144 samples for simplification.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 1144\n",
      "})\n",
      "When the first episode of Torchwood was launched in October 2006 on BBC Three, 2.4 million people watched it.\n",
      "It was first broadcast in October 2006 and had 2.4 million viewers.\n",
      "1144 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.92 | sps: 2.09\n",
      "Processing 126 samples for clarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e3c3977e984f14b59f5f5259bf73c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/126 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.91 | batch/sps: 10/0.66\n",
      "10-19/126 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.92 | batch/sps: 10/0.69\n",
      "20-29/126 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.91 | batch/sps: 10/0.72\n",
      "30-39/126 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.91 | batch/sps: 10/1.27\n",
      "40-49/126 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.89 | batch/sps: 10/1.44\n",
      "50-59/126 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.94 | batch/sps: 10/0.75\n",
      "60-69/126 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | batch/sps: 10/1.23\n",
      "70-79/126 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.91 | batch/sps: 10/0.86\n",
      "80-89/126 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | batch/sps: 10/0.93\n",
      "90-99/126 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.91 | batch/sps: 10/1.13\n",
      "100-109/126 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.91 | batch/sps: 10/1.31\n",
      "110-119/126 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.92 | batch/sps: 10/0.75\n",
      "120-125/126 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | batch/sps: 6/0.43\n",
      "Finished processing 126 samples for clarity.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 126\n",
      "})\n",
      "Canals are waterway channels, or artificial waterways, for water conveyance, or to service water transport vehicles.\n",
      "Canals are artificial waterways, or channels, for water conveyance, or to service water transport vehicles.\n",
      "126 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | sps: 0.87\n",
      "Processing 1590 samples for paraphrase\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c13beb5793c4a848a90289f2398756e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.92 | batch/sps: 10/1.93\n",
      "10-19/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.92 | batch/sps: 10/2.11\n",
      "20-29/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.92 | batch/sps: 10/1.91\n",
      "30-39/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | batch/sps: 10/2.01\n",
      "40-49/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.92 | batch/sps: 10/2.11\n",
      "50-59/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.92 | batch/sps: 10/2.14\n",
      "60-69/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.92 | batch/sps: 10/2.36\n",
      "70-79/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.92 | batch/sps: 10/2.11\n",
      "80-89/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.92 | batch/sps: 10/2.36\n",
      "90-99/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.92 | batch/sps: 10/2.22\n",
      "100-109/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.93 | batch/sps: 10/1.77\n",
      "110-119/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.92 | batch/sps: 10/1.88\n",
      "120-129/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.93 | batch/sps: 10/1.75\n",
      "130-139/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.92 | batch/sps: 10/1.84\n",
      "140-149/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.92 | batch/sps: 10/1.71\n",
      "150-159/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | batch/sps: 10/1.97\n",
      "160-169/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.91 | batch/sps: 10/1.46\n",
      "170-179/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.91 | batch/sps: 10/2.08\n",
      "180-189/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.93 | batch/sps: 10/1.84\n",
      "190-199/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | batch/sps: 10/2.14\n",
      "200-209/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.93 | batch/sps: 10/1.73\n",
      "210-219/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.94 | batch/sps: 10/2.04\n",
      "220-229/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.93 | batch/sps: 10/2.06\n",
      "230-239/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.91 | batch/sps: 10/1.89\n",
      "240-249/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.92 | batch/sps: 10/2.17\n",
      "250-259/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.92 | batch/sps: 10/2.15\n",
      "260-269/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.91 | batch/sps: 10/2.40\n",
      "270-279/1590 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.56/13.91 | batch/sps: 10/1.99\n",
      "280-289/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.91 | batch/sps: 10/1.66\n",
      "290-299/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.90 | batch/sps: 10/2.28\n",
      "300-309/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.91 | batch/sps: 10/1.95\n",
      "310-319/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.91 | batch/sps: 10/2.10\n",
      "320-329/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.92 | batch/sps: 10/1.63\n",
      "330-339/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.93 | batch/sps: 10/1.89\n",
      "340-349/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.92 | batch/sps: 10/2.09\n",
      "350-359/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.92 | batch/sps: 10/2.23\n",
      "360-369/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.92 | batch/sps: 10/2.13\n",
      "370-379/1590 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.56/13.92 | batch/sps: 10/2.20\n",
      "380-389/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.93 | batch/sps: 10/2.03\n",
      "390-399/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | batch/sps: 10/1.91\n",
      "400-409/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.91 | batch/sps: 10/2.07\n",
      "410-419/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.92 | batch/sps: 10/1.65\n",
      "420-429/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.92 | batch/sps: 10/1.86\n",
      "430-439/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.92 | batch/sps: 10/2.04\n",
      "440-449/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.92 | batch/sps: 10/1.76\n",
      "450-459/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.92 | batch/sps: 10/1.73\n",
      "460-469/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.93 | batch/sps: 10/1.81\n",
      "470-479/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.92 | batch/sps: 10/1.87\n",
      "480-489/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.93 | batch/sps: 10/2.20\n",
      "490-499/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.93 | batch/sps: 10/1.65\n",
      "500-509/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.93 | batch/sps: 10/2.00\n",
      "510-519/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.93 | batch/sps: 10/1.56\n",
      "520-529/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.90 | batch/sps: 10/1.62\n",
      "530-539/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.95 | batch/sps: 10/2.07\n",
      "540-549/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.96 | batch/sps: 10/1.53\n",
      "550-559/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.96 | batch/sps: 10/1.65\n",
      "560-569/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.95 | batch/sps: 10/1.73\n",
      "570-579/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.95 | batch/sps: 10/1.73\n",
      "580-589/1590 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.56/13.95 | batch/sps: 10/2.00\n",
      "590-599/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.95 | batch/sps: 10/1.86\n",
      "600-609/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.93 | batch/sps: 10/1.62\n",
      "610-619/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.93 | batch/sps: 10/1.72\n",
      "620-629/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.94 | batch/sps: 10/2.12\n",
      "630-639/1590 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.94 | batch/sps: 10/1.97\n",
      "640-649/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.95 | batch/sps: 10/1.91\n",
      "650-659/1590 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.93 | batch/sps: 10/1.84\n",
      "660-669/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.94 | batch/sps: 10/2.19\n",
      "670-679/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.94 | batch/sps: 10/1.96\n",
      "680-689/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.94 | batch/sps: 10/1.66\n",
      "690-699/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.94 | batch/sps: 10/1.56\n",
      "700-709/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.95 | batch/sps: 10/1.86\n",
      "710-719/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.95 | batch/sps: 10/1.40\n",
      "720-729/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.94 | batch/sps: 10/1.86\n",
      "730-739/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.95 | batch/sps: 10/1.65\n",
      "740-749/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.95 | batch/sps: 10/1.74\n",
      "750-759/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.95 | batch/sps: 10/1.59\n",
      "760-769/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.95 | batch/sps: 10/1.62\n",
      "770-779/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.96 | batch/sps: 10/1.72\n",
      "780-789/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.96 | batch/sps: 10/1.34\n",
      "790-799/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.95 | batch/sps: 10/1.71\n",
      "800-809/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.96 | batch/sps: 10/1.43\n",
      "810-819/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.95 | batch/sps: 10/1.84\n",
      "820-829/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.95 | batch/sps: 10/1.50\n",
      "830-839/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.94 | batch/sps: 10/1.40\n",
      "840-849/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.98 | batch/sps: 10/1.54\n",
      "850-859/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/14.00 | batch/sps: 10/1.62\n",
      "860-869/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.99 | batch/sps: 10/1.27\n",
      "870-879/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.99 | batch/sps: 10/1.65\n",
      "880-889/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.98 | batch/sps: 10/1.10\n",
      "890-899/1590 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.99 | batch/sps: 10/1.60\n",
      "900-909/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/1.41\n",
      "910-919/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.98 | batch/sps: 10/1.60\n",
      "920-929/1590 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.98 | batch/sps: 10/1.39\n",
      "930-939/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/1.38\n",
      "940-949/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/1.48\n",
      "950-959/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.98 | batch/sps: 10/1.42\n",
      "960-969/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.98 | batch/sps: 10/1.65\n",
      "970-979/1590 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.56/13.97 | batch/sps: 10/1.25\n",
      "980-989/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.97 | batch/sps: 10/1.40\n",
      "990-999/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.97 | batch/sps: 10/1.61\n",
      "1000-1009/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.98 | batch/sps: 10/1.44\n",
      "1010-1019/1590 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.56/13.99 | batch/sps: 10/1.19\n",
      "1020-1029/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.99 | batch/sps: 10/1.40\n",
      "1030-1039/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.99 | batch/sps: 10/1.43\n",
      "1040-1049/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.99 | batch/sps: 10/1.22\n",
      "1050-1059/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/1.20\n",
      "1060-1069/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.98 | batch/sps: 10/1.03\n",
      "1070-1079/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/1.84\n",
      "1080-1089/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.98 | batch/sps: 10/2.44\n",
      "1090-1099/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.99 | batch/sps: 10/1.36\n",
      "1100-1109/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.99 | batch/sps: 10/2.63\n",
      "1110-1119/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.99 | batch/sps: 10/2.51\n",
      "1120-1129/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.98 | batch/sps: 10/2.58\n",
      "1130-1139/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.98 | batch/sps: 10/1.92\n",
      "1140-1149/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.98 | batch/sps: 10/2.42\n",
      "1150-1159/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.98 | batch/sps: 10/1.88\n",
      "1160-1169/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/4.17\n",
      "1170-1179/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/1.95\n",
      "1180-1189/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.97 | batch/sps: 10/1.73\n",
      "1190-1199/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.97 | batch/sps: 10/2.90\n",
      "1200-1209/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.97 | batch/sps: 10/2.55\n",
      "1210-1219/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.97 | batch/sps: 10/2.42\n",
      "1220-1229/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/2.47\n",
      "1230-1239/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/3.12\n",
      "1240-1249/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/2.31\n",
      "1250-1259/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/2.94\n",
      "1260-1269/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.97 | batch/sps: 10/4.26\n",
      "1270-1279/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/3.50\n",
      "1280-1289/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/3.58\n",
      "1290-1299/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/3.54\n",
      "1300-1309/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.97 | batch/sps: 10/3.65\n",
      "1310-1319/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/2.43\n",
      "1320-1329/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.96 | batch/sps: 10/2.78\n",
      "1330-1339/1590 | total/used/cuda/res/ram (Gb): 10.00/7.26/2.92/5.56/13.97 | batch/sps: 10/3.74\n",
      "1340-1349/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/1.79\n",
      "1350-1359/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/2.10\n",
      "1360-1369/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.97 | batch/sps: 10/2.84\n",
      "1370-1379/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.97 | batch/sps: 10/3.69\n",
      "1380-1389/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/2.92\n",
      "1390-1399/1590 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.97 | batch/sps: 10/2.38\n",
      "1400-1409/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.97 | batch/sps: 10/1.81\n",
      "1410-1419/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.97 | batch/sps: 10/2.55\n",
      "1420-1429/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/3.03\n",
      "1430-1439/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/2.43\n",
      "1440-1449/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/3.41\n",
      "1450-1459/1590 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.98 | batch/sps: 10/1.91\n",
      "1460-1469/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/1.63\n",
      "1470-1479/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.98 | batch/sps: 10/2.34\n",
      "1480-1489/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.97 | batch/sps: 10/4.14\n",
      "1490-1499/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/3.18\n",
      "1500-1509/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/2.95\n",
      "1510-1519/1590 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.97 | batch/sps: 10/2.01\n",
      "1520-1529/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/1.97\n",
      "1530-1539/1590 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.95 | batch/sps: 10/1.63\n",
      "1540-1549/1590 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.95 | batch/sps: 10/4.47\n",
      "1550-1559/1590 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/14.00 | batch/sps: 10/2.78\n",
      "1560-1569/1590 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.99 | batch/sps: 10/2.86\n",
      "1570-1579/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.99 | batch/sps: 10/1.87\n",
      "1580-1589/1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.99 | batch/sps: 10/4.23\n",
      "Finished processing 1590 samples for paraphrase.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 1590\n",
      "})\n",
      "Yet, as Jack walked slowly over the wooden gangway to the depressed, deserted beach, the thought of Speedy Parker was dancing on the boundary of his awareness.\n",
      "Jack's mind was still on Speedy Parker as he walked along the boardwalk and down to the empty beach.\n",
      "1590 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.99 | sps: 1.91\n",
      "Processing 1062 samples for coherence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f05dfd08c3941ddb1d7021f3fed18a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/1062 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.98 | batch/sps: 10/1.35\n",
      "10-19/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/14.00 | batch/sps: 10/1.15\n",
      "20-29/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.97 | batch/sps: 10/1.24\n",
      "30-39/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.96 | batch/sps: 10/1.35\n",
      "40-49/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/1.34\n",
      "50-59/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/1.42\n",
      "60-69/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.98 | batch/sps: 10/1.07\n",
      "70-79/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/1.15\n",
      "80-89/1062 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.97 | batch/sps: 10/1.14\n",
      "90-99/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.97 | batch/sps: 10/1.12\n",
      "100-109/1062 | total/used/cuda/res/ram (Gb): 10.00/7.29/2.92/5.56/13.97 | batch/sps: 10/1.04\n",
      "110-119/1062 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.97 | batch/sps: 10/1.21\n",
      "120-129/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.97 | batch/sps: 10/1.28\n",
      "130-139/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/1.14\n",
      "140-149/1062 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.97 | batch/sps: 10/1.00\n",
      "150-159/1062 | total/used/cuda/res/ram (Gb): 10.00/7.25/2.92/5.56/13.98 | batch/sps: 10/1.12\n",
      "160-169/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.97 | batch/sps: 10/1.27\n",
      "170-179/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/1.23\n",
      "180-189/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/1.19\n",
      "190-199/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.95 | batch/sps: 10/1.47\n",
      "200-209/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.99 | batch/sps: 10/1.48\n",
      "210-219/1062 | total/used/cuda/res/ram (Gb): 10.00/7.28/2.92/5.56/13.99 | batch/sps: 10/1.58\n",
      "220-229/1062 | total/used/cuda/res/ram (Gb): 10.00/7.27/2.92/5.56/13.97 | batch/sps: 10/1.30\n",
      "230-239/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.95 | batch/sps: 10/1.00\n",
      "240-249/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.95 | batch/sps: 10/1.30\n",
      "250-259/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.95 | batch/sps: 10/1.13\n",
      "260-269/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.96 | batch/sps: 10/1.37\n",
      "270-279/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.95 | batch/sps: 10/1.18\n",
      "280-289/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.96 | batch/sps: 10/1.44\n",
      "290-299/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.96 | batch/sps: 10/1.41\n",
      "300-309/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.96 | batch/sps: 10/1.25\n",
      "310-319/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.95 | batch/sps: 10/1.37\n",
      "320-329/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/1.31\n",
      "330-339/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.95 | batch/sps: 10/1.23\n",
      "340-349/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.95 | batch/sps: 10/1.38\n",
      "350-359/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.96 | batch/sps: 10/1.30\n",
      "360-369/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.96 | batch/sps: 10/1.28\n",
      "370-379/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.96 | batch/sps: 10/1.29\n",
      "380-389/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.96 | batch/sps: 10/1.23\n",
      "390-399/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.96 | batch/sps: 10/1.52\n",
      "400-409/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.96 | batch/sps: 10/0.91\n",
      "410-419/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.96 | batch/sps: 10/1.36\n",
      "420-429/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/1.10\n",
      "430-439/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/14.00 | batch/sps: 10/1.30\n",
      "440-449/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/1.29\n",
      "450-459/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.99 | batch/sps: 10/1.41\n",
      "460-469/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.97 | batch/sps: 10/1.25\n",
      "470-479/1062 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.98 | batch/sps: 10/1.15\n",
      "480-489/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.97 | batch/sps: 10/1.05\n",
      "490-499/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/1.31\n",
      "500-509/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.97 | batch/sps: 10/1.14\n",
      "510-519/1062 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.97 | batch/sps: 10/1.17\n",
      "520-529/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.97 | batch/sps: 10/1.38\n",
      "530-539/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.97 | batch/sps: 10/1.35\n",
      "540-549/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.98 | batch/sps: 10/1.28\n",
      "550-559/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.97 | batch/sps: 10/1.42\n",
      "560-569/1062 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.97 | batch/sps: 10/1.17\n",
      "570-579/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.99 | batch/sps: 10/1.14\n",
      "580-589/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.98 | batch/sps: 10/1.35\n",
      "590-599/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.98 | batch/sps: 10/1.22\n",
      "600-609/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.98 | batch/sps: 10/1.11\n",
      "610-619/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/1.14\n",
      "620-629/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.98 | batch/sps: 10/1.16\n",
      "630-639/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.99 | batch/sps: 10/1.42\n",
      "640-649/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.95 | batch/sps: 10/1.22\n",
      "650-659/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/14.00 | batch/sps: 10/1.33\n",
      "660-669/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/14.00 | batch/sps: 10/1.22\n",
      "670-679/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.97 | batch/sps: 10/1.37\n",
      "680-689/1062 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.97 | batch/sps: 10/1.19\n",
      "690-699/1062 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.95 | batch/sps: 10/1.19\n",
      "700-709/1062 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.95 | batch/sps: 10/1.17\n",
      "710-719/1062 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.95 | batch/sps: 10/1.39\n",
      "720-729/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.96 | batch/sps: 10/1.24\n",
      "730-739/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.96 | batch/sps: 10/1.09\n",
      "740-749/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.96 | batch/sps: 10/1.04\n",
      "750-759/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.96 | batch/sps: 10/1.29\n",
      "760-769/1062 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.96 | batch/sps: 10/1.14\n",
      "770-779/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/1.30\n",
      "780-789/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.97 | batch/sps: 10/1.13\n",
      "790-799/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/1.24\n",
      "800-809/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.96 | batch/sps: 10/1.36\n",
      "810-819/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.96 | batch/sps: 10/1.24\n",
      "820-829/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.97 | batch/sps: 10/1.11\n",
      "830-839/1062 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.96 | batch/sps: 10/1.22\n",
      "840-849/1062 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.97 | batch/sps: 10/1.30\n",
      "850-859/1062 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.96 | batch/sps: 10/1.35\n",
      "860-869/1062 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.96 | batch/sps: 10/1.23\n",
      "870-879/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/14.01 | batch/sps: 10/1.07\n",
      "880-889/1062 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.00 | batch/sps: 10/1.27\n",
      "890-899/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.98 | batch/sps: 10/1.34\n",
      "900-909/1062 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.97 | batch/sps: 10/1.13\n",
      "910-919/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.97 | batch/sps: 10/1.16\n",
      "920-929/1062 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.97 | batch/sps: 10/1.13\n",
      "930-939/1062 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.97 | batch/sps: 10/1.01\n",
      "940-949/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.98 | batch/sps: 10/1.18\n",
      "950-959/1062 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.97 | batch/sps: 10/1.18\n",
      "960-969/1062 | total/used/cuda/res/ram (Gb): 10.00/7.43/2.92/5.56/13.97 | batch/sps: 10/1.38\n",
      "970-979/1062 | total/used/cuda/res/ram (Gb): 10.00/7.44/2.92/5.56/13.98 | batch/sps: 10/1.18\n",
      "980-989/1062 | total/used/cuda/res/ram (Gb): 10.00/7.44/2.92/5.56/13.97 | batch/sps: 10/1.28\n",
      "990-999/1062 | total/used/cuda/res/ram (Gb): 10.00/7.48/2.92/5.56/13.97 | batch/sps: 10/1.43\n",
      "1000-1009/1062 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/13.96 | batch/sps: 10/1.16\n",
      "1010-1019/1062 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.97 | batch/sps: 10/1.14\n",
      "1020-1029/1062 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.96 | batch/sps: 10/1.03\n",
      "1030-1039/1062 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.98 | batch/sps: 10/1.06\n",
      "1040-1049/1062 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/13.97 | batch/sps: 10/1.13\n",
      "1050-1059/1062 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.97 | batch/sps: 10/1.76\n",
      "1060-1061/1062 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/13.98 | batch/sps: 2/0.39\n",
      "Finished processing 1062 samples for coherence.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 1062\n",
      "})\n",
      "Outside the town, 6 tourists were reported killed. However, official documents indicate that at least 255 local residents were killed, with a further 29 never found.\n",
      "Outside the town, 6 tourists were reported killed. However, official documents indicate that at least 255 local residents were killed, with a further 29 never found.\n",
      "1062 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/13.98 | sps: 1.22\n",
      "Processing 2031 samples for gec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef72da867494fc99db1804b6f9581c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.97 | batch/sps: 10/2.68\n",
      "10-19/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/1.86\n",
      "20-29/2031 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.96 | batch/sps: 10/1.51\n",
      "30-39/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.99 | batch/sps: 10/2.02\n",
      "40-49/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.01 | batch/sps: 10/1.52\n",
      "50-59/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/14.00 | batch/sps: 10/1.70\n",
      "60-69/2031 | total/used/cuda/res/ram (Gb): 10.00/7.42/2.92/5.56/14.00 | batch/sps: 10/3.72\n",
      "70-79/2031 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/14.00 | batch/sps: 10/3.08\n",
      "80-89/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.97 | batch/sps: 10/2.50\n",
      "90-99/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/3.37\n",
      "100-109/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/2.83\n",
      "110-119/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.99 | batch/sps: 10/2.51\n",
      "120-129/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/2.05\n",
      "130-139/2031 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.99 | batch/sps: 10/2.02\n",
      "140-149/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.99 | batch/sps: 10/2.66\n",
      "150-159/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.99 | batch/sps: 10/2.45\n",
      "160-169/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/14.00 | batch/sps: 10/2.04\n",
      "170-179/2031 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.99 | batch/sps: 10/2.16\n",
      "180-189/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.99 | batch/sps: 10/2.16\n",
      "190-199/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.99 | batch/sps: 10/1.56\n",
      "200-209/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.99 | batch/sps: 10/2.72\n",
      "210-219/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.99 | batch/sps: 10/2.33\n",
      "220-229/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.99 | batch/sps: 10/2.46\n",
      "230-239/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.99 | batch/sps: 10/3.75\n",
      "240-249/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.99 | batch/sps: 10/2.18\n",
      "250-259/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.99 | batch/sps: 10/3.41\n",
      "260-269/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.99 | batch/sps: 10/3.34\n",
      "270-279/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.99 | batch/sps: 10/2.39\n",
      "280-289/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/2.38\n",
      "290-299/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/2.05\n",
      "300-309/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/3.89\n",
      "310-319/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/3.08\n",
      "320-329/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.99 | batch/sps: 10/2.73\n",
      "330-339/2031 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.99 | batch/sps: 10/2.58\n",
      "340-349/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.99 | batch/sps: 10/1.48\n",
      "350-359/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/2.65\n",
      "360-369/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/3.70\n",
      "370-379/2031 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/14.00 | batch/sps: 10/2.47\n",
      "380-389/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.99 | batch/sps: 10/1.89\n",
      "390-399/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/1.66\n",
      "400-409/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/2.66\n",
      "410-419/2031 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.99 | batch/sps: 10/2.20\n",
      "420-429/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.99 | batch/sps: 10/2.91\n",
      "430-439/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.99 | batch/sps: 10/1.79\n",
      "440-449/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.96 | batch/sps: 10/1.28\n",
      "450-459/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/2.12\n",
      "460-469/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.01 | batch/sps: 10/2.44\n",
      "470-479/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/14.00 | batch/sps: 10/2.27\n",
      "480-489/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.99 | batch/sps: 10/2.45\n",
      "490-499/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.99 | batch/sps: 10/3.27\n",
      "500-509/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.99 | batch/sps: 10/3.89\n",
      "510-519/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.98 | batch/sps: 10/3.40\n",
      "520-529/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.98 | batch/sps: 10/2.11\n",
      "530-539/2031 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/13.98 | batch/sps: 10/2.68\n",
      "540-549/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.97 | batch/sps: 10/1.89\n",
      "550-559/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.99 | batch/sps: 10/1.79\n",
      "560-569/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.98 | batch/sps: 10/2.23\n",
      "570-579/2031 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/13.99 | batch/sps: 10/2.61\n",
      "580-589/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.98 | batch/sps: 10/2.27\n",
      "590-599/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.98 | batch/sps: 10/2.41\n",
      "600-609/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.98 | batch/sps: 10/1.34\n",
      "610-619/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.98 | batch/sps: 10/2.39\n",
      "620-629/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.98 | batch/sps: 10/2.28\n",
      "630-639/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.99 | batch/sps: 10/4.05\n",
      "640-649/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.99 | batch/sps: 10/2.94\n",
      "650-659/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.99 | batch/sps: 10/1.53\n",
      "660-669/2031 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.99 | batch/sps: 10/3.23\n",
      "670-679/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.98 | batch/sps: 10/1.41\n",
      "680-689/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.99 | batch/sps: 10/2.07\n",
      "690-699/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.99 | batch/sps: 10/1.70\n",
      "700-709/2031 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.98 | batch/sps: 10/1.77\n",
      "710-719/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/1.81\n",
      "720-729/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/3.43\n",
      "730-739/2031 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/2.08\n",
      "740-749/2031 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.98 | batch/sps: 10/3.25\n",
      "750-759/2031 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/2.24\n",
      "760-769/2031 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.97 | batch/sps: 10/4.07\n",
      "770-779/2031 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/2.50\n",
      "780-789/2031 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.98 | batch/sps: 10/2.12\n",
      "790-799/2031 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.98 | batch/sps: 10/1.88\n",
      "800-809/2031 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/13.98 | batch/sps: 10/2.51\n",
      "810-819/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.97 | batch/sps: 10/2.20\n",
      "820-829/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.00 | batch/sps: 10/3.62\n",
      "830-839/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.98 | batch/sps: 10/1.65\n",
      "840-849/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.97 | batch/sps: 10/2.75\n",
      "850-859/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.97 | batch/sps: 10/2.31\n",
      "860-869/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/14.02 | batch/sps: 10/1.88\n",
      "870-879/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/14.02 | batch/sps: 10/2.04\n",
      "880-889/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/14.02 | batch/sps: 10/2.40\n",
      "890-899/2031 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.98 | batch/sps: 10/1.76\n",
      "900-909/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.98 | batch/sps: 10/2.52\n",
      "910-919/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.96 | batch/sps: 10/3.90\n",
      "920-929/2031 | total/used/cuda/res/ram (Gb): 10.00/7.44/2.92/5.56/13.96 | batch/sps: 10/2.34\n",
      "930-939/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/13.96 | batch/sps: 10/2.24\n",
      "940-949/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.96 | batch/sps: 10/2.79\n",
      "950-959/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.95 | batch/sps: 10/1.46\n",
      "960-969/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.97 | batch/sps: 10/2.17\n",
      "970-979/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.97 | batch/sps: 10/2.40\n",
      "980-989/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.96 | batch/sps: 10/1.53\n",
      "990-999/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.96 | batch/sps: 10/1.62\n",
      "1000-1009/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.96 | batch/sps: 10/2.08\n",
      "1010-1019/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.96 | batch/sps: 10/1.79\n",
      "1020-1029/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.95 | batch/sps: 10/2.32\n",
      "1030-1039/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.96 | batch/sps: 10/1.50\n",
      "1040-1049/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/13.96 | batch/sps: 10/3.12\n",
      "1050-1059/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.96 | batch/sps: 10/2.77\n",
      "1060-1069/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.96 | batch/sps: 10/3.17\n",
      "1070-1079/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.96 | batch/sps: 10/2.06\n",
      "1080-1089/2031 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.96 | batch/sps: 10/2.70\n",
      "1090-1099/2031 | total/used/cuda/res/ram (Gb): 10.00/7.30/2.92/5.56/13.96 | batch/sps: 10/1.86\n",
      "1100-1109/2031 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/13.96 | batch/sps: 10/1.93\n",
      "1110-1119/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.96 | batch/sps: 10/1.98\n",
      "1120-1129/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.97 | batch/sps: 10/2.12\n",
      "1130-1139/2031 | total/used/cuda/res/ram (Gb): 10.00/7.42/2.92/5.56/13.96 | batch/sps: 10/1.79\n",
      "1140-1149/2031 | total/used/cuda/res/ram (Gb): 10.00/7.43/2.92/5.56/13.96 | batch/sps: 10/2.45\n",
      "1150-1159/2031 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/13.96 | batch/sps: 10/2.97\n",
      "1160-1169/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/13.96 | batch/sps: 10/2.04\n",
      "1170-1179/2031 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/13.95 | batch/sps: 10/1.73\n",
      "1180-1189/2031 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/13.96 | batch/sps: 10/1.54\n",
      "1190-1199/2031 | total/used/cuda/res/ram (Gb): 10.00/7.45/2.92/5.56/13.95 | batch/sps: 10/2.27\n",
      "1200-1209/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/13.96 | batch/sps: 10/2.40\n",
      "1210-1219/2031 | total/used/cuda/res/ram (Gb): 10.00/7.46/2.92/5.56/13.96 | batch/sps: 10/2.38\n",
      "1220-1229/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/13.95 | batch/sps: 10/2.86\n",
      "1230-1239/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/13.97 | batch/sps: 10/2.27\n",
      "1240-1249/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/14.01 | batch/sps: 10/2.95\n",
      "1250-1259/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.01 | batch/sps: 10/2.82\n",
      "1260-1269/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/14.04 | batch/sps: 10/2.13\n",
      "1270-1279/2031 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.92/5.56/14.01 | batch/sps: 10/1.88\n",
      "1280-1289/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/14.01 | batch/sps: 10/2.63\n",
      "1290-1299/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/14.02 | batch/sps: 10/1.98\n",
      "1300-1309/2031 | total/used/cuda/res/ram (Gb): 10.00/7.42/2.92/5.56/14.01 | batch/sps: 10/2.73\n",
      "1310-1319/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/14.00 | batch/sps: 10/1.67\n",
      "1320-1329/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.00 | batch/sps: 10/2.58\n",
      "1330-1339/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/14.01 | batch/sps: 10/1.81\n",
      "1340-1349/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/14.01 | batch/sps: 10/3.48\n",
      "1350-1359/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/14.01 | batch/sps: 10/2.42\n",
      "1360-1369/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/14.01 | batch/sps: 10/1.95\n",
      "1370-1379/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.01 | batch/sps: 10/1.93\n",
      "1380-1389/2031 | total/used/cuda/res/ram (Gb): 10.00/7.37/2.92/5.56/14.01 | batch/sps: 10/2.98\n",
      "1390-1399/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/14.01 | batch/sps: 10/2.09\n",
      "1400-1409/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/14.01 | batch/sps: 10/2.78\n",
      "1410-1419/2031 | total/used/cuda/res/ram (Gb): 10.00/7.40/2.92/5.56/14.00 | batch/sps: 10/2.60\n",
      "1420-1429/2031 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/14.00 | batch/sps: 10/2.93\n",
      "1430-1439/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.00 | batch/sps: 10/1.83\n",
      "1440-1449/2031 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/14.01 | batch/sps: 10/1.74\n",
      "1450-1459/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/14.01 | batch/sps: 10/2.90\n",
      "1460-1469/2031 | total/used/cuda/res/ram (Gb): 10.00/7.31/2.92/5.56/14.01 | batch/sps: 10/1.90\n",
      "1470-1479/2031 | total/used/cuda/res/ram (Gb): 10.00/7.41/2.92/5.56/14.02 | batch/sps: 10/1.76\n",
      "1480-1489/2031 | total/used/cuda/res/ram (Gb): 10.00/7.39/2.92/5.56/14.01 | batch/sps: 10/1.85\n",
      "1490-1499/2031 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.92/5.56/14.01 | batch/sps: 10/2.62\n",
      "1500-1509/2031 | total/used/cuda/res/ram (Gb): 10.00/7.36/2.92/5.56/14.02 | batch/sps: 10/2.93\n",
      "1510-1519/2031 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/14.02 | batch/sps: 10/1.70\n",
      "1520-1529/2031 | total/used/cuda/res/ram (Gb): 10.00/7.33/2.92/5.56/14.02 | batch/sps: 10/3.17\n",
      "1530-1539/2031 | total/used/cuda/res/ram (Gb): 10.00/7.32/2.92/5.56/14.02 | batch/sps: 10/2.54\n",
      "1540-1549/2031 | total/used/cuda/res/ram (Gb): 10.00/7.38/2.92/5.56/14.02 | batch/sps: 10/0.85\n",
      "1550-1559/2031 | total/used/cuda/res/ram (Gb): 10.00/7.72/2.92/5.90/13.98 | batch/sps: 10/0.36\n",
      "1560-1569/2031 | total/used/cuda/res/ram (Gb): 10.00/7.70/2.92/5.90/14.01 | batch/sps: 10/0.55\n",
      "1570-1579/2031 | total/used/cuda/res/ram (Gb): 10.00/7.71/2.92/5.90/13.98 | batch/sps: 10/0.46\n",
      "1580-1589/2031 | total/used/cuda/res/ram (Gb): 10.00/7.70/2.92/5.90/13.96 | batch/sps: 10/0.38\n",
      "1590-1599/2031 | total/used/cuda/res/ram (Gb): 10.00/7.74/2.92/5.90/13.96 | batch/sps: 10/0.50\n",
      "1600-1609/2031 | total/used/cuda/res/ram (Gb): 10.00/7.73/2.92/5.90/13.98 | batch/sps: 10/0.43\n",
      "1610-1619/2031 | total/used/cuda/res/ram (Gb): 10.00/8.02/2.92/6.25/13.97 | batch/sps: 10/0.35\n",
      "1620-1629/2031 | total/used/cuda/res/ram (Gb): 10.00/8.08/2.92/6.25/13.99 | batch/sps: 10/0.52\n",
      "1630-1639/2031 | total/used/cuda/res/ram (Gb): 10.00/8.05/2.92/6.25/13.98 | batch/sps: 10/0.37\n",
      "1640-1649/2031 | total/used/cuda/res/ram (Gb): 10.00/8.07/2.92/6.25/13.99 | batch/sps: 10/0.35\n",
      "1650-1659/2031 | total/used/cuda/res/ram (Gb): 10.00/8.08/2.92/6.25/13.98 | batch/sps: 10/0.54\n",
      "1660-1669/2031 | total/used/cuda/res/ram (Gb): 10.00/8.09/2.92/6.25/13.98 | batch/sps: 10/0.62\n",
      "1670-1679/2031 | total/used/cuda/res/ram (Gb): 10.00/8.08/2.92/6.25/13.97 | batch/sps: 10/0.62\n",
      "1680-1689/2031 | total/used/cuda/res/ram (Gb): 10.00/8.06/2.92/6.25/13.98 | batch/sps: 10/0.60\n",
      "1690-1699/2031 | total/used/cuda/res/ram (Gb): 10.00/8.08/2.92/6.25/14.00 | batch/sps: 10/0.55\n",
      "1700-1709/2031 | total/used/cuda/res/ram (Gb): 10.00/8.09/2.92/6.25/13.98 | batch/sps: 10/0.49\n",
      "1710-1719/2031 | total/used/cuda/res/ram (Gb): 10.00/8.07/2.92/6.25/13.98 | batch/sps: 10/0.35\n",
      "1720-1729/2031 | total/used/cuda/res/ram (Gb): 10.00/8.11/2.92/6.25/13.96 | batch/sps: 10/0.53\n",
      "1730-1739/2031 | total/used/cuda/res/ram (Gb): 10.00/8.04/2.92/6.25/13.99 | batch/sps: 10/0.54\n",
      "1740-1749/2031 | total/used/cuda/res/ram (Gb): 10.00/8.41/2.92/6.60/13.97 | batch/sps: 10/0.33\n",
      "1750-1759/2031 | total/used/cuda/res/ram (Gb): 10.00/8.40/2.92/6.60/13.97 | batch/sps: 10/0.64\n",
      "1760-1769/2031 | total/used/cuda/res/ram (Gb): 10.00/8.37/2.92/6.60/13.98 | batch/sps: 10/0.76\n",
      "1770-1779/2031 | total/used/cuda/res/ram (Gb): 10.00/8.36/2.92/6.60/13.98 | batch/sps: 10/0.59\n",
      "1780-1789/2031 | total/used/cuda/res/ram (Gb): 10.00/8.36/2.92/6.60/13.98 | batch/sps: 10/0.68\n",
      "1790-1799/2031 | total/used/cuda/res/ram (Gb): 10.00/8.39/2.92/6.60/13.97 | batch/sps: 10/0.53\n",
      "1800-1809/2031 | total/used/cuda/res/ram (Gb): 10.00/8.45/2.92/6.60/13.97 | batch/sps: 10/0.69\n",
      "1810-1819/2031 | total/used/cuda/res/ram (Gb): 10.00/8.26/2.92/6.60/13.98 | batch/sps: 10/0.52\n",
      "1820-1829/2031 | total/used/cuda/res/ram (Gb): 10.00/8.29/2.92/6.60/13.97 | batch/sps: 10/0.75\n",
      "1830-1839/2031 | total/used/cuda/res/ram (Gb): 10.00/8.23/2.92/6.60/14.00 | batch/sps: 10/0.73\n",
      "1840-1849/2031 | total/used/cuda/res/ram (Gb): 10.00/8.23/2.92/6.60/13.97 | batch/sps: 10/0.56\n",
      "1850-1859/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.95 | batch/sps: 10/0.49\n",
      "1860-1869/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.93 | batch/sps: 10/0.45\n",
      "1870-1879/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.94 | batch/sps: 10/0.73\n",
      "1880-1889/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.93 | batch/sps: 10/0.35\n",
      "1890-1899/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.93 | batch/sps: 10/0.52\n",
      "1900-1909/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.94 | batch/sps: 10/0.64\n",
      "1910-1919/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.94 | batch/sps: 10/0.57\n",
      "1920-1929/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.96 | batch/sps: 10/0.51\n",
      "1930-1939/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/14.01 | batch/sps: 10/0.67\n",
      "1940-1949/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.99 | batch/sps: 10/0.48\n",
      "1950-1959/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.97 | batch/sps: 10/0.40\n",
      "1960-1969/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.97 | batch/sps: 10/0.45\n",
      "1970-1979/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.98 | batch/sps: 10/0.72\n",
      "1980-1989/2031 | total/used/cuda/res/ram (Gb): 10.00/8.25/2.92/6.60/13.97 | batch/sps: 10/0.66\n",
      "1990-1999/2031 | total/used/cuda/res/ram (Gb): 10.00/8.72/2.92/7.06/13.97 | batch/sps: 10/0.30\n",
      "2000-2009/2031 | total/used/cuda/res/ram (Gb): 10.00/8.72/2.92/7.06/13.97 | batch/sps: 10/0.77\n",
      "2010-2019/2031 | total/used/cuda/res/ram (Gb): 10.00/8.72/2.92/7.06/13.97 | batch/sps: 10/0.64\n",
      "2020-2029/2031 | total/used/cuda/res/ram (Gb): 10.00/8.72/2.92/7.06/13.98 | batch/sps: 10/0.40\n",
      "2030-2030/2031 | total/used/cuda/res/ram (Gb): 10.00/8.72/2.92/7.06/13.98 | batch/sps: 1/0.10\n",
      "Finished processing 2031 samples for gec.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 2031\n",
      "})\n",
      "Despite the strict Japanese society, I feel happy when I have dinner with my family.\n",
      "Despite the strict Japanese society, I feel happy when I have dinner with my family.\n",
      "2031 | total/used/cuda/res/ram (Gb): 10.00/8.72/2.92/7.06/13.98 | sps: 1.22\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 2031\n",
      "})\n",
      "['Despite the strict Japanese society, I feel happy when I have dinner with my '\n",
      " 'family.',\n",
      " 'They have been increasing rapidly in Japan for a couple of years.']\n",
      "CPU times: user 1h 15min 52s, sys: 8min 8s, total: 1h 24min 1s\n",
      "Wall time: 1h 24min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "# batch_size = 8  # gpt2-large\n",
    "batch_size = 10  # gemma-2b, phi-2\n",
    "max_length = 350\n",
    "max_new_tokens = 350\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"input\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    # print(batch[\"request\"])\n",
    "    inputs = tokenizer(batch[\"request\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    # inputs = tokenizer(batch[\"request\"], return_tensors=\"pt\").inputs.to(device)\n",
    "    # inputs = tokenizer(item[\"request\"], return_tensors=\"pt\").inputs\n",
    "    # print(inputs)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    # outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    outputs = model.generate(**inputs, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=10,\n",
    "    #     num_return_sequences=1,\n",
    "    #     pad_token_id=tokenizer.eos_token_id,\n",
    "    #     max_length=max_length,\n",
    "    # )\n",
    "    # print(outputs)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    processed = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    # print(processed)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for task, samples in test_dataset_dict.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {task}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {task}.\")\n",
    "    print(processed_samples)\n",
    "    print(processed_samples[\"reference\"][0])\n",
    "    print(processed_samples[\"processed\"][0])\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[task] = {\n",
    "        # \"task\": task,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"elapsed_time\": elapsed_time,\n",
    "        \"sps\": sps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_length\": max_length,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_gec_samples = processed_samples_map[\"gec\"][\"samples\"]\n",
    "\n",
    "pprint(processed_gec_samples)\n",
    "pprint(processed_gec_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving eval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: t5-large-coedit, model_id: iliazlobin/t5-large-coedit, model_path: iliazlobin_t5-large-coedit\n",
      "Total/trainable params: 737668096/737668096\n",
      "task: neutralize, samples: 1127\n",
      "s: rouge, v: {'rouge1': 0.9345984440771747, 'rouge2': 0.8660910861945696, 'rougeL': 0.9342957494577149, 'rougeLsum': 0.9343557800065598}\n",
      "s: sacreblue, v: {'score': 88.88925200809757, 'counts': [24720, 22581, 20618, 18795], 'totals': [25905, 24778, 23651, 22526], 'precisions': [95.4255935147655, 91.13326337880378, 87.17601792736036, 83.4369173399627], 'bp': 0.9967241574770993, 'sys_len': 25905, 'ref_len': 25990}\n",
      "s: sari, v: {'sari': 69.22160529964609}\n",
      "s: em, v: {'exact_match': 0.07808340727595386}\n",
      "task: simplification, samples: 1144\n",
      "s: rouge, v: {'rouge1': 0.5756454028137281, 'rouge2': 0.3750410604150481, 'rougeL': 0.5300814613166529, 'rougeLsum': 0.529782834578091}\n",
      "s: sacreblue, v: {'score': 28.159721395031095, 'counts': [11777, 6757, 4337, 2838], 'totals': [21409, 20265, 19121, 17977], 'precisions': [55.00957541220982, 33.34320256600049, 22.681868103132683, 15.78683873838794], 'bp': 0.9891757351673821, 'sys_len': 21409, 'ref_len': 21642}\n",
      "s: sari, v: {'sari': 49.01483553698067}\n",
      "s: em, v: {'exact_match': 0.0}\n",
      "task: paraphrase, samples: 1590\n",
      "s: rouge, v: {'rouge1': 0.49089820911318016, 'rouge2': 0.22372195388542543, 'rougeL': 0.33749433120472594, 'rougeLsum': 0.3376407837585879}\n",
      "s: sacreblue, v: {'score': 13.876131093539326, 'counts': [19959, 7899, 3624, 1700], 'totals': [42656, 41066, 39476, 37886], 'precisions': [46.79060390097524, 19.234890176788586, 9.180261424663087, 4.487145647468722], 'bp': 1.0, 'sys_len': 42656, 'ref_len': 41212}\n",
      "s: sari, v: {'sari': 43.34193178307007}\n",
      "s: em, v: {'exact_match': 0.0006289308176100629}\n",
      "task: coherence, samples: 1062\n",
      "s: rouge, v: {'rouge1': 0.9422943132384911, 'rouge2': 0.8840807795162937, 'rougeL': 0.9218687805376427, 'rougeLsum': 0.9219375830113115}\n",
      "s: sacreblue, v: {'score': 84.62383457439117, 'counts': [30246, 27500, 25120, 22892], 'totals': [32315, 31253, 30191, 29129], 'precisions': [93.59740058796224, 87.99155281093014, 83.20360372296379, 78.58834838133818], 'bp': 0.9878509769535114, 'sys_len': 32315, 'ref_len': 32710}\n",
      "s: sari, v: {'sari': 67.1673642670334}\n",
      "s: em, v: {'exact_match': 0.064030131826742}\n",
      "task: clarity, samples: 126\n",
      "s: rouge, v: {'rouge1': 0.8791005220251644, 'rouge2': 0.8107851968581379, 'rougeL': 0.8749104419149449, 'rougeLsum': 0.8734248626645782}\n",
      "s: sacreblue, v: {'score': 78.99945106369442, 'counts': [3561, 3211, 2959, 2739], 'totals': [4119, 3993, 3867, 3741], 'precisions': [86.4530225782957, 80.41572752316554, 76.5192655805534, 73.21571772253408], 'bp': 1.0, 'sys_len': 4119, 'ref_len': 3943}\n",
      "s: sari, v: {'sari': 61.93433229391854}\n",
      "s: em, v: {'exact_match': 0.0}\n",
      "task: gec, samples: 2031\n",
      "s: rouge, v: {'rouge1': 0.8885424516170547, 'rouge2': 0.7672756698262866, 'rougeL': 0.8814960606839991, 'rougeLsum': 0.8814241965325934}\n",
      "s: sacreblue, v: {'score': 60.42302582123768, 'counts': [40116, 31300, 24729, 19558], 'totals': [46265, 44235, 42208, 40183], 'precisions': [86.70917540257214, 70.75844919181644, 58.58841925701289, 48.672324117163974], 'bp': 0.9342608774508439, 'sys_len': 46265, 'ref_len': 49411}\n",
      "s: sari, v: {'sari': 67.219822862705}\n",
      "s: em, v: {'exact_match': 0.10142786804529788}\n",
      "{'model': 'iliazlobin/t5-large-coedit', 'hardware': 'HomeDesktop (RTX3080)', 'total_params': 737668096, 'neutralize': {'task': 'neutralize', 'total_samples': 1127, 'elapsed_time': 99.55471134185791, 'sps': 11.320408495083962, 'batch_size': 20, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.9345984440771747, 'rouge2': 0.8660910861945696, 'rougeL': 0.9342957494577149, 'rougeLsum': 0.9343557800065598}, 'sacreblue': {'score': 88.88925200809757, 'counts': [24720, 22581, 20618, 18795], 'totals': [25905, 24778, 23651, 22526], 'precisions': [95.4255935147655, 91.13326337880378, 87.17601792736036, 83.4369173399627], 'bp': 0.9967241574770993, 'sys_len': 25905, 'ref_len': 25990}, 'sari': {'sari': 69.22160529964609}, 'em': {'exact_match': 0.07808340727595386}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8456749056, 'cuda_allocated': 2959717888, 'cuda_reserved': 5257560064, 'ram_usage': 16314470400}}, 'simplification': {'task': 'simplification', 'total_samples': 1144, 'elapsed_time': 72.35123538970947, 'sps': 15.811754890403865, 'batch_size': 20, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.5756454028137281, 'rouge2': 0.3750410604150481, 'rougeL': 0.5300814613166529, 'rougeLsum': 0.529782834578091}, 'sacreblue': {'score': 28.159721395031095, 'counts': [11777, 6757, 4337, 2838], 'totals': [21409, 20265, 19121, 17977], 'precisions': [55.00957541220982, 33.34320256600049, 22.681868103132683, 15.78683873838794], 'bp': 0.9891757351673821, 'sys_len': 21409, 'ref_len': 21642}, 'sari': {'sari': 49.01483553698067}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8612265984, 'cuda_allocated': 2959717888, 'cuda_reserved': 5479858176, 'ram_usage': 20779782144}}, 'paraphrase': {'task': 'paraphrase', 'total_samples': 1590, 'elapsed_time': 92.76879668235779, 'sps': 17.139383681392264, 'batch_size': 20, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.49089820911318016, 'rouge2': 0.22372195388542543, 'rougeL': 0.33749433120472594, 'rougeLsum': 0.3376407837585879}, 'sacreblue': {'score': 13.876131093539326, 'counts': [19959, 7899, 3624, 1700], 'totals': [42656, 41066, 39476, 37886], 'precisions': [46.79060390097524, 19.234890176788586, 9.180261424663087, 4.487145647468722], 'bp': 1.0, 'sys_len': 42656, 'ref_len': 41212}, 'sari': {'sari': 43.34193178307007}, 'em': {'exact_match': 0.0006289308176100629}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8624578560, 'cuda_allocated': 2959717888, 'cuda_reserved': 5479858176, 'ram_usage': 20709875712}}, 'coherence': {'task': 'coherence', 'total_samples': 1062, 'elapsed_time': 80.70291924476624, 'sps': 13.159375273390411, 'batch_size': 20, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.9422943132384911, 'rouge2': 0.8840807795162937, 'rougeL': 0.9218687805376427, 'rougeLsum': 0.9219375830113115}, 'sacreblue': {'score': 84.62383457439117, 'counts': [30246, 27500, 25120, 22892], 'totals': [32315, 31253, 30191, 29129], 'precisions': [93.59740058796224, 87.99155281093014, 83.20360372296379, 78.58834838133818], 'bp': 0.9878509769535114, 'sys_len': 32315, 'ref_len': 32710}, 'sari': {'sari': 67.1673642670334}, 'em': {'exact_match': 0.064030131826742}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8605736960, 'cuda_allocated': 2959717888, 'cuda_reserved': 5479858176, 'ram_usage': 20725186560}}, 'clarity': {'task': 'clarity', 'total_samples': 126, 'elapsed_time': 18.039865732192993, 'sps': 6.984530920047096, 'batch_size': 20, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.8791005220251644, 'rouge2': 0.8107851968581379, 'rougeL': 0.8749104419149449, 'rougeLsum': 0.8734248626645782}, 'sacreblue': {'score': 78.99945106369442, 'counts': [3561, 3211, 2959, 2739], 'totals': [4119, 3993, 3867, 3741], 'precisions': [86.4530225782957, 80.41572752316554, 76.5192655805534, 73.21571772253408], 'bp': 1.0, 'sys_len': 4119, 'ref_len': 3943}, 'sari': {'sari': 61.93433229391854}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 8612749312, 'cuda_allocated': 2959717888, 'cuda_reserved': 5479858176, 'ram_usage': 20740374528}}, 'gec': {'task': 'gec', 'total_samples': 2031, 'elapsed_time': 137.90735483169556, 'sps': 14.727278341888773, 'batch_size': 20, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.8885424516170547, 'rouge2': 0.7672756698262866, 'rougeL': 0.8814960606839991, 'rougeLsum': 0.8814241965325934}, 'sacreblue': {'score': 60.42302582123768, 'counts': [40116, 31300, 24729, 19558], 'totals': [46265, 44235, 42208, 40183], 'precisions': [86.70917540257214, 70.75844919181644, 58.58841925701289, 48.672324117163974], 'bp': 0.9342608774508439, 'sys_len': 46265, 'ref_len': 49411}, 'sari': {'sari': 67.219822862705}, 'em': {'exact_match': 0.10142786804529788}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 9815482368, 'cuda_allocated': 2959717888, 'cuda_reserved': 7293894656, 'ram_usage': 20715585536}}}\n"
     ]
    }
   ],
   "source": [
    "hardware = \"HomeDesktop (RTX3080)\"\n",
    "# hardware = \"NC24 (A100)\"\n",
    "print(f\"model_name: {model_name}, model_id: {model_id}, model_path: {model_path}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "if list in globals():\n",
    "    del list\n",
    "\n",
    "all_flats = []\n",
    "all_scores = []\n",
    "if os.path.exists(\"results/all-scores.csv\"):\n",
    "    all_scores = pd.read_csv(\"results/all-scores.csv\").to_dict(\"records\")\n",
    "\n",
    "all_fulls = []\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "\n",
    "def calculate_scores(processed_samples):\n",
    "    rouge_score = rouge_metric.compute(\n",
    "        predictions=processed_samples[\"processed\"], references=processed_samples[\"references\"]\n",
    "    )\n",
    "    # pprint(rouge_score)\n",
    "\n",
    "    sacreblue_score = sacreblue_metric.compute(\n",
    "        predictions=processed_samples[\"processed\"], references=processed_samples[\"references\"]\n",
    "    )\n",
    "    # pprint(sacreblue_score)\n",
    "\n",
    "    sari_score = sari_metric.compute(\n",
    "        sources=processed_samples[\"input\"],\n",
    "        predictions=processed_samples[\"processed\"],\n",
    "        references=processed_samples[\"references\"],\n",
    "    )\n",
    "    # pprint(sari_score)\n",
    "\n",
    "    em_score = em_metric.compute(predictions=processed_samples[\"processed\"], references=processed_samples[\"reference\"])\n",
    "    # pprint(em_score)\n",
    "\n",
    "    return {\n",
    "        \"rouge\": rouge_score,\n",
    "        \"sacreblue\": sacreblue_score,\n",
    "        \"sari\": sari_score,\n",
    "        \"em\": em_score,\n",
    "    }\n",
    "\n",
    "\n",
    "for task, obj in processed_samples_map.items():\n",
    "    print(f\"task: {task}, samples: {len(obj['samples'])}\")\n",
    "\n",
    "    batch = obj[\"samples\"]\n",
    "    total_samples = len(batch)\n",
    "\n",
    "    all_saved_samples = batch.remove_columns([\"references\"])\n",
    "    saved_samples = all_saved_samples[:100] if len(all_saved_samples) > 100 else all_saved_samples\n",
    "    flats_frame = pd.DataFrame.from_records(saved_samples)\n",
    "    flats_frame.to_json(f\"samples/{model_path}_{task}.json\", orient=\"records\")\n",
    "\n",
    "    scores = calculate_scores(batch)\n",
    "    # pprint(scores)\n",
    "\n",
    "    score_paths = [\n",
    "        \"rouge.rouge1\",\n",
    "        # \"rouge.rouge2\",\n",
    "        # \"rouge.rougeL\",\n",
    "        # \"rouge.rougeLsum\",\n",
    "        \"sacreblue.score\",\n",
    "        \"sari.sari\",\n",
    "        \"em.exact_match\",\n",
    "    ]\n",
    "\n",
    "    normalized_scores = {}\n",
    "    for s, v in scores.items():\n",
    "        print(f\"s: {s}, v: {v}\")\n",
    "        for ss, vv in v.items():\n",
    "            if not isinstance(vv, list):\n",
    "                # normalized_scores[f\"score.{k}.{ss}\"] = vv\n",
    "                path = f\"{s}.{ss}\"\n",
    "                if path in score_paths:\n",
    "                    normalized_scores[f\"score.{s}.{ss}\"] = vv\n",
    "    # pprint(normalized_scores)\n",
    "\n",
    "    normalized_utilization = {}\n",
    "    for s, v in obj[\"utilization\"].items():\n",
    "        if not isinstance(v, list):\n",
    "            normalized_utilization[f\"utilization.{s}\"] = v\n",
    "    # print(normalized_utilization)\n",
    "\n",
    "    flat_dict = {\n",
    "        \"model\": model_id,\n",
    "        \"hardware\": hardware,\n",
    "        \"total_params\": total_params,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"elapsed_time\": obj[\"elapsed_time\"],\n",
    "        \"sps\": obj[\"sps\"],\n",
    "        \"batch_size\": obj[\"batch_size\"],\n",
    "        \"max_length\": obj[\"max_length\"],\n",
    "        \"task\": task,\n",
    "    }\n",
    "    flat_dict.update(normalized_scores)\n",
    "    flat_dict.update(normalized_utilization)\n",
    "    # pprint(frame)\n",
    "\n",
    "    all_flats.append(flat_dict)\n",
    "    all_scores.append(flat_dict)\n",
    "\n",
    "    fulls_frame = {\n",
    "        \"task\": task,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"elapsed_time\": obj[\"elapsed_time\"],\n",
    "        \"sps\": obj[\"sps\"],\n",
    "        \"batch_size\": obj[\"batch_size\"],\n",
    "        \"max_length\": obj[\"max_length\"],\n",
    "    }\n",
    "    fulls_frame.update(\n",
    "        {\n",
    "            \"scores\": scores,\n",
    "            \"utilization\": obj[\"utilization\"],\n",
    "        }\n",
    "    )\n",
    "    all_fulls.append(fulls_frame)\n",
    "\n",
    "flats_frame = pd.DataFrame.from_records(all_flats)\n",
    "flats_frame.to_csv(f\"results/{model_path}.csv\", index=False)\n",
    "\n",
    "scores_frame = pd.DataFrame.from_records(all_scores)\n",
    "scores_frame.to_csv(f\"results/all-scores.csv\", index=False)\n",
    "\n",
    "fulls_dict = {\n",
    "    \"model\": model_id,\n",
    "    \"hardware\": hardware,\n",
    "    \"total_params\": total_params,\n",
    "}\n",
    "for full in all_fulls:\n",
    "    fulls_dict[full[\"task\"]] = full\n",
    "\n",
    "print(fulls_dict)\n",
    "fulls_frame = pd.DataFrame.from_records([fulls_dict])\n",
    "fulls_frame.to_json(f\"results/{model_path}.json\", orient=\"records\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
