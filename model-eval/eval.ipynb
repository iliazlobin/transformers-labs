{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install huggingface_hub\n",
    "# %pip install transformers evaluate gradio datasets chardet cchardet librosa ipython sentencepiece plotly phonemizer\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading custom gpt2 / gpt2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large,model_id: openai-community/gpt2-large,model_path: openai-community_gpt2-large\n",
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "50256\n",
      "50256\n",
      "left\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"padding_side\": \"left\"\n",
      "}\n",
      "\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1280)\n",
      "    (wpe): Embedding(1024, 1280)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-35): 36 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "model_repo = f\"openai-community\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "model.generation_config.max_new_tokens = 0\n",
    "# model.generation_config.new_tokens = 350\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large-coedit,model_id: iliazlobin/gpt2-large-coedit,model_path: iliazlobin_gpt2-large-coedit\n",
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "50256\n",
      "50256\n",
      "left\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"iliazlobin/gpt2-large-coedit\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_new_tokens\": 350,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"padding_side\": \"left\"\n",
      "}\n",
      "\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1280)\n",
      "    (wpe): Embedding(1024, 1280)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-35): 36 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large-coedit\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "print(model.config)\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# model.generation_config.new_tokens = 350\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "model_repo = f\"openai-community\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "# print(model.config)\n",
    "# print(model)\n",
    "\n",
    "trained_model_name = \"gpt2-large-bnb8-coedit\"\n",
    "trained_model_repo = f\"iliazlobin\"\n",
    "trained_model_id = f\"{trained_model_repo}/{trained_model_name}\"\n",
    "trained_model_checkpoint = f\"{trained_model_repo}/{trained_model_name}\"\n",
    "trained_model_path = f\"{trained_model_repo}_{trained_model_name}\"\n",
    "print(\n",
    "    f\"trained_model_name: {trained_model_name},\"\n",
    "    f\"trained_model_id: {trained_model_id},\"\n",
    "    f\"trained_model_path: {trained_model_path}\"\n",
    ")\n",
    "\n",
    "\n",
    "adapters_path = f\"../model-train/model-{trained_model_repo}_{trained_model_name}\"\n",
    "peft_model = PeftModel.from_pretrained(model, adapters_path)\n",
    "\n",
    "print(type(peft_model))\n",
    "print(peft_model.config)\n",
    "print(peft_model)\n",
    "\n",
    "origin_model = model\n",
    "model = peft_model\n",
    "model_name = trained_model_name\n",
    "model_repo = trained_model_repo\n",
    "model_id = trained_model_id\n",
    "model_checkpoint = trained_model_checkpoint\n",
    "model_path = trained_model_path\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BART\n",
    "* https://huggingface.co/facebook/bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: bart-large,model_id: facebook/bart-large,model_path: facebook_bart-large\n",
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BartForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"bart-large\"\n",
    "model_repo = f\"facebook\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_id)\n",
    "tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: bart-large-coedit,model_id: iliazlobin/bart-large-coedit,model_path: iliazlobin_bart-large-coedit\n",
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BartForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"bart-large-coedit\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_id)\n",
    "tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0, max_memory={0: \"20GIB\"})\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "BartConfig {\n",
      "  \"_name_or_path\": \"iliazlobin/bart-grammarly\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"iliazlobin/bart-grammarly\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# model = BartModel.from_pretrained(model_name)\n",
    "# model = BartForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading google/t5-large\n",
    "* https://huggingface.co/google-t5/t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: t5-large,model_id: google-t5/t5-large,model_path: google-t5_t5-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-large\"\n",
    "model_repo = f\"google-t5\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_id)\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0, max_memory={0: \"20GIB\"})\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=0)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading grammarly/coedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: coedit-large,model_id: grammarly/coedit-large,model_path: grammarly_coedit-large\n",
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"grammarly/coedit-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"coedit-large\"\n",
    "model_repo = f\"grammarly\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(type(tokenizer))\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gemma-2b\"\n",
    "# model_name = \"gemma-7b-it\"\n",
    "# model_name = \"gemma-7b\"\n",
    "model_repo = f\"google\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gemma-2b-coedit,model_id: iliazlobin/gemma-2b-coedit,model_path: iliazlobin_gemma-2b-coedit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "1\n",
      "0\n",
      "left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c958100cb84d138f313964a2c0621f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.modeling_gemma.GemmaForCausalLM'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from AutoGPTQ.auto_gptq import quantization\n",
    "\n",
    "model_name = \"gemma-2b-coedit\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Phi-2\n",
    "* https://huggingface.co/microsoft/phi-2\n",
    "* https://huggingface.co/TheBloke/phi-2-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token: <|endoftext|>\n",
      "PhiConfig {\n",
      "  \"_name_or_path\": \"TheBloke/phi-2-GPTQ\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"TheBloke/phi-2-GPTQ--configuration_phi.PhiConfig\",\n",
      "    \"AutoModelForCausalLM\": \"TheBloke/phi-2-GPTQ--modeling_phi.PhiForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"flash_attn\": false,\n",
      "  \"flash_rotary\": false,\n",
      "  \"fused_dense\": false,\n",
      "  \"img_processor\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"phi-msft\",\n",
      "  \"n_embd\": 2560,\n",
      "  \"n_head\": 32,\n",
      "  \"n_head_kv\": null,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 32,\n",
      "  \"n_positions\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": true,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"phi-2\"\n",
    "model_repo = f\"microsoft\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model.config)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading mixtral-8x7B\n",
    "* https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "* https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ\n",
    "\n",
    "Quantization\n",
    "* https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from transformers import BitsAndBytesConfig\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     29\u001b[0m     model_name,\n\u001b[1;32m     30\u001b[0m     use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# tokenizer.pad_token = tokenizer.eos_token\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3039\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3039\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3043\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/quantizers/auto.py:153\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m AutoQuantizationConfig\u001b[38;5;241m.\u001b[39mfrom_dict(quantization_config)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig)) \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ config collision\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m \u001b[43mquantization_config_from_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loading_attributes\u001b[49m()\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m loading_attr_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(quantization_config, attr, val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'"
     ]
    }
   ],
   "source": [
    "# model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "model_name = 'TheBloke/Mixtral-8x7B-v0.1-GPTQ'\n",
    "model_alias = model_name.replace('/', '_')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=0,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # load_in_4bit=True,\n",
    "#     # quantization_config=quantization_config,\n",
    "# )\n",
    "\n",
    "# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\n",
    "# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# text = 'Hello my name is'\n",
    "# inputs = tokenizer(text, return_tensors='pt')\n",
    "# outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"TheBloke/Llama-2-7B-GPTQ\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": false,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_name = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)\n",
    "\n",
    "# from auto_gptq import exllama_set_max_input_length\n",
    "# model = exllama_set_max_input_length(model, max_input_length=2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly/coedit dataset\n",
    "* https://huggingface.co/datasets/grammarly/coedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set {'clarity', 'simplification', 'gec', 'neutralize', 'paraphrase', 'coherence'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 63703\n",
      "})\n",
      "test set {'clarity', 'simplification', 'gec', 'neutralize', 'paraphrase', 'coherence'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 7080\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 63703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 7080\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'], 'request': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'prompt': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "# print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "# train_ratio = 0.01\n",
    "# test_ratio = 0.001\n",
    "# train_ratio = 0.1\n",
    "# test_ratio = 0.01\n",
    "train_ratio = 0.9\n",
    "test_ratio = 0.1\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "train_gec_dataset = gec_dataset.select(range(0, int(train_ratio * len(gec_dataset))))\n",
    "test_gec_dataset = gec_dataset.select(range(int((1 - test_ratio) * len(gec_dataset)), len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, int(train_ratio * len(simplification_dataset))))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(simplification_dataset)), len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, int(train_ratio * len(clarity_dataset))))\n",
    "test_clarity_dataset = clarity_dataset.select(range(int((1 - test_ratio) * len(clarity_dataset)), len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, int(train_ratio * len(coherence_dataset))))\n",
    "test_coherence_dataset = coherence_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(coherence_dataset)), len(coherence_dataset))\n",
    ")\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, int(train_ratio * len(paraphrase_dataset))))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(paraphrase_dataset)), len(paraphrase_dataset))\n",
    ")\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(train_ratio * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, int(train_ratio * len(neutralize_dataset))))\n",
    "test_neutralize_dataset = neutralize_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(neutralize_dataset)), len(neutralize_dataset))\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "print(train_dataset)\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "def add_prompt(item):\n",
    "    return {\n",
    "        \"request\": f\"{item['input']}\\nResponse:\",\n",
    "        \"prompt\": f\"{item['input']}\\nResponse:{item['reference']}\",\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "dataset = dataset.map(add_prompt)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_length train: 342\n",
      "max_input_length test: 248\n"
     ]
    }
   ],
   "source": [
    "# find the longest sequence in the dataset\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"train\"])\n",
    "print(f\"max_input_length train: {max_input_length}\")\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"test\"])\n",
    "print(f\"max_input_length test: {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/clarity: 1126\n",
      "train/simplification: 10296\n",
      "train/gec: 18277\n",
      "train/neutralize: 10143\n",
      "train/paraphrase: 14307\n",
      "train/coherence: 9554\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    train_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    train_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "train_dataset_map = {}\n",
    "for task, l in train_lists_map.items():\n",
    "    train_dataset_map[task] = Dataset.from_list(l)\n",
    "# print(train_dataset_map)\n",
    "\n",
    "train_dataset_dict = DatasetDict(train_dataset_map)\n",
    "# print(train_dataset_dict)\n",
    "\n",
    "# for task, ds in train_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"train/{task}: {len(train_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/clarity: 126\n",
      "test/simplification: 1144\n",
      "test/gec: 2031\n",
      "test/neutralize: 1127\n",
      "test/paraphrase: 1590\n",
      "test/coherence: 1062\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    test_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    test_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "test_dataset_map = {}\n",
    "for task, l in test_lists_map.items():\n",
    "    test_dataset_map[task] = Dataset.from_list(l)\n",
    "# print(test_dataset_map)\n",
    "\n",
    "test_dataset_dict = DatasetDict(test_dataset_map)\n",
    "# print(test_dataset_dict)\n",
    "\n",
    "# for task, ds in test_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"test/{task}: {len(test_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gec\n",
      "Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.\n",
      "Despite the strict Japanese society, I feel happy when I have dinner with my family.\n",
      "Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.\n",
      "Response:Despite the strict Japanese society, I feel happy when I have dinner with my family.\n",
      "Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][0][\"task\"])\n",
    "print(dataset[\"test\"][0][\"input\"])\n",
    "print(dataset[\"test\"][0][\"reference\"])\n",
    "print(dataset[\"test\"][0][\"prompt\"])\n",
    "print(dataset[\"test\"][0][\"request\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=350) and `max_length`(=350) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> gec\n",
      "input: ['Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.', 'Fix grammaticality in this sentence: They are increasing rapidly in Japan for a couple of years.']\n",
      "result: ['Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.', 'Fix grammaticality in this sentence: They are increasing rapidly in Japan for a couple of years.']\n"
     ]
    }
   ],
   "source": [
    "max_batch = 2\n",
    "max_length = 350\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"input: {input_batch['input']}\")\n",
    "\n",
    "    input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> paraphrase\n",
      "request: ['Write a paraphrased version of the sentence: Still, the thought of Speedy Parker danced at the edge of his mind as Jack ambled across the boardwalk and down to the depressingly empty beach.\\nResponse:', 'Rewrite the sentence with different wording: \"The proposal, if your Majesty will forgive my saying so, is most generous,\" Valgon observed, concluding his reading of the latest treaty offered by Ran Borune.\\nResponse:']\n",
      "reference: ['Yet, as Jack walked slowly over the wooden gangway to the depressed, deserted beach, the thought of Speedy Parker was dancing on the boundary of his awareness.', '\"This suggestion is, indeed, very noble, \" said Valgon after completing his readings about the recent treaty proposed by Ran Borune, \"if your Majesty allows me to call it that.']\n",
      "result: ['But as Jack walked along the boardwalk and onto the empty beach, the idea of Speedy Parker still danced in his head.', 'Valgon concluded his reading of the latest treaty Ran Borune had offered, and said, \"Your Majesty, this proposal is most generous.\"']\n",
      "\n",
      ">> coherence\n",
      "request: ['Fix coherence: Outside the town, 6 tourists were reported killed. Official documents indicate that at least 255 local residents were killed, with a further 29 never found.\\nResponse:', 'Make the text more cohesive: Whereas at some times (and in some places) a Corps of two divisions was sufficient, at other times 5 or 6 divisions were necessary. Under the Hindenburg regime (from summer 1916), new Corps headquarters were created without organic divisions.\\nResponse:']\n",
      "reference: ['Outside the town, 6 tourists were reported killed. However, official documents indicate that at least 255 local residents were killed, with a further 29 never found.', 'Whereas at some times (and in some places) a Corps of two divisions was sufficient, at other times 5 or 6 divisions were necessary. Therefore, under the Hindenburg regime (from summer 1916), new Corps headquarters were created without organic divisions.']\n",
      "result: ['Outside the town, 6 tourists were reported killed, but official documents indicate that at least 255 local residents were killed, with a further 29 never found.', 'Whereas at some times (and in some places) a Corps of two divisions was sufficient, at other times 5 or 6 divisions were necessary. Furthermore, under the Hindenburg regime (from summer 1916), new Corps headquarters were created without organic divisions.']\n",
      "\n",
      ">> simplification\n",
      "request: ['Change to simpler wording: \" Torchwood \" launched with 2.4 million viewers in October 2006.\\nResponse:', 'Simplify this sentence: Felony disenfranchisement in Florida began with the 1838 ratification of the state constitution.\\nResponse:']\n",
      "reference: ['When the first episode of Torchwood was launched in October 2006 on BBC Three, 2.4 million people watched it.', 'Florida ’ s constitution was ratified in 1838 and with that felony disenfranchisement was established in Florida.']\n",
      "result: ['It was watched by 2.4 million people in its first broadcast in October 2006.', 'Felony disenfranchisement in Florida began in 1838.']\n",
      "\n",
      ">> clarity\n",
      "request: ['Use clearer wording: Canals are waterways channels, or artificial waterways, for water conveyance, or to service water transport vehicles.\\nResponse:', 'Clarify this text: Canals are waterways channels, or artificial waterways, for water conveyance, or to service water transport vehicles.\\nResponse:']\n",
      "reference: ['Canals are waterway channels, or artificial waterways, for water conveyance, or to service water transport vehicles.', 'Canals are waterways channels, or artificial waterways, for water conveyance, or for servicing water transport vehicles.']\n",
      "result: ['Canals are waterways channels, or artificial waterways, for water conveyance, or to service water transport vehicles.', 'Canals are waterways channels, or artificial waterways, for water conveyance, or to service water transport vehicles.']\n",
      "\n",
      ">> neutralize\n",
      "request: ['Remove non-neutral POV: new moon received poor reviews from critics.\\nResponse:', 'Remove POVs in this text: rhonda shear (born 1954), american television personality, comedienne, and actress\\nResponse:']\n",
      "reference: ['new moon received negative reviews from critics.', 'rhonda shear (born 1954), american television personality, comedian, and actress']\n",
      "result: ['new moon received mixed reviews from critics.', 'rhona shear (born 1954), american television personality, comedian, and actress']\n",
      "\n",
      ">> gec\n",
      "request: ['Fix grammaticality in this sentence: Despite strict japanese society, I feel happy when I had dinner with my family.\\nResponse:', 'Fix grammaticality in this sentence: They are increasing rapidly in Japan for a couple of years.\\nResponse:']\n",
      "reference: ['Despite the strict Japanese society, I feel happy when I have dinner with my family.', 'They have been increasing rapidly in Japan for over a couple of years.']\n",
      "result: ['Despite the strict Japanese society, I feel happy when I have dinner with my family.', 'They have been increasing rapidly in Japan for a couple of years.']\n"
     ]
    }
   ],
   "source": [
    "max_batch = 2\n",
    "max_length = 350\n",
    "max_new_tokens = 350\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"request: {input_batch['request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch['request'], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    # outputs = model.generate(**inputs, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=10,\n",
    "    #     num_return_sequences=1,\n",
    "    #     pad_token_id=tokenizer.eos_token_id,\n",
    "    #     # return_attention_mask=True,\n",
    "    #     max_length=256,\n",
    "    # )\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total/trainable params: 2506172416/524363776\n",
      "{'total_memory': 10736893952, 'memory_used': 4461305856, 'cuda_allocated': 3033374720, 'cuda_reserved': 3066036224, 'ram_usage': 11819257856}\n",
      "{'total_memory': '10.00', 'memory_used': '4.15', 'cuda_allocated': '2.83', 'cuda_reserved': '2.86', 'ram_usage': '11.01'}\n",
      "total/used/cuda/res/ram(Gb): 10.00/4.15/2.83/2.86/11.01\n",
      "Total/used/available memory (Gb): 10.00/{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\n",
      "Recommended/actual fraction: 0.58/0.95\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total/trainable params: {total_params}/{total_trainable_params}')\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "print(utilization)\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(utilization_str)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram(Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "actual_fraction = 0.95\n",
    "available_memory = utilization['total_memory'] - utilization['memory_used']\n",
    "recommended_fraction = available_memory / utilization['total_memory']\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"Total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    \"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f'Recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}')\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.empty(utilization['total_memory'] // 2, dtype=torch.int8, device='cuda')\n",
    "# print_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval encoder-decoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 20 # home: t5, grammarly/coedit\n",
    "# batch_size = 100\n",
    "max_length = 350\n",
    "\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"input\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    inputs = tokenizer(batch[\"input\"], max_length=max_length, padding=True, return_tensors=\"pt\").to(device)\n",
    "    # input_ids = tokenizer(batch['task'], return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(item['task'], return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(input_ids, max_length=512)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    # print(f\"outputs: {outputs}\")\n",
    "    processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    return {\"processed\": processed}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for task, samples in test_dataset_dict.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {task}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {task}.\")\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[task] = {\n",
    "        # \"task\": task,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"elapsed_time\": elapsed_time,\n",
    "        \"sps\": sps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_length\": max_length,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_gec_samples = processed_samples_map[\"gec\"][\"samples\"]\n",
    "\n",
    "pprint(processed_gec_samples)\n",
    "pprint(processed_gec_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval decoder-only models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "total/used/cuda/res/ram (Gb): 10.00/4.17/2.83/2.86/11.08\n",
      "Processing 126 samples for clarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416ba3596876420fb1b13cccbfaf2e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.46 | batch/sps: 10/0.55\n",
      "10-19/126 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.85/6.08/11.46 | batch/sps: 10/0.75\n",
      "20-29/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.45 | batch/sps: 10/0.89\n",
      "30-39/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.44 | batch/sps: 10/1.36\n",
      "40-49/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.44 | batch/sps: 10/1.26\n",
      "50-59/126 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.85/6.08/11.44 | batch/sps: 10/0.92\n",
      "60-69/126 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.85/6.08/11.44 | batch/sps: 10/1.28\n",
      "70-79/126 | total/used/cuda/res/ram (Gb): 10.00/7.34/2.85/6.08/11.44 | batch/sps: 10/0.74\n",
      "80-89/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.45 | batch/sps: 10/0.99\n",
      "90-99/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.43 | batch/sps: 10/1.31\n",
      "100-109/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.46 | batch/sps: 10/1.44\n",
      "110-119/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.45 | batch/sps: 10/0.93\n",
      "120-125/126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.44 | batch/sps: 6/0.58\n",
      "Finished processing 126 samples for clarity.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 126\n",
      "})\n",
      "Canals are waterway channels, or artificial waterways, for water conveyance, or to service water transport vehicles.\n",
      "Canals are waterways channels, or artificial waterways, for water conveyance, or to service water transport vehicles.\n",
      "126 | total/used/cuda/res/ram (Gb): 10.00/7.35/2.85/6.08/11.44 | sps: 0.90\n",
      "Processing 1144 samples for simplification\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e8315234294c11b0d1f096d7b9d56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/2.32\n",
      "10-19/1144 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.01 | batch/sps: 10/1.48\n",
      "20-29/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/2.60\n",
      "30-39/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.01 | batch/sps: 10/1.57\n",
      "40-49/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/2.56\n",
      "50-59/1144 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.02 | batch/sps: 10/1.60\n",
      "60-69/1144 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.02 | batch/sps: 10/1.74\n",
      "70-79/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.02 | batch/sps: 10/1.99\n",
      "80-89/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.02 | batch/sps: 10/2.66\n",
      "90-99/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/1.53\n",
      "100-109/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/2.21\n",
      "110-119/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.01 | batch/sps: 10/2.43\n",
      "120-129/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/2.26\n",
      "130-139/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/1.71\n",
      "140-149/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/2.99\n",
      "150-159/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/1.84\n",
      "160-169/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.68\n",
      "170-179/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/2.34\n",
      "180-189/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.16\n",
      "190-199/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/1.89\n",
      "200-209/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.78\n",
      "210-219/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/2.15\n",
      "220-229/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.01 | batch/sps: 10/2.04\n",
      "230-239/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.97\n",
      "240-249/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.04\n",
      "250-259/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/1.54\n",
      "260-269/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/2.13\n",
      "270-279/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/1.92\n",
      "280-289/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.99 | batch/sps: 10/1.83\n",
      "290-299/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.01 | batch/sps: 10/1.14\n",
      "300-309/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/2.14\n",
      "310-319/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/1.66\n",
      "320-329/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.99 | batch/sps: 10/1.96\n",
      "330-339/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.99 | batch/sps: 10/1.84\n",
      "340-349/1144 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/11.99 | batch/sps: 10/1.61\n",
      "350-359/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.99 | batch/sps: 10/3.31\n",
      "360-369/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.99 | batch/sps: 10/2.07\n",
      "370-379/1144 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/11.99 | batch/sps: 10/1.87\n",
      "380-389/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/2.24\n",
      "390-399/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/2.88\n",
      "400-409/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.99 | batch/sps: 10/2.08\n",
      "410-419/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.99 | batch/sps: 10/3.03\n",
      "420-429/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.00 | batch/sps: 10/2.19\n",
      "430-439/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.98 | batch/sps: 10/2.05\n",
      "440-449/1144 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/11.99 | batch/sps: 10/1.21\n",
      "450-459/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.99 | batch/sps: 10/2.07\n",
      "460-469/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.98 | batch/sps: 10/1.89\n",
      "470-479/1144 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/11.98 | batch/sps: 10/1.32\n",
      "480-489/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/11.98 | batch/sps: 10/2.91\n",
      "490-499/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.70\n",
      "500-509/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/1.92\n",
      "510-519/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.96\n",
      "520-529/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/2.62\n",
      "530-539/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/1.98\n",
      "540-549/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.01 | batch/sps: 10/3.17\n",
      "550-559/1144 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.01 | batch/sps: 10/1.58\n",
      "560-569/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/1.95\n",
      "570-579/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.44\n",
      "580-589/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.00 | batch/sps: 10/3.55\n",
      "590-599/1144 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/11.99 | batch/sps: 10/2.22\n",
      "600-609/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.00 | batch/sps: 10/1.65\n",
      "610-619/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/1.98\n",
      "620-629/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.19\n",
      "630-639/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/3.22\n",
      "640-649/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.01 | batch/sps: 10/1.54\n",
      "650-659/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/11.97 | batch/sps: 10/1.55\n",
      "660-669/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.02 | batch/sps: 10/2.00\n",
      "670-679/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.02 | batch/sps: 10/1.75\n",
      "680-689/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.02 | batch/sps: 10/1.86\n",
      "690-699/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.01 | batch/sps: 10/2.48\n",
      "700-709/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.24\n",
      "710-719/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.00 | batch/sps: 10/1.78\n",
      "720-729/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.08\n",
      "730-739/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.01 | batch/sps: 10/2.12\n",
      "740-749/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.00 | batch/sps: 10/1.60\n",
      "750-759/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.55\n",
      "760-769/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.58\n",
      "770-779/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/11.99 | batch/sps: 10/1.84\n",
      "780-789/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/11.99 | batch/sps: 10/1.86\n",
      "790-799/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.23\n",
      "800-809/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.00 | batch/sps: 10/1.88\n",
      "810-819/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.33\n",
      "820-829/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.37\n",
      "830-839/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.39\n",
      "840-849/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.01 | batch/sps: 10/3.63\n",
      "850-859/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.88\n",
      "860-869/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.88\n",
      "870-879/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/1.95\n",
      "880-889/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.60\n",
      "890-899/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.42\n",
      "900-909/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/1.91\n",
      "910-919/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.86\n",
      "920-929/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.17\n",
      "930-939/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.15\n",
      "940-949/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.01 | batch/sps: 10/2.21\n",
      "950-959/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.00 | batch/sps: 10/1.69\n",
      "960-969/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.76\n",
      "970-979/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.05\n",
      "980-989/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/11.99 | batch/sps: 10/2.01\n",
      "990-999/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/11.99 | batch/sps: 10/1.48\n",
      "1000-1009/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.00 | batch/sps: 10/2.84\n",
      "1010-1019/1144 | total/used/cuda/res/ram (Gb): 10.00/9.39/2.85/8.04/12.00 | batch/sps: 10/1.64\n",
      "1020-1029/1144 | total/used/cuda/res/ram (Gb): 10.00/9.40/2.85/8.04/12.00 | batch/sps: 10/2.93\n",
      "1030-1039/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.01 | batch/sps: 10/1.78\n",
      "1040-1049/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/11.97 | batch/sps: 10/2.65\n",
      "1050-1059/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.01 | batch/sps: 10/2.39\n",
      "1060-1069/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.02 | batch/sps: 10/2.82\n",
      "1070-1079/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.01 | batch/sps: 10/2.04\n",
      "1080-1089/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.01 | batch/sps: 10/3.00\n",
      "1090-1099/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.02 | batch/sps: 10/2.02\n",
      "1100-1109/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.02 | batch/sps: 10/2.13\n",
      "1110-1119/1144 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.02 | batch/sps: 10/2.34\n",
      "1120-1129/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.02 | batch/sps: 10/3.63\n",
      "1130-1139/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.02 | batch/sps: 10/1.89\n",
      "1140-1143/1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.01 | batch/sps: 4/0.91\n",
      "Finished processing 1144 samples for simplification.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 1144\n",
      "})\n",
      "When the first episode of Torchwood was launched in October 2006 on BBC Three, 2.4 million people watched it.\n",
      "It was watched by 2.4 million people in its first broadcast in October 2006.\n",
      "1144 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.01 | sps: 2.06\n",
      "Processing 2031 samples for gec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68791cf1401d4cff80a3e1001f7fdbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/2031 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.43 | batch/sps: 10/3.18\n",
      "10-19/2031 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.43 | batch/sps: 10/1.96\n",
      "20-29/2031 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.43 | batch/sps: 10/1.65\n",
      "30-39/2031 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.42 | batch/sps: 10/2.14\n",
      "40-49/2031 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.43 | batch/sps: 10/1.76\n",
      "50-59/2031 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.42 | batch/sps: 10/1.73\n",
      "60-69/2031 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.43 | batch/sps: 10/4.30\n",
      "70-79/2031 | total/used/cuda/res/ram (Gb): 10.00/9.35/2.85/8.04/12.43 | batch/sps: 10/3.16\n",
      "80-89/2031 | total/used/cuda/res/ram (Gb): 10.00/9.38/2.85/8.04/12.40 | batch/sps: 10/2.66\n",
      "90-99/2031 | total/used/cuda/res/ram (Gb): 10.00/9.39/2.85/8.04/12.40 | batch/sps: 10/3.49\n",
      "100-109/2031 | total/used/cuda/res/ram (Gb): 10.00/9.34/2.85/8.04/12.40 | batch/sps: 10/2.97\n",
      "110-119/2031 | total/used/cuda/res/ram (Gb): 10.00/9.33/2.85/8.04/12.41 | batch/sps: 10/2.89\n",
      "120-129/2031 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.40 | batch/sps: 10/2.24\n",
      "130-139/2031 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.41 | batch/sps: 10/2.45\n",
      "140-149/2031 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.41 | batch/sps: 10/2.74\n",
      "150-159/2031 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.42 | batch/sps: 10/2.78\n",
      "160-169/2031 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.39 | batch/sps: 10/2.19\n",
      "170-179/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/1.74\n",
      "180-189/2031 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.40 | batch/sps: 10/2.26\n",
      "190-199/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/1.86\n",
      "200-209/2031 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.40 | batch/sps: 10/2.83\n",
      "210-219/2031 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.39 | batch/sps: 10/2.87\n",
      "220-229/2031 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.39 | batch/sps: 10/2.85\n",
      "230-239/2031 | total/used/cuda/res/ram (Gb): 10.00/9.32/2.85/8.04/12.39 | batch/sps: 10/4.06\n",
      "240-249/2031 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.40 | batch/sps: 10/1.91\n",
      "250-259/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/3.82\n",
      "260-269/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/3.64\n",
      "270-279/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/2.58\n",
      "280-289/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/2.80\n",
      "290-299/2031 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.40 | batch/sps: 10/2.33\n",
      "300-309/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/3.69\n",
      "310-319/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/2.90\n",
      "320-329/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/3.12\n",
      "330-339/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.96\n",
      "340-349/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/1.66\n",
      "350-359/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/2.76\n",
      "360-369/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/3.48\n",
      "370-379/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.37\n",
      "380-389/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/2.04\n",
      "390-399/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/1.85\n",
      "400-409/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.52\n",
      "410-419/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.58\n",
      "420-429/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/3.29\n",
      "430-439/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.10\n",
      "440-449/2031 | total/used/cuda/res/ram (Gb): 10.00/9.28/2.85/8.04/12.39 | batch/sps: 10/1.50\n",
      "450-459/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.84\n",
      "460-469/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.74\n",
      "470-479/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.00\n",
      "480-489/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.75\n",
      "490-499/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/3.80\n",
      "500-509/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/3.11\n",
      "510-519/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/3.52\n",
      "520-529/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.24\n",
      "530-539/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.69\n",
      "540-549/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/2.13\n",
      "550-559/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/1.97\n",
      "560-569/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.38\n",
      "570-579/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.97\n",
      "580-589/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.78\n",
      "590-599/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.43\n",
      "600-609/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/1.70\n",
      "610-619/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.25\n",
      "620-629/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/2.15\n",
      "630-639/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/3.72\n",
      "640-649/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/3.16\n",
      "650-659/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/1.81\n",
      "660-669/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/3.52\n",
      "670-679/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/1.63\n",
      "680-689/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.34\n",
      "690-699/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/1.95\n",
      "700-709/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.01\n",
      "710-719/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.38\n",
      "720-729/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/3.65\n",
      "730-739/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.24\n",
      "740-749/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.85\n",
      "750-759/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.36 | batch/sps: 10/2.60\n",
      "760-769/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.36 | batch/sps: 10/3.92\n",
      "770-779/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.44\n",
      "780-789/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.42 | batch/sps: 10/2.14\n",
      "790-799/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.03\n",
      "800-809/2031 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.40 | batch/sps: 10/3.02\n",
      "810-819/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.39\n",
      "820-829/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/3.62\n",
      "830-839/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/1.94\n",
      "840-849/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.64\n",
      "850-859/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.81\n",
      "860-869/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.41 | batch/sps: 10/2.18\n",
      "870-879/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.06\n",
      "880-889/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.29\n",
      "890-899/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/1.75\n",
      "900-909/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.42 | batch/sps: 10/2.47\n",
      "910-919/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.42 | batch/sps: 10/4.39\n",
      "920-929/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/3.06\n",
      "930-939/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.42 | batch/sps: 10/2.84\n",
      "940-949/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/3.82\n",
      "950-959/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/1.61\n",
      "960-969/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.33\n",
      "970-979/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.77\n",
      "980-989/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/1.73\n",
      "990-999/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/1.74\n",
      "1000-1009/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.34\n",
      "1010-1019/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/1.90\n",
      "1020-1029/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/2.60\n",
      "1030-1039/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/1.76\n",
      "1040-1049/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/3.42\n",
      "1050-1059/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.95\n",
      "1060-1069/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.43 | batch/sps: 10/3.64\n",
      "1070-1079/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.17\n",
      "1080-1089/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.73\n",
      "1090-1099/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.10\n",
      "1100-1109/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.20\n",
      "1110-1119/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.40 | batch/sps: 10/2.11\n",
      "1120-1129/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.40\n",
      "1130-1139/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/1.89\n",
      "1140-1149/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/2.56\n",
      "1150-1159/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/3.52\n",
      "1160-1169/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.53\n",
      "1170-1179/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/1.94\n",
      "1180-1189/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.38 | batch/sps: 10/1.83\n",
      "1190-1199/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/2.59\n",
      "1200-1209/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.42 | batch/sps: 10/2.82\n",
      "1210-1219/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/2.97\n",
      "1220-1229/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.41 | batch/sps: 10/3.04\n",
      "1230-1239/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.73\n",
      "1240-1249/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/3.23\n",
      "1250-1259/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/2.88\n",
      "1260-1269/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.51\n",
      "1270-1279/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/2.01\n",
      "1280-1289/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.87\n",
      "1290-1299/2031 | total/used/cuda/res/ram (Gb): 10.00/9.28/2.85/8.04/12.41 | batch/sps: 10/2.25\n",
      "1300-1309/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/2.97\n",
      "1310-1319/2031 | total/used/cuda/res/ram (Gb): 10.00/9.27/2.85/8.04/12.39 | batch/sps: 10/1.72\n",
      "1320-1329/2031 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.40 | batch/sps: 10/3.29\n",
      "1330-1339/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/2.14\n",
      "1340-1349/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/3.51\n",
      "1350-1359/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.39 | batch/sps: 10/2.65\n",
      "1360-1369/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.12\n",
      "1370-1379/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/2.55\n",
      "1380-1389/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.95\n",
      "1390-1399/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.53\n",
      "1400-1409/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/3.14\n",
      "1410-1419/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/3.11\n",
      "1420-1429/2031 | total/used/cuda/res/ram (Gb): 10.00/9.31/2.85/8.04/12.38 | batch/sps: 10/2.99\n",
      "1430-1439/2031 | total/used/cuda/res/ram (Gb): 10.00/9.29/2.85/8.04/12.38 | batch/sps: 10/1.96\n",
      "1440-1449/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/1.92\n",
      "1450-1459/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.35\n",
      "1460-1469/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.39 | batch/sps: 10/2.59\n",
      "1470-1479/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/2.11\n",
      "1480-1489/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.40 | batch/sps: 10/1.96\n",
      "1490-1499/2031 | total/used/cuda/res/ram (Gb): 10.00/9.30/2.85/8.04/12.38 | batch/sps: 10/2.72\n",
      "1500-1509/2031 | total/used/cuda/res/ram (Gb): 10.00/9.36/2.85/8.04/12.39 | batch/sps: 10/3.34\n",
      "1510-1519/2031 | total/used/cuda/res/ram (Gb): 10.00/9.41/2.85/8.04/12.40 | batch/sps: 10/1.90\n",
      "1520-1529/2031 | total/used/cuda/res/ram (Gb): 10.00/9.39/2.85/8.04/12.39 | batch/sps: 10/3.12\n",
      "1530-1539/2031 | total/used/cuda/res/ram (Gb): 10.00/9.41/2.85/8.04/12.39 | batch/sps: 10/2.56\n",
      "1540-1549/2031 | total/used/cuda/res/ram (Gb): 10.00/9.39/2.85/8.04/12.38 | batch/sps: 10/1.01\n",
      "1550-1559/2031 | total/used/cuda/res/ram (Gb): 10.00/9.43/2.85/8.04/12.38 | batch/sps: 10/0.43\n",
      "1560-1569/2031 | total/used/cuda/res/ram (Gb): 10.00/9.41/2.85/8.04/12.42 | batch/sps: 10/0.64\n",
      "1570-1579/2031 | total/used/cuda/res/ram (Gb): 10.00/9.39/2.85/8.04/12.37 | batch/sps: 10/0.55\n",
      "1580-1589/2031 | total/used/cuda/res/ram (Gb): 10.00/9.44/2.85/8.04/12.36 | batch/sps: 10/0.42\n",
      "1590-1599/2031 | total/used/cuda/res/ram (Gb): 10.00/9.44/2.85/8.04/12.36 | batch/sps: 10/0.72\n",
      "1600-1609/2031 | total/used/cuda/res/ram (Gb): 10.00/9.46/2.85/8.04/12.36 | batch/sps: 10/0.56\n",
      "1610-1619/2031 | total/used/cuda/res/ram (Gb): 10.00/9.42/2.85/8.04/12.36 | batch/sps: 10/0.42\n",
      "1620-1629/2031 | total/used/cuda/res/ram (Gb): 10.00/9.42/2.85/8.04/12.36 | batch/sps: 10/0.65\n",
      "1630-1639/2031 | total/used/cuda/res/ram (Gb): 10.00/9.43/2.85/8.04/12.36 | batch/sps: 10/0.40\n",
      "1640-1649/2031 | total/used/cuda/res/ram (Gb): 10.00/9.43/2.85/8.04/12.36 | batch/sps: 10/0.43\n",
      "1650-1659/2031 | total/used/cuda/res/ram (Gb): 10.00/9.43/2.85/8.04/12.42 | batch/sps: 10/0.63\n",
      "1660-1669/2031 | total/used/cuda/res/ram (Gb): 10.00/9.44/2.85/8.04/12.40 | batch/sps: 10/0.71\n",
      "1670-1679/2031 | total/used/cuda/res/ram (Gb): 10.00/9.44/2.85/8.04/12.40 | batch/sps: 10/0.62\n",
      "1680-1689/2031 | total/used/cuda/res/ram (Gb): 10.00/9.44/2.85/8.04/12.39 | batch/sps: 10/0.68\n",
      "1690-1699/2031 | total/used/cuda/res/ram (Gb): 10.00/9.46/2.85/8.04/12.39 | batch/sps: 10/0.67\n",
      "1700-1709/2031 | total/used/cuda/res/ram (Gb): 10.00/9.45/2.85/8.04/12.40 | batch/sps: 10/0.61\n",
      "1710-1719/2031 | total/used/cuda/res/ram (Gb): 10.00/9.56/2.85/8.04/12.40 | batch/sps: 10/0.43\n",
      "1720-1729/2031 | total/used/cuda/res/ram (Gb): 10.00/9.53/2.85/8.04/12.40 | batch/sps: 10/0.63\n",
      "1730-1739/2031 | total/used/cuda/res/ram (Gb): 10.00/9.46/2.85/8.04/12.41 | batch/sps: 10/0.68\n",
      "1740-1749/2031 | total/used/cuda/res/ram (Gb): 10.00/9.47/2.85/8.04/12.40 | batch/sps: 10/0.40\n",
      "1750-1759/2031 | total/used/cuda/res/ram (Gb): 10.00/9.45/2.85/8.04/12.39 | batch/sps: 10/0.69\n",
      "1760-1769/2031 | total/used/cuda/res/ram (Gb): 10.00/9.46/2.85/8.04/12.41 | batch/sps: 10/0.94\n",
      "1770-1779/2031 | total/used/cuda/res/ram (Gb): 10.00/9.39/2.85/8.04/12.41 | batch/sps: 10/0.70\n",
      "1780-1789/2031 | total/used/cuda/res/ram (Gb): 10.00/9.39/2.85/8.04/12.42 | batch/sps: 10/0.83\n",
      "1790-1799/2031 | total/used/cuda/res/ram (Gb): 10.00/9.40/2.85/8.04/12.41 | batch/sps: 10/0.62\n",
      "1800-1809/2031 | total/used/cuda/res/ram (Gb): 10.00/9.40/2.85/8.04/12.42 | batch/sps: 10/0.69\n",
      "1810-1819/2031 | total/used/cuda/res/ram (Gb): 10.00/9.48/2.85/8.04/12.41 | batch/sps: 10/0.65\n",
      "1820-1829/2031 | total/used/cuda/res/ram (Gb): 10.00/9.46/2.85/8.04/12.41 | batch/sps: 10/0.89\n",
      "1830-1839/2031 | total/used/cuda/res/ram (Gb): 10.00/9.48/2.85/8.04/12.42 | batch/sps: 10/0.77\n",
      "1840-1849/2031 | total/used/cuda/res/ram (Gb): 10.00/9.43/2.85/8.04/12.41 | batch/sps: 10/0.69\n",
      "1850-1859/2031 | total/used/cuda/res/ram (Gb): 10.00/9.45/2.85/8.04/12.41 | batch/sps: 10/0.69\n",
      "1860-1869/2031 | total/used/cuda/res/ram (Gb): 10.00/9.53/2.85/8.04/12.41 | batch/sps: 10/0.46\n",
      "1870-1879/2031 | total/used/cuda/res/ram (Gb): 10.00/9.51/2.85/8.04/12.41 | batch/sps: 10/0.85\n",
      "1880-1889/2031 | total/used/cuda/res/ram (Gb): 10.00/9.54/2.85/8.04/12.43 | batch/sps: 10/0.37\n",
      "1890-1899/2031 | total/used/cuda/res/ram (Gb): 10.00/9.52/2.85/8.04/12.42 | batch/sps: 10/0.60\n",
      "1900-1909/2031 | total/used/cuda/res/ram (Gb): 10.00/9.47/2.85/8.04/12.43 | batch/sps: 10/0.77\n",
      "1910-1919/2031 | total/used/cuda/res/ram (Gb): 10.00/9.50/2.85/8.04/12.42 | batch/sps: 10/0.59\n",
      "1920-1929/2031 | total/used/cuda/res/ram (Gb): 10.00/9.45/2.85/8.04/12.43 | batch/sps: 10/0.59\n",
      "1930-1939/2031 | total/used/cuda/res/ram (Gb): 10.00/9.45/2.85/8.04/12.42 | batch/sps: 10/0.73\n",
      "1940-1949/2031 | total/used/cuda/res/ram (Gb): 10.00/9.48/2.85/8.04/12.43 | batch/sps: 10/0.61\n",
      "1950-1959/2031 | total/used/cuda/res/ram (Gb): 10.00/9.48/2.85/8.04/12.44 | batch/sps: 10/0.45\n",
      "1960-1969/2031 | total/used/cuda/res/ram (Gb): 10.00/9.47/2.85/8.04/12.42 | batch/sps: 10/0.58\n",
      "1970-1979/2031 | total/used/cuda/res/ram (Gb): 10.00/9.47/2.85/8.04/12.42 | batch/sps: 10/0.70\n",
      "1980-1989/2031 | total/used/cuda/res/ram (Gb): 10.00/9.52/2.85/8.04/12.42 | batch/sps: 10/0.71\n",
      "1990-1999/2031 | total/used/cuda/res/ram (Gb): 10.00/9.71/2.85/8.42/12.42 | batch/sps: 10/0.29\n",
      "2000-2009/2031 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/12.41 | batch/sps: 10/0.87\n",
      "2010-2019/2031 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.41 | batch/sps: 10/0.73\n",
      "2020-2029/2031 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.41 | batch/sps: 10/0.47\n",
      "2030-2030/2031 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.84/8.48/12.41 | batch/sps: 1/0.08\n",
      "Finished processing 2031 samples for gec.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 2031\n",
      "})\n",
      "Despite the strict Japanese society, I feel happy when I have dinner with my family.\n",
      "Despite the strict Japanese society, I feel happy when I have dinner with my family.\n",
      "2031 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.84/8.48/12.41 | sps: 1.37\n",
      "Processing 1127 samples for neutralize\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3c7065870944bbae21db68e3e5fe49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.77 | batch/sps: 10/2.25\n",
      "10-19/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.77 | batch/sps: 10/2.61\n",
      "20-29/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.77 | batch/sps: 10/2.82\n",
      "30-39/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.78 | batch/sps: 10/2.76\n",
      "40-49/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.78 | batch/sps: 10/2.00\n",
      "50-59/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.78 | batch/sps: 10/2.88\n",
      "60-69/1127 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/12.81 | batch/sps: 10/2.57\n",
      "70-79/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.77 | batch/sps: 10/3.20\n",
      "80-89/1127 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/12.78 | batch/sps: 10/2.55\n",
      "90-99/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.77 | batch/sps: 10/3.15\n",
      "100-109/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.77 | batch/sps: 10/2.96\n",
      "110-119/1127 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/12.79 | batch/sps: 10/2.45\n",
      "120-129/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.77 | batch/sps: 10/3.28\n",
      "130-139/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.78 | batch/sps: 10/2.48\n",
      "140-149/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.78 | batch/sps: 10/3.45\n",
      "150-159/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.78 | batch/sps: 10/2.34\n",
      "160-169/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.78 | batch/sps: 10/3.78\n",
      "170-179/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.78 | batch/sps: 10/2.81\n",
      "180-189/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.78 | batch/sps: 10/2.69\n",
      "190-199/1127 | total/used/cuda/res/ram (Gb): 10.00/9.74/2.85/8.48/12.78 | batch/sps: 10/2.76\n",
      "200-209/1127 | total/used/cuda/res/ram (Gb): 10.00/9.73/2.85/8.48/12.78 | batch/sps: 10/2.44\n",
      "210-219/1127 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/12.78 | batch/sps: 10/2.54\n",
      "220-229/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.78 | batch/sps: 10/2.83\n",
      "230-239/1127 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/12.76 | batch/sps: 10/3.21\n",
      "240-249/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.81 | batch/sps: 10/2.58\n",
      "250-259/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.81 | batch/sps: 10/1.50\n",
      "260-269/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.81 | batch/sps: 10/2.03\n",
      "270-279/1127 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/12.80 | batch/sps: 10/3.88\n",
      "280-289/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.75 | batch/sps: 10/2.90\n",
      "290-299/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.75 | batch/sps: 10/2.53\n",
      "300-309/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.76 | batch/sps: 10/2.45\n",
      "310-319/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.77 | batch/sps: 10/3.89\n",
      "320-329/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.77 | batch/sps: 10/3.67\n",
      "330-339/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.75 | batch/sps: 10/2.71\n",
      "340-349/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.75 | batch/sps: 10/2.98\n",
      "350-359/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.75 | batch/sps: 10/3.26\n",
      "360-369/1127 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/12.76 | batch/sps: 10/2.59\n",
      "370-379/1127 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/12.75 | batch/sps: 10/3.00\n",
      "380-389/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.75 | batch/sps: 10/3.11\n",
      "390-399/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.75 | batch/sps: 10/2.00\n",
      "400-409/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.76 | batch/sps: 10/3.27\n",
      "410-419/1127 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/12.74 | batch/sps: 10/2.57\n",
      "420-429/1127 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/12.75 | batch/sps: 10/1.27\n",
      "430-439/1127 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/12.74 | batch/sps: 10/1.05\n",
      "440-449/1127 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/12.74 | batch/sps: 10/1.50\n",
      "450-459/1127 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/12.75 | batch/sps: 10/1.01\n",
      "460-469/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.74 | batch/sps: 10/0.81\n",
      "470-479/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.75 | batch/sps: 10/1.17\n",
      "480-489/1127 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/12.74 | batch/sps: 10/0.92\n",
      "490-499/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.73 | batch/sps: 10/1.53\n",
      "500-509/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.72 | batch/sps: 10/1.38\n",
      "510-519/1127 | total/used/cuda/res/ram (Gb): 10.00/9.74/2.85/8.48/12.73 | batch/sps: 10/0.97\n",
      "520-529/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.74 | batch/sps: 10/0.94\n",
      "530-539/1127 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/12.74 | batch/sps: 10/1.09\n",
      "540-549/1127 | total/used/cuda/res/ram (Gb): 10.00/9.75/2.85/8.48/12.78 | batch/sps: 10/1.33\n",
      "550-559/1127 | total/used/cuda/res/ram (Gb): 10.00/9.73/2.85/8.48/12.79 | batch/sps: 10/1.12\n",
      "560-569/1127 | total/used/cuda/res/ram (Gb): 10.00/9.73/2.85/8.48/12.78 | batch/sps: 10/0.87\n",
      "570-579/1127 | total/used/cuda/res/ram (Gb): 10.00/9.73/2.85/8.48/12.78 | batch/sps: 10/1.19\n",
      "580-589/1127 | total/used/cuda/res/ram (Gb): 10.00/9.73/2.85/8.48/12.78 | batch/sps: 10/0.92\n",
      "590-599/1127 | total/used/cuda/res/ram (Gb): 10.00/9.73/2.85/8.48/12.79 | batch/sps: 10/1.25\n",
      "600-609/1127 | total/used/cuda/res/ram (Gb): 10.00/9.73/2.85/8.48/12.78 | batch/sps: 10/0.83\n",
      "610-619/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.77 | batch/sps: 10/1.51\n",
      "620-629/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.78 | batch/sps: 10/1.40\n",
      "630-639/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.77 | batch/sps: 10/0.59\n",
      "640-649/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.78 | batch/sps: 10/0.52\n",
      "650-659/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.78 | batch/sps: 10/1.00\n",
      "660-669/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.79 | batch/sps: 10/1.04\n",
      "670-679/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.79 | batch/sps: 10/0.79\n",
      "680-689/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.78 | batch/sps: 10/1.12\n",
      "690-699/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.79 | batch/sps: 10/1.03\n",
      "700-709/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.79 | batch/sps: 10/1.08\n",
      "710-719/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.80 | batch/sps: 10/0.94\n",
      "720-729/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.81 | batch/sps: 10/0.86\n",
      "730-739/1127 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/12.80 | batch/sps: 10/0.64\n",
      "740-749/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.81 | batch/sps: 10/1.11\n",
      "750-759/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.80 | batch/sps: 10/1.10\n",
      "760-769/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.81 | batch/sps: 10/0.72\n",
      "770-779/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.80 | batch/sps: 10/1.09\n",
      "780-789/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.78 | batch/sps: 10/0.92\n",
      "790-799/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.79 | batch/sps: 10/1.48\n",
      "800-809/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.80 | batch/sps: 10/1.13\n",
      "810-819/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.79 | batch/sps: 10/0.51\n",
      "820-829/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.79 | batch/sps: 10/0.85\n",
      "830-839/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.79 | batch/sps: 10/0.90\n",
      "840-849/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.79 | batch/sps: 10/1.16\n",
      "850-859/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.79 | batch/sps: 10/1.08\n",
      "860-869/1127 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/12.81 | batch/sps: 10/0.59\n",
      "870-879/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.80 | batch/sps: 10/0.93\n",
      "880-889/1127 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/12.78 | batch/sps: 10/0.69\n",
      "890-899/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.79 | batch/sps: 10/1.03\n",
      "900-909/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.79 | batch/sps: 10/0.78\n",
      "910-919/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.80 | batch/sps: 10/0.52\n",
      "920-929/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.79 | batch/sps: 10/0.97\n",
      "930-939/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.78 | batch/sps: 10/0.83\n",
      "940-949/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.78 | batch/sps: 10/0.96\n",
      "950-959/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.79 | batch/sps: 10/0.93\n",
      "960-969/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.78 | batch/sps: 10/0.94\n",
      "970-979/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.79 | batch/sps: 10/0.72\n",
      "980-989/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.79 | batch/sps: 10/1.48\n",
      "990-999/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.78 | batch/sps: 10/1.09\n",
      "1000-1009/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.78 | batch/sps: 10/1.08\n",
      "1010-1019/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.78 | batch/sps: 10/1.57\n",
      "1020-1029/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.80 | batch/sps: 10/0.92\n",
      "1030-1039/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.79 | batch/sps: 10/1.03\n",
      "1040-1049/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.78 | batch/sps: 10/0.71\n",
      "1050-1059/1127 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/12.79 | batch/sps: 10/1.58\n",
      "1060-1069/1127 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/12.80 | batch/sps: 10/1.30\n",
      "1070-1079/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.79 | batch/sps: 10/0.55\n",
      "1080-1089/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.79 | batch/sps: 10/1.11\n",
      "1090-1099/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.79 | batch/sps: 10/0.86\n",
      "1100-1109/1127 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/12.79 | batch/sps: 10/1.35\n",
      "1110-1119/1127 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/12.78 | batch/sps: 10/1.22\n",
      "1120-1126/1127 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/12.80 | batch/sps: 7/0.67\n",
      "Finished processing 1127 samples for neutralize.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 1127\n",
      "})\n",
      "new moon received negative reviews from critics.\n",
      "new moon received mixed reviews from critics.\n",
      "1127 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/12.80 | sps: 1.23\n",
      "Processing 1590 samples for paraphrase\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5536dbf0328741cbb1dc75bdd099cca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/1590 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/13.16 | batch/sps: 10/1.33\n",
      "10-19/1590 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/13.16 | batch/sps: 10/1.50\n",
      "20-29/1590 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/13.16 | batch/sps: 10/1.67\n",
      "30-39/1590 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/13.16 | batch/sps: 10/2.13\n",
      "40-49/1590 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/13.16 | batch/sps: 10/1.84\n",
      "50-59/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.16 | batch/sps: 10/1.74\n",
      "60-69/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.16 | batch/sps: 10/1.98\n",
      "70-79/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.16 | batch/sps: 10/2.01\n",
      "80-89/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.16 | batch/sps: 10/1.81\n",
      "90-99/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.16 | batch/sps: 10/2.24\n",
      "100-109/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.16 | batch/sps: 10/1.93\n",
      "110-119/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.16 | batch/sps: 10/1.99\n",
      "120-129/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.17 | batch/sps: 10/1.72\n",
      "130-139/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.15 | batch/sps: 10/1.87\n",
      "140-149/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/1.64\n",
      "150-159/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.19 | batch/sps: 10/1.97\n",
      "160-169/1590 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/13.18 | batch/sps: 10/1.51\n",
      "170-179/1590 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/13.19 | batch/sps: 10/1.98\n",
      "180-189/1590 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/13.18 | batch/sps: 10/1.60\n",
      "190-199/1590 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/13.18 | batch/sps: 10/1.83\n",
      "200-209/1590 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/13.18 | batch/sps: 10/1.87\n",
      "210-219/1590 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/13.18 | batch/sps: 10/1.79\n",
      "220-229/1590 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/13.19 | batch/sps: 10/1.84\n",
      "230-239/1590 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/13.17 | batch/sps: 10/1.82\n",
      "240-249/1590 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/13.17 | batch/sps: 10/1.74\n",
      "250-259/1590 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/13.18 | batch/sps: 10/1.77\n",
      "260-269/1590 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/13.17 | batch/sps: 10/1.82\n",
      "270-279/1590 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/13.18 | batch/sps: 10/1.78\n",
      "280-289/1590 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/13.17 | batch/sps: 10/1.58\n",
      "290-299/1590 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/13.18 | batch/sps: 10/1.98\n",
      "300-309/1590 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/13.17 | batch/sps: 10/1.69\n",
      "310-319/1590 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/13.18 | batch/sps: 10/1.88\n",
      "320-329/1590 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/13.17 | batch/sps: 10/1.42\n",
      "330-339/1590 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/13.17 | batch/sps: 10/1.88\n",
      "340-349/1590 | total/used/cuda/res/ram (Gb): 10.00/9.95/2.85/8.48/13.17 | batch/sps: 10/1.92\n",
      "350-359/1590 | total/used/cuda/res/ram (Gb): 10.00/9.94/2.85/8.48/13.18 | batch/sps: 10/2.19\n",
      "360-369/1590 | total/used/cuda/res/ram (Gb): 10.00/9.91/2.85/8.48/13.18 | batch/sps: 10/1.75\n",
      "370-379/1590 | total/used/cuda/res/ram (Gb): 10.00/9.92/2.85/8.48/13.18 | batch/sps: 10/2.14\n",
      "380-389/1590 | total/used/cuda/res/ram (Gb): 10.00/9.91/2.85/8.48/13.18 | batch/sps: 10/1.89\n",
      "390-399/1590 | total/used/cuda/res/ram (Gb): 10.00/9.92/2.85/8.48/13.17 | batch/sps: 10/1.84\n",
      "400-409/1590 | total/used/cuda/res/ram (Gb): 10.00/9.92/2.85/8.48/13.18 | batch/sps: 10/1.70\n",
      "410-419/1590 | total/used/cuda/res/ram (Gb): 10.00/9.87/2.85/8.48/13.18 | batch/sps: 10/1.68\n",
      "420-429/1590 | total/used/cuda/res/ram (Gb): 10.00/9.92/2.85/8.48/13.18 | batch/sps: 10/1.72\n",
      "430-439/1590 | total/used/cuda/res/ram (Gb): 10.00/9.87/2.85/8.48/13.17 | batch/sps: 10/1.80\n",
      "440-449/1590 | total/used/cuda/res/ram (Gb): 10.00/9.89/2.85/8.48/13.17 | batch/sps: 10/1.46\n",
      "450-459/1590 | total/used/cuda/res/ram (Gb): 10.00/9.87/2.85/8.48/13.15 | batch/sps: 10/1.72\n",
      "460-469/1590 | total/used/cuda/res/ram (Gb): 10.00/9.88/2.85/8.48/13.18 | batch/sps: 10/1.70\n",
      "470-479/1590 | total/used/cuda/res/ram (Gb): 10.00/9.87/2.85/8.48/13.18 | batch/sps: 10/1.89\n",
      "480-489/1590 | total/used/cuda/res/ram (Gb): 10.00/9.86/2.85/8.48/13.17 | batch/sps: 10/1.77\n",
      "490-499/1590 | total/used/cuda/res/ram (Gb): 10.00/9.87/2.85/8.48/13.14 | batch/sps: 10/1.73\n",
      "500-509/1590 | total/used/cuda/res/ram (Gb): 10.00/9.89/2.85/8.48/13.14 | batch/sps: 10/1.72\n",
      "510-519/1590 | total/used/cuda/res/ram (Gb): 10.00/9.87/2.85/8.48/13.13 | batch/sps: 10/1.37\n",
      "520-529/1590 | total/used/cuda/res/ram (Gb): 10.00/9.90/2.85/8.48/13.13 | batch/sps: 10/1.61\n",
      "530-539/1590 | total/used/cuda/res/ram (Gb): 10.00/9.88/2.85/8.48/13.13 | batch/sps: 10/1.78\n",
      "540-549/1590 | total/used/cuda/res/ram (Gb): 10.00/9.89/2.85/8.48/13.14 | batch/sps: 10/1.58\n",
      "550-559/1590 | total/used/cuda/res/ram (Gb): 10.00/9.88/2.85/8.48/13.14 | batch/sps: 10/1.99\n",
      "560-569/1590 | total/used/cuda/res/ram (Gb): 10.00/9.88/2.85/8.48/13.13 | batch/sps: 10/1.70\n",
      "570-579/1590 | total/used/cuda/res/ram (Gb): 10.00/9.88/2.85/8.48/13.14 | batch/sps: 10/1.57\n",
      "580-589/1590 | total/used/cuda/res/ram (Gb): 10.00/9.90/2.85/8.48/13.14 | batch/sps: 10/1.62\n",
      "590-599/1590 | total/used/cuda/res/ram (Gb): 10.00/9.87/2.85/8.48/13.14 | batch/sps: 10/1.64\n",
      "600-609/1590 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/13.14 | batch/sps: 10/1.57\n",
      "610-619/1590 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.85/8.48/13.14 | batch/sps: 10/1.39\n",
      "620-629/1590 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.85/8.48/13.14 | batch/sps: 10/1.45\n",
      "630-639/1590 | total/used/cuda/res/ram (Gb): 10.00/9.86/2.85/8.48/13.14 | batch/sps: 10/1.59\n",
      "640-649/1590 | total/used/cuda/res/ram (Gb): 10.00/9.86/2.85/8.48/13.15 | batch/sps: 10/1.52\n",
      "650-659/1590 | total/used/cuda/res/ram (Gb): 10.00/9.82/2.85/8.48/13.16 | batch/sps: 10/1.48\n",
      "660-669/1590 | total/used/cuda/res/ram (Gb): 10.00/9.75/2.85/8.48/13.15 | batch/sps: 10/1.61\n",
      "670-679/1590 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/13.14 | batch/sps: 10/1.63\n",
      "680-689/1590 | total/used/cuda/res/ram (Gb): 10.00/9.83/2.85/8.48/13.14 | batch/sps: 10/1.58\n",
      "690-699/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.15 | batch/sps: 10/1.65\n",
      "700-709/1590 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/13.15 | batch/sps: 10/1.51\n",
      "710-719/1590 | total/used/cuda/res/ram (Gb): 10.00/9.75/2.85/8.48/13.15 | batch/sps: 10/1.41\n",
      "720-729/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.17 | batch/sps: 10/1.50\n",
      "730-739/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.16 | batch/sps: 10/1.43\n",
      "740-749/1590 | total/used/cuda/res/ram (Gb): 10.00/9.80/2.85/8.48/13.18 | batch/sps: 10/1.47\n",
      "750-759/1590 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/13.19 | batch/sps: 10/1.54\n",
      "760-769/1590 | total/used/cuda/res/ram (Gb): 10.00/9.81/2.85/8.48/13.19 | batch/sps: 10/1.41\n",
      "770-779/1590 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.19 | batch/sps: 10/1.62\n",
      "780-789/1590 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.18 | batch/sps: 10/1.55\n",
      "790-799/1590 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.20 | batch/sps: 10/1.60\n",
      "800-809/1590 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.20 | batch/sps: 10/1.59\n",
      "810-819/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.52\n",
      "820-829/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.26\n",
      "830-839/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.47\n",
      "840-849/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.19 | batch/sps: 10/1.31\n",
      "850-859/1590 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.18 | batch/sps: 10/1.59\n",
      "860-869/1590 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.18 | batch/sps: 10/1.76\n",
      "870-879/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.54\n",
      "880-889/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.17 | batch/sps: 10/1.25\n",
      "890-899/1590 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.18 | batch/sps: 10/1.71\n",
      "900-909/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.17 | batch/sps: 10/1.48\n",
      "910-919/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.52\n",
      "920-929/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.46\n",
      "930-939/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.39\n",
      "940-949/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.41\n",
      "950-959/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.26\n",
      "960-969/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.50\n",
      "970-979/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/1.45\n",
      "980-989/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/0.21\n",
      "990-999/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/1.27\n",
      "1000-1009/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/1.43\n",
      "1010-1019/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/1.32\n",
      "1020-1029/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/1.14\n",
      "1030-1039/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/1.26\n",
      "1040-1049/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/1.04\n",
      "1050-1059/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/1.22\n",
      "1060-1069/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/1.22\n",
      "1070-1079/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/3.34\n",
      "1080-1089/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.19 | batch/sps: 10/2.95\n",
      "1090-1099/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/1.67\n",
      "1100-1109/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.19 | batch/sps: 10/2.42\n",
      "1110-1119/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/3.00\n",
      "1120-1129/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/2.74\n",
      "1130-1139/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.21 | batch/sps: 10/2.23\n",
      "1140-1149/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.19 | batch/sps: 10/2.71\n",
      "1150-1159/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/2.91\n",
      "1160-1169/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/3.85\n",
      "1170-1179/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/2.21\n",
      "1180-1189/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/1.93\n",
      "1190-1199/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.19 | batch/sps: 10/3.18\n",
      "1200-1209/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.18 | batch/sps: 10/2.60\n",
      "1210-1219/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.18 | batch/sps: 10/2.66\n",
      "1220-1229/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/2.72\n",
      "1230-1239/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/2.97\n",
      "1240-1249/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/2.35\n",
      "1250-1259/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.18 | batch/sps: 10/2.95\n",
      "1260-1269/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.19 | batch/sps: 10/3.55\n",
      "1270-1279/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.19 | batch/sps: 10/3.65\n",
      "1280-1289/1590 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.17 | batch/sps: 10/1.67\n",
      "1290-1299/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.17 | batch/sps: 10/4.15\n",
      "1300-1309/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.21 | batch/sps: 10/3.20\n",
      "1310-1319/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.21 | batch/sps: 10/3.02\n",
      "1320-1329/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.21 | batch/sps: 10/2.49\n",
      "1330-1339/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/2.91\n",
      "1340-1349/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.21 | batch/sps: 10/2.06\n",
      "1350-1359/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/2.37\n",
      "1360-1369/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/2.90\n",
      "1370-1379/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.21 | batch/sps: 10/3.71\n",
      "1380-1389/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/2.81\n",
      "1390-1399/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/2.53\n",
      "1400-1409/1590 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.20 | batch/sps: 10/1.78\n",
      "1410-1419/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/2.34\n",
      "1420-1429/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/3.30\n",
      "1430-1439/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.21 | batch/sps: 10/2.68\n",
      "1440-1449/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.20 | batch/sps: 10/3.04\n",
      "1450-1459/1590 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.21 | batch/sps: 10/1.94\n",
      "1460-1469/1590 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.20 | batch/sps: 10/1.50\n",
      "1470-1479/1590 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.20 | batch/sps: 10/2.69\n",
      "1480-1489/1590 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.20 | batch/sps: 10/3.66\n",
      "1490-1499/1590 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.20 | batch/sps: 10/3.39\n",
      "1500-1509/1590 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.20 | batch/sps: 10/3.23\n",
      "1510-1519/1590 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.20 | batch/sps: 10/2.28\n",
      "1520-1529/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.21 | batch/sps: 10/2.08\n",
      "1530-1539/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.21 | batch/sps: 10/2.13\n",
      "1540-1549/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.22 | batch/sps: 10/3.84\n",
      "1550-1559/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.20 | batch/sps: 10/2.61\n",
      "1560-1569/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.21 | batch/sps: 10/3.53\n",
      "1570-1579/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.21 | batch/sps: 10/1.91\n",
      "1580-1589/1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.21 | batch/sps: 10/3.25\n",
      "Finished processing 1590 samples for paraphrase.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 1590\n",
      "})\n",
      "Yet, as Jack walked slowly over the wooden gangway to the depressed, deserted beach, the thought of Speedy Parker was dancing on the boundary of his awareness.\n",
      "But as Jack walked along the boardwalk and onto the beach, the idea of Speedy Parker still danced in his head.\n",
      "1590 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.00 | sps: 1.74\n",
      "Processing 1062 samples for coherence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17f3bade11b415088c0dceaf59e86b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.38 | batch/sps: 10/1.12\n",
      "10-19/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.12\n",
      "20-29/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.36\n",
      "30-39/1062 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.37 | batch/sps: 10/1.36\n",
      "40-49/1062 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.37 | batch/sps: 10/1.44\n",
      "50-59/1062 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.37 | batch/sps: 10/1.38\n",
      "60-69/1062 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.36 | batch/sps: 10/1.28\n",
      "70-79/1062 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.36 | batch/sps: 10/1.22\n",
      "80-89/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.38 | batch/sps: 10/1.22\n",
      "90-99/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.17\n",
      "100-109/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.08\n",
      "110-119/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.36\n",
      "120-129/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.13\n",
      "130-139/1062 | total/used/cuda/res/ram (Gb): 10.00/9.79/2.85/8.48/13.36 | batch/sps: 10/1.30\n",
      "140-149/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.22\n",
      "150-159/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.22\n",
      "160-169/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.38 | batch/sps: 10/1.36\n",
      "170-179/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.39 | batch/sps: 10/1.20\n",
      "180-189/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.38\n",
      "190-199/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.38\n",
      "200-209/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.40\n",
      "210-219/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.25\n",
      "220-229/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.11\n",
      "230-239/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.19\n",
      "240-249/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.28\n",
      "250-259/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.00\n",
      "260-269/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.25\n",
      "270-279/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.22\n",
      "280-289/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.26\n",
      "290-299/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.33 | batch/sps: 10/1.11\n",
      "300-309/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.38 | batch/sps: 10/1.15\n",
      "310-319/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.19\n",
      "320-329/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.35\n",
      "330-339/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.26\n",
      "340-349/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.35 | batch/sps: 10/1.20\n",
      "350-359/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.35 | batch/sps: 10/1.28\n",
      "360-369/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.24\n",
      "370-379/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.41\n",
      "380-389/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.35 | batch/sps: 10/1.36\n",
      "390-399/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.35 | batch/sps: 10/1.47\n",
      "400-409/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.35 | batch/sps: 10/1.19\n",
      "410-419/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.35 | batch/sps: 10/1.29\n",
      "420-429/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.35 | batch/sps: 10/1.18\n",
      "430-439/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.11\n",
      "440-449/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.50\n",
      "450-459/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.38 | batch/sps: 10/1.49\n",
      "460-469/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.16\n",
      "470-479/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.37\n",
      "480-489/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.13\n",
      "490-499/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.43\n",
      "500-509/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.36 | batch/sps: 10/1.12\n",
      "510-519/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.36 | batch/sps: 10/1.10\n",
      "520-529/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.39 | batch/sps: 10/1.34\n",
      "530-539/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.37 | batch/sps: 10/1.25\n",
      "540-549/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.37 | batch/sps: 10/1.21\n",
      "550-559/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.37 | batch/sps: 10/1.24\n",
      "560-569/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.36 | batch/sps: 10/1.24\n",
      "570-579/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.36 | batch/sps: 10/1.20\n",
      "580-589/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.36 | batch/sps: 10/1.33\n",
      "590-599/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.02\n",
      "600-609/1062 | total/used/cuda/res/ram (Gb): 10.00/9.75/2.85/8.48/13.36 | batch/sps: 10/1.18\n",
      "610-619/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.31\n",
      "620-629/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.11\n",
      "630-639/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.38 | batch/sps: 10/1.38\n",
      "640-649/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.34\n",
      "650-659/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.40\n",
      "660-669/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.13\n",
      "670-679/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.42\n",
      "680-689/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.21\n",
      "690-699/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.18\n",
      "700-709/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.13\n",
      "710-719/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.19\n",
      "720-729/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.31\n",
      "730-739/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.22\n",
      "740-749/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.38 | batch/sps: 10/1.20\n",
      "750-759/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.38\n",
      "760-769/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.21\n",
      "770-779/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.37 | batch/sps: 10/1.42\n",
      "780-789/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.38 | batch/sps: 10/1.27\n",
      "790-799/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.23\n",
      "800-809/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.22\n",
      "810-819/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.35 | batch/sps: 10/1.21\n",
      "820-829/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.06\n",
      "830-839/1062 | total/used/cuda/res/ram (Gb): 10.00/9.78/2.85/8.48/13.36 | batch/sps: 10/1.43\n",
      "840-849/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.37\n",
      "850-859/1062 | total/used/cuda/res/ram (Gb): 10.00/9.76/2.85/8.48/13.36 | batch/sps: 10/1.48\n",
      "860-869/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.25\n",
      "870-879/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.28\n",
      "880-889/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.35 | batch/sps: 10/1.31\n",
      "890-899/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.19\n",
      "900-909/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.14\n",
      "910-919/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.31\n",
      "920-929/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.28\n",
      "930-939/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.06\n",
      "940-949/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/0.99\n",
      "950-959/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.02\n",
      "960-969/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.34 | batch/sps: 10/1.34\n",
      "970-979/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.38\n",
      "980-989/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.39\n",
      "990-999/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.48\n",
      "1000-1009/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.18\n",
      "1010-1019/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.17\n",
      "1020-1029/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.38 | batch/sps: 10/1.13\n",
      "1030-1039/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.23\n",
      "1040-1049/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 10/1.25\n",
      "1050-1059/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.36 | batch/sps: 10/1.55\n",
      "1060-1061/1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | batch/sps: 2/0.31\n",
      "Finished processing 1062 samples for coherence.\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 1062\n",
      "})\n",
      "Outside the town, 6 tourists were reported killed. However, official documents indicate that at least 255 local residents were killed, with a further 29 never found.\n",
      "Outside the town, 6 tourists were reported killed, but official documents indicate that at least 255 local residents were killed, with a further 29 never found.\n",
      "1062 | total/used/cuda/res/ram (Gb): 10.00/9.77/2.85/8.48/13.37 | sps: 1.23\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'processed'],\n",
      "    num_rows: 2031\n",
      "})\n",
      "['Despite the strict Japanese society, I feel happy when I have dinner with my '\n",
      " 'family.',\n",
      " 'They have been increasing rapidly in Japan for a couple of years.']\n",
      "CPU times: user 1h 13min 10s, sys: 8min, total: 1h 21min 10s\n",
      "Wall time: 1h 21min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "# batch_size = 8  # gpt2-large\n",
    "batch_size = 10  # gemma-2b\n",
    "max_length = 350\n",
    "max_new_tokens = 350\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"input\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    # print(batch[\"request\"])\n",
    "    inputs = tokenizer(batch[\"request\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    # inputs = tokenizer(batch[\"request\"], return_tensors=\"pt\").inputs.to(device)\n",
    "    # inputs = tokenizer(item[\"request\"], return_tensors=\"pt\").inputs\n",
    "    # print(inputs)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    # outputs = model.generate(**inputs, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=10,\n",
    "    #     num_return_sequences=1,\n",
    "    #     pad_token_id=tokenizer.eos_token_id,\n",
    "    #     max_length=max_length,\n",
    "    # )\n",
    "    # print(outputs)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    processed = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    # print(processed)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for task, samples in test_dataset_dict.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {task}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {task}.\")\n",
    "    print(processed_samples)\n",
    "    print(processed_samples[\"reference\"][0])\n",
    "    print(processed_samples[\"processed\"][0])\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[task] = {\n",
    "        # \"task\": task,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"elapsed_time\": elapsed_time,\n",
    "        \"sps\": sps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_length\": max_length,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_gec_samples = processed_samples_map[\"gec\"][\"samples\"]\n",
    "\n",
    "pprint(processed_gec_samples)\n",
    "pprint(processed_gec_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving eval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gemma-2b-coedit, model_id: iliazlobin/gemma-2b-coedit, model_path: iliazlobin_gemma-2b-coedit\n",
      "Total/trainable params: 2506172416/524363776\n",
      "task: clarity, samples: 126\n",
      "s: rouge, v: {'rouge1': 0.8525000991321239, 'rouge2': 0.7729314536009477, 'rougeL': 0.842399976151688, 'rougeLsum': 0.8418838757260907}\n",
      "s: sacreblue, v: {'score': 75.35217765548724, 'counts': [3543, 3158, 2896, 2673], 'totals': [4240, 4114, 3988, 3862], 'precisions': [83.56132075471699, 76.76227515799708, 72.61785356068205, 69.21284308648369], 'bp': 1.0, 'sys_len': 4240, 'ref_len': 3943}\n",
      "s: sari, v: {'sari': 57.73233309882409}\n",
      "s: em, v: {'exact_match': 0.007936507936507936}\n",
      "task: simplification, samples: 1144\n",
      "s: rouge, v: {'rouge1': 0.5956759052785148, 'rouge2': 0.4029569724115429, 'rougeL': 0.5470605614967735, 'rougeLsum': 0.546840226978884}\n",
      "s: sacreblue, v: {'score': 26.575632887494233, 'counts': [10945, 6279, 4092, 2773], 'totals': [17899, 16755, 15611, 14467], 'precisions': [61.14866752332532, 37.47538048343778, 26.212286208442766, 19.16776111149513], 'bp': 0.8112996363506836, 'sys_len': 17899, 'ref_len': 21642}\n",
      "s: sari, v: {'sari': 55.208517372478724}\n",
      "s: em, v: {'exact_match': 0.07080419580419581}\n",
      "task: gec, samples: 2031\n",
      "s: rouge, v: {'rouge1': 0.9026507064794635, 'rouge2': 0.7974630976020651, 'rougeL': 0.8949305647089896, 'rougeLsum': 0.8950273375465294}\n",
      "s: sacreblue, v: {'score': 68.53197804029635, 'counts': [42824, 34761, 28675, 23705], 'totals': [48827, 46796, 44769, 42749], 'precisions': [87.70557273639585, 74.28198991366783, 64.0510174451071, 55.45158951086575], 'bp': 0.9881106480295823, 'sys_len': 48827, 'ref_len': 49411}\n",
      "s: sari, v: {'sari': 74.52039483224418}\n",
      "s: em, v: {'exact_match': 0.2191038897095027}\n",
      "task: neutralize, samples: 1127\n",
      "s: rouge, v: {'rouge1': 0.9471066520612496, 'rouge2': 0.9014601925484079, 'rougeL': 0.9469515066604917, 'rougeLsum': 0.946846635064823}\n",
      "s: sacreblue, v: {'score': 91.90428520064748, 'counts': [25017, 23145, 21449, 19884], 'totals': [25812, 24685, 23558, 22433], 'precisions': [96.92003719200372, 93.7613935588414, 91.04762713303336, 88.63727544242856], 'bp': 0.9931277056087713, 'sys_len': 25812, 'ref_len': 25990}\n",
      "s: sari, v: {'sari': 79.5776847738217}\n",
      "s: em, v: {'exact_match': 0.45962732919254656}\n",
      "task: paraphrase, samples: 1590\n",
      "s: rouge, v: {'rouge1': 0.5332524678513089, 'rouge2': 0.2802348245163171, 'rougeL': 0.44264254265984265, 'rougeLsum': 0.4429832819340773}\n",
      "s: sacreblue, v: {'score': 17.744190155563402, 'counts': [21154, 9228, 4697, 2441], 'totals': [40792, 39202, 37612, 36022], 'precisions': [51.858207491665034, 23.539615325748688, 12.488035733276615, 6.776414413414025], 'bp': 0.9897566872670258, 'sys_len': 40792, 'ref_len': 41212}\n",
      "s: sari, v: {'sari': 51.00808637717292}\n",
      "s: em, v: {'exact_match': 0.02012578616352201}\n",
      "task: coherence, samples: 1062\n",
      "s: rouge, v: {'rouge1': 0.9726846461745827, 'rouge2': 0.9453944118259234, 'rougeL': 0.9682700307950426, 'rougeLsum': 0.968219342684267}\n",
      "s: sacreblue, v: {'score': 92.40446173238468, 'counts': [31618, 29654, 27861, 26114], 'totals': [32717, 31655, 30593, 29531], 'precisions': [96.64089005715682, 93.67872374032538, 91.0698525806557, 88.42910839456842], 'bp': 1.0, 'sys_len': 32717, 'ref_len': 32710}\n",
      "s: sari, v: {'sari': 81.83189000086652}\n",
      "s: em, v: {'exact_match': 0.4171374764595104}\n",
      "{'model': 'iliazlobin/gemma-2b-coedit', 'hardware': 'HomeDesktop (RTX3080)', 'total_params': 2506172416, 'clarity': {'task': 'clarity', 'total_samples': 126, 'elapsed_time': 139.39610958099365, 'sps': 0.9038989709163291, 'batch_size': 10, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.8525000991321239, 'rouge2': 0.7729314536009477, 'rougeL': 0.842399976151688, 'rougeLsum': 0.8418838757260907}, 'sacreblue': {'score': 75.35217765548724, 'counts': [3543, 3158, 2896, 2673], 'totals': [4240, 4114, 3988, 3862], 'precisions': [83.56132075471699, 76.76227515799708, 72.61785356068205, 69.21284308648369], 'bp': 1.0, 'sys_len': 4240, 'ref_len': 3943}, 'sari': {'sari': 57.73233309882409}, 'em': {'exact_match': 0.007936507936507936}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 7890673664, 'cuda_allocated': 3057090560, 'cuda_reserved': 6532628480, 'ram_usage': 12284923904}}, 'simplification': {'task': 'simplification', 'total_samples': 1144, 'elapsed_time': 554.4263393878937, 'sps': 2.063394032222597, 'batch_size': 10, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.5956759052785148, 'rouge2': 0.4029569724115429, 'rougeL': 0.5470605614967735, 'rougeLsum': 0.546840226978884}, 'sacreblue': {'score': 26.575632887494233, 'counts': [10945, 6279, 4092, 2773], 'totals': [17899, 16755, 15611, 14467], 'precisions': [61.14866752332532, 37.47538048343778, 26.212286208442766, 19.16776111149513], 'bp': 0.8112996363506836, 'sys_len': 17899, 'ref_len': 21642}, 'sari': {'sari': 55.208517372478724}, 'em': {'exact_match': 0.07080419580419581}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 10042597376, 'cuda_allocated': 3056888320, 'cuda_reserved': 8631877632, 'ram_usage': 12898955264}}, 'gec': {'task': 'gec', 'total_samples': 2031, 'elapsed_time': 1482.3575615882874, 'sps': 1.370114777047357, 'batch_size': 10, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.9026507064794635, 'rouge2': 0.7974630976020651, 'rougeL': 0.8949305647089896, 'rougeLsum': 0.8950273375465294}, 'sacreblue': {'score': 68.53197804029635, 'counts': [42824, 34761, 28675, 23705], 'totals': [48827, 46796, 44769, 42749], 'precisions': [87.70557273639585, 74.28198991366783, 64.0510174451071, 55.45158951086575], 'bp': 0.9881106480295823, 'sys_len': 48827, 'ref_len': 49411}, 'sari': {'sari': 74.52039483224418}, 'em': {'exact_match': 0.2191038897095027}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 10534858752, 'cuda_allocated': 3051524608, 'cuda_reserved': 9105833984, 'ram_usage': 13325615104}}, 'neutralize': {'task': 'neutralize', 'total_samples': 1127, 'elapsed_time': 912.6849405765533, 'sps': 1.2348182268549992, 'batch_size': 10, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.9471066520612496, 'rouge2': 0.9014601925484079, 'rougeL': 0.9469515066604917, 'rougeLsum': 0.946846635064823}, 'sacreblue': {'score': 91.90428520064748, 'counts': [25017, 23145, 21449, 19884], 'totals': [25812, 24685, 23558, 22433], 'precisions': [96.92003719200372, 93.7613935588414, 91.04762713303336, 88.63727544242856], 'bp': 0.9931277056087713, 'sys_len': 25812, 'ref_len': 25990}, 'sari': {'sari': 79.5776847738217}, 'em': {'exact_match': 0.45962732919254656}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 10576740352, 'cuda_allocated': 3058157056, 'cuda_reserved': 9107931136, 'ram_usage': 13749112832}}, 'paraphrase': {'task': 'paraphrase', 'total_samples': 1590, 'elapsed_time': 915.9524033069611, 'sps': 1.7358980600514313, 'batch_size': 10, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.5332524678513089, 'rouge2': 0.2802348245163171, 'rougeL': 0.44264254265984265, 'rougeLsum': 0.4429832819340773}, 'sacreblue': {'score': 17.744190155563402, 'counts': [21154, 9228, 4697, 2441], 'totals': [40792, 39202, 37612, 36022], 'precisions': [51.858207491665034, 23.539615325748688, 12.488035733276615, 6.776414413414025], 'bp': 0.9897566872670258, 'sys_len': 40792, 'ref_len': 41212}, 'sari': {'sari': 51.00808637717292}, 'em': {'exact_match': 0.02012578616352201}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 10499522560, 'cuda_allocated': 3056526336, 'cuda_reserved': 9107931136, 'ram_usage': 13956931584}}, 'coherence': {'task': 'coherence', 'total_samples': 1062, 'elapsed_time': 864.8481607437134, 'sps': 1.2279612170149599, 'batch_size': 10, 'max_length': 350, 'scores': {'rouge': {'rouge1': 0.9726846461745827, 'rouge2': 0.9453944118259234, 'rougeL': 0.9682700307950426, 'rougeLsum': 0.968219342684267}, 'sacreblue': {'score': 92.40446173238468, 'counts': [31618, 29654, 27861, 26114], 'totals': [32717, 31655, 30593, 29531], 'precisions': [96.64089005715682, 93.67872374032538, 91.0698525806557, 88.42910839456842], 'bp': 1.0, 'sys_len': 32717, 'ref_len': 32710}, 'sari': {'sari': 81.83189000086652}, 'em': {'exact_match': 0.4171374764595104}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 10494476288, 'cuda_allocated': 3055548416, 'cuda_reserved': 9107931136, 'ram_usage': 14355210240}}}\n"
     ]
    }
   ],
   "source": [
    "hardware = \"HomeDesktop (RTX3080)\"\n",
    "# hardware = \"NC24 (A100)\"\n",
    "print(f\"model_name: {model_name}, model_id: {model_id}, model_path: {model_path}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "if list in globals():\n",
    "    del list\n",
    "\n",
    "all_flats = []\n",
    "all_scores = []\n",
    "if os.path.exists(\"results/all-scores.csv\"):\n",
    "    all_scores = pd.read_csv(\"results/all-scores.csv\").to_dict(\"records\")\n",
    "\n",
    "all_fulls = []\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "\n",
    "def calculate_scores(processed_samples):\n",
    "    rouge_score = rouge_metric.compute(\n",
    "        predictions=processed_samples[\"processed\"], references=processed_samples[\"references\"]\n",
    "    )\n",
    "    # pprint(rouge_score)\n",
    "\n",
    "    sacreblue_score = sacreblue_metric.compute(\n",
    "        predictions=processed_samples[\"processed\"], references=processed_samples[\"references\"]\n",
    "    )\n",
    "    # pprint(sacreblue_score)\n",
    "\n",
    "    sari_score = sari_metric.compute(\n",
    "        sources=processed_samples[\"input\"],\n",
    "        predictions=processed_samples[\"processed\"],\n",
    "        references=processed_samples[\"references\"],\n",
    "    )\n",
    "    # pprint(sari_score)\n",
    "\n",
    "    em_score = em_metric.compute(predictions=processed_samples[\"processed\"], references=processed_samples[\"reference\"])\n",
    "    # pprint(em_score)\n",
    "\n",
    "    return {\n",
    "        \"rouge\": rouge_score,\n",
    "        \"sacreblue\": sacreblue_score,\n",
    "        \"sari\": sari_score,\n",
    "        \"em\": em_score,\n",
    "    }\n",
    "\n",
    "\n",
    "for task, obj in processed_samples_map.items():\n",
    "    print(f\"task: {task}, samples: {len(obj['samples'])}\")\n",
    "\n",
    "    batch = obj[\"samples\"]\n",
    "    total_samples = len(batch)\n",
    "\n",
    "    all_saved_samples = batch.remove_columns([\"references\"])\n",
    "    saved_samples = all_saved_samples[:100] if len(all_saved_samples) > 100 else all_saved_samples\n",
    "    flats_frame = pd.DataFrame.from_records(saved_samples)\n",
    "    flats_frame.to_json(f\"samples/{model_path}_{task}.json\", orient=\"records\")\n",
    "\n",
    "    scores = calculate_scores(batch)\n",
    "    # pprint(scores)\n",
    "\n",
    "    score_paths = [\n",
    "        \"rouge.rouge1\",\n",
    "        # \"rouge.rouge2\",\n",
    "        # \"rouge.rougeL\",\n",
    "        # \"rouge.rougeLsum\",\n",
    "        \"sacreblue.score\",\n",
    "        \"sari.sari\",\n",
    "        \"em.exact_match\",\n",
    "    ]\n",
    "\n",
    "    normalized_scores = {}\n",
    "    for s, v in scores.items():\n",
    "        print(f\"s: {s}, v: {v}\")\n",
    "        for ss, vv in v.items():\n",
    "            if not isinstance(vv, list):\n",
    "                # normalized_scores[f\"score.{k}.{ss}\"] = vv\n",
    "                path = f\"{s}.{ss}\"\n",
    "                if path in score_paths:\n",
    "                    normalized_scores[f\"score.{s}.{ss}\"] = vv\n",
    "    # pprint(normalized_scores)\n",
    "\n",
    "    normalized_utilization = {}\n",
    "    for s, v in obj[\"utilization\"].items():\n",
    "        if not isinstance(v, list):\n",
    "            normalized_utilization[f\"utilization.{s}\"] = v\n",
    "    # print(normalized_utilization)\n",
    "\n",
    "    flat_dict = {\n",
    "        \"model\": model_id,\n",
    "        \"hardware\": hardware,\n",
    "        \"total_params\": total_params,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"elapsed_time\": obj[\"elapsed_time\"],\n",
    "        \"sps\": obj[\"sps\"],\n",
    "        \"batch_size\": obj[\"batch_size\"],\n",
    "        \"max_length\": obj[\"max_length\"],\n",
    "        \"task\": task,\n",
    "    }\n",
    "    flat_dict.update(normalized_scores)\n",
    "    flat_dict.update(normalized_utilization)\n",
    "    # pprint(frame)\n",
    "\n",
    "    all_flats.append(flat_dict)\n",
    "    all_scores.append(flat_dict)\n",
    "\n",
    "    fulls_frame = {\n",
    "        \"task\": task,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"elapsed_time\": obj[\"elapsed_time\"],\n",
    "        \"sps\": obj[\"sps\"],\n",
    "        \"batch_size\": obj[\"batch_size\"],\n",
    "        \"max_length\": obj[\"max_length\"],\n",
    "    }\n",
    "    fulls_frame.update(\n",
    "        {\n",
    "            \"scores\": scores,\n",
    "            \"utilization\": obj[\"utilization\"],\n",
    "        }\n",
    "    )\n",
    "    all_fulls.append(fulls_frame)\n",
    "\n",
    "flats_frame = pd.DataFrame.from_records(all_flats)\n",
    "flats_frame.to_csv(f\"results/{model_path}.csv\", index=False)\n",
    "\n",
    "scores_frame = pd.DataFrame.from_records(all_scores)\n",
    "scores_frame.to_csv(f\"results/all-scores.csv\", index=False)\n",
    "\n",
    "fulls_dict = {\n",
    "    \"model\": model_id,\n",
    "    \"hardware\": hardware,\n",
    "    \"total_params\": total_params,\n",
    "}\n",
    "for full in all_fulls:\n",
    "    fulls_dict[full[\"task\"]] = full\n",
    "\n",
    "print(fulls_dict)\n",
    "fulls_frame = pd.DataFrame.from_records([fulls_dict])\n",
    "fulls_frame.to_json(f\"results/{model_path}.json\", orient=\"records\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
