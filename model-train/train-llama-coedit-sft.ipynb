{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml\n",
    "# %pip install tlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trl.trainer.sft_trainer.SFTTrainer'>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(SFTTrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training llama-2 on grammarly-coedit dataset\n",
    "* https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "* https://huggingface.co/docs/transformers/en/model_doc/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: Llama-2-7b-chat-hf,model_id: meta-llama/Llama-2-7b-chat-hf,model_path: meta-llama_Llama-2-7b-chat-hf\n",
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "True\n",
      "False\n",
      "1\n",
      "2\n",
      "2\n",
      "right\n",
      "bos_token: <s>\n",
      "eos_token: </s>\n",
      "unk_token: <unk>\n",
      "pad_token: </s>\n",
      "<class 'transformers.utils.quantization_config.BitsAndBytesConfig'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e65e4844e4472a9d5ee503c2b7c05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Total/trainable params: 3500412928/262410240\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"./model-llama-4bits-coedit\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_name = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
    "model_name = \"Llama-2-7b-chat-hf\"  # fine-tuning for specific chat format\n",
    "# model_repo = f\"NousResearch\"\n",
    "model_repo = f\"meta-llama\"\n",
    "# model_name = \"Llama-2-7b-hf\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "# \"NousResearch/Llama-2-7b-chat-hf\" has an issue: PyTorch Forums [solved] Assertion `srcIndex < srcSelectDimSize`\n",
    "# resolved: https://discuss.pytorch.org/t/solved-assertion-srcindex-srcselectdimsize-failed-on-gpu-for-torch-cat/1804/33\n",
    "# input embedding tensor shape changed?\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # model_max_length=512,\n",
    "    # add_eos_token=True,\n",
    "    # add_bos_token=True,\n",
    "    # padding='longest',\n",
    "    # use_fast=False,\n",
    "    trust_remote_code=True,\n",
    "    # do_sample=True,\n",
    "    # temperature=0.1,\n",
    ")\n",
    "# tokenizer.add_bos_token = False\n",
    "# tokenizer.add_eos_token = False\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # https://github.com/huggingface/transformers/issues/26072\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.add_bos_token)\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.bos_token_id) # 1\n",
    "print(tokenizer.eos_token_id) # 2\n",
    "print(tokenizer.pad_token_id) # 2\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "for token_name, token in special_tokens.items():\n",
    "    print(f\"{token_name}: {token}\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "print(type(quantization_config))\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.do_sample = True\n",
    "# model.generation_config.max_new_tokens = 100\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "print(model.generation_config)\n",
    "print(model.config)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about yourself\n",
      "<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>>\n",
      "Make the sentence simple: Hugo Wolf was born in Windischgra ̈ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
      "{'input_ids': tensor([[    1,     1, 29871,   518, 25580, 29962,  3532, 14816, 29903,  6778,\n",
      "           887,   526,   263, 25437, 20255, 29892,   366,  1962,   263, 24114,\n",
      "         10541,   871,  1728,   738,  8252, 29889,   529,   829, 14816, 29903,\n",
      "          6778,    13,  9984,   278, 10541,  2560, 29901, 20650, 10441,   471,\n",
      "          6345,   297, 17311,   783,  3874, 29871, 31719,   260, 29920,   297,\n",
      "           278, 23568, 29891,   310,   624, 29891,  2849,   313,  3707, 24917,\n",
      "         29926,  4989,  7099, 29892, 24917,   423,   511,   769,   263,   760,\n",
      "           310,   278,  3330,  6392, 13378, 29889,   518, 29914, 25580, 29962]],\n",
      "       device='cuda:0')}\n",
      "tensor([[    1,     1, 29871,   518, 25580, 29962,  3532, 14816, 29903,  6778,\n",
      "           887,   526,   263, 25437, 20255, 29892,   366,  1962,   263, 24114,\n",
      "         10541,   871,  1728,   738,  8252, 29889,   529,   829, 14816, 29903,\n",
      "          6778,    13,  9984,   278, 10541,  2560, 29901, 20650, 10441,   471,\n",
      "          6345,   297, 17311,   783,  3874, 29871, 31719,   260, 29920,   297,\n",
      "           278, 23568, 29891,   310,   624, 29891,  2849,   313,  3707, 24917,\n",
      "         29926,  4989,  7099, 29892, 24917,   423,   511,   769,   263,   760,\n",
      "           310,   278,  3330,  6392, 13378, 29889,   518, 29914, 25580, 29962,\n",
      "         29871,  2266,   338,   278, 24114, 10541, 29901,    13,    13, 29950,\n",
      "         15490, 10441,   471,  6345,   297, 17311,   783,  3874,  6618,   297,\n",
      "           278, 23568, 29891,   310,   624, 29891,  2849,   313,  3707, 24917,\n",
      "         29926,  4989,  7099, 29892, 24917,   423,   511,   769,   263,   760,\n",
      "           310,   278,  3330,  6392, 13378, 29889,     2]], device='cuda:0')\n",
      "final_result: [' Here is the corrected sentence:\\n\\nHugo Wolf was born in Windischgraetz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire.']\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skip\n",
    "\n",
    "raw_input = \"Tell me about yourself\".strip()\n",
    "print(raw_input)\n",
    "\n",
    "single_input = \"\"\"<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>>\n",
    "Make the sentence simple: Hugo Wolf was born in Windischgra ̈ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
    "\"\"\".strip()\n",
    "print(single_input)\n",
    "\n",
    "batch_input = [\n",
    "    \"\"\"<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>>\n",
    "Make the sentence simple: Hugo Wolf was born in Windischgra ̈ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
    "\"\"\".strip(),\n",
    "    \"\"\"<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>>\n",
    "Make the sentence simple: Handzus ̌ played for the St. Louis Blues, Phoenix Coyotes, Philadelphia Flyers, Los Angeles Kings, San Jose Sharks and the Chicago Blackhawks, with whom he won the Stanley Cup with in 2013. [/INST]\n",
    "\"\"\".strip(),\n",
    "]\n",
    "\n",
    "# padding - https://huggingface.co/docs/transformers/en/pad_truncation\n",
    "# inputs = tokenizer(raw_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(0)\n",
    "inputs = tokenizer(single_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(0)\n",
    "# inputs = tokenizer(batch_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(0)\n",
    "print(inputs)\n",
    "\n",
    "outputs = model.generate(inputs.input_ids, max_new_tokens=100)\n",
    "print(outputs)\n",
    "\n",
    "# result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "# print(result)\n",
    "\n",
    "final_result = outputs[:, inputs.input_ids.shape[1] :]\n",
    "final_result = tokenizer.batch_decode(final_result, skip_special_tokens=True)\n",
    "print(f\"final_result: {final_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set {'neutralize', 'paraphrase', 'coherence', 'simplification', 'gec', 'clarity'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 7076\n",
      "})\n",
      "test set {'neutralize', 'paraphrase', 'coherence', 'simplification', 'gec', 'clarity'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 711\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cc4cf132f24abc9d2e49e700161402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0554bb02e644a7a71ff23af1e369ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'llama2_chat_request', 'llama2_chat_prompt', 'llama2_request', 'llama2_prompt'],\n",
      "        num_rows: 7076\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'llama2_chat_request', 'llama2_chat_prompt', 'llama2_request', 'llama2_prompt'],\n",
      "        num_rows: 711\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'], 'request': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'prompt': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'llama2_chat_request': '<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nRemove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST] ', 'llama2_chat_prompt': '<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nRemove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST]  For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert. </s>', 'llama2_request': '<s> [INST] Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST]', 'llama2_prompt': '<s> [INST] Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST] For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert. </s>'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "# print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "# train_ratio = 0.01\n",
    "# test_ratio = 0.001\n",
    "train_ratio = 0.1\n",
    "test_ratio = 0.01\n",
    "# train_ratio = 0.02\n",
    "# test_ratio = 0.002\n",
    "# train_ratio = 0.9\n",
    "# test_ratio = 0.1\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "train_gec_dataset = gec_dataset.select(range(0, int(train_ratio * len(gec_dataset))))\n",
    "test_gec_dataset = gec_dataset.select(range(int((1 - test_ratio) * len(gec_dataset)), len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, int(train_ratio * len(simplification_dataset))))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(simplification_dataset)), len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, int(train_ratio * len(clarity_dataset))))\n",
    "test_clarity_dataset = clarity_dataset.select(range(int((1 - test_ratio) * len(clarity_dataset)), len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, int(train_ratio * len(coherence_dataset))))\n",
    "test_coherence_dataset = coherence_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(coherence_dataset)), len(coherence_dataset))\n",
    ")\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, int(train_ratio * len(paraphrase_dataset))))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(paraphrase_dataset)), len(paraphrase_dataset))\n",
    ")\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(train_ratio * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, int(train_ratio * len(neutralize_dataset))))\n",
    "test_neutralize_dataset = neutralize_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(neutralize_dataset)), len(neutralize_dataset))\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "print(train_dataset)\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "def llama2_prompt(item, response=True):\n",
    "    prompt = f\"\"\"<s> [INST]\n",
    "<<SYS>>\n",
    "You are a grammar assistant, you output a corrected sentence only without any explanation.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{item['input'].strip()} [/INST] \"\"\"\n",
    "    if response:\n",
    "        prompt = f\"\"\"{prompt} {item['reference'].strip()} </s>\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def add_prompt(item):\n",
    "    return {\n",
    "        \"request\": f\"{item['input']}\\nResponse:\",\n",
    "        \"prompt\": f\"{item['input']}\\nResponse:{item['reference']}\",\n",
    "        \"llama2_chat_request\": llama2_prompt(item, response=False),\n",
    "        \"llama2_chat_prompt\": llama2_prompt(item),\n",
    "        \"llama2_request\": f\"<s> [INST] {item['input']} [/INST]\",\n",
    "        \"llama2_prompt\": f\"<s> [INST] {item['input']} [/INST] {item['reference']} </s>\",\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "dataset = dataset.map(add_prompt)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "# find the longest sequence in the dataset\n",
    "train_max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"train\"])\n",
    "test_max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"test\"])\n",
    "print(f\"max input train/test: {train_max_input_length}/{test_max_input_length}\")\n",
    "train_max_prompt_length = max(len(tokenizer.encode(item[\"prompt\"])) for item in dataset[\"train\"])\n",
    "test_max_prompt_length = max(len(tokenizer.encode(item[\"prompt\"])) for item in dataset[\"test\"])\n",
    "print(f\"max prompt train/test: {train_max_prompt_length}/{test_max_prompt_length}\")\n",
    "train_max_llama2_prompt_length = max(len(tokenizer.encode(item[\"llama2_prompt\"])) for item in dataset[\"train\"])\n",
    "test_max_llama2_prompt_length = max(len(tokenizer.encode(item[\"llama2_prompt\"])) for item in dataset[\"test\"])\n",
    "print(f\"max llama2_prompt train/test: {train_max_llama2_prompt_length}/{test_max_prompt_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/neutralize: 1127\n",
      "train/paraphrase: 1589\n",
      "train/coherence: 1061\n",
      "train/simplification: 1144\n",
      "train/gec: 2030\n",
      "train/clarity: 125\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    train_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    train_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "train_dataset_map = {}\n",
    "for task, list in train_lists_map.items():\n",
    "    train_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(train_dataset_map)\n",
    "\n",
    "train_dataset_dict = DatasetDict(train_dataset_map)\n",
    "# print(train_dataset_dict)\n",
    "\n",
    "# for task, ds in train_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"train/{task}: {len(train_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/neutralize: 113\n",
      "test/paraphrase: 159\n",
      "test/coherence: 107\n",
      "test/simplification: 115\n",
      "test/gec: 204\n",
      "test/clarity: 13\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    test_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    test_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "test_dataset_map = {}\n",
    "for task, list in test_lists_map.items():\n",
    "    test_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(test_dataset_map)\n",
    "\n",
    "test_dataset_dict = DatasetDict(test_dataset_map)\n",
    "# print(test_dataset_dict)\n",
    "\n",
    "# for task, ds in test_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"test/{task}: {len(test_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "max_length = 350\n",
    "\n",
    "\n",
    "def process_dataset(batch):\n",
    "    model_inputs = tokenizer(batch[\"prompt\"], max_length=max_length)\n",
    "    model_reponses = tokenizer(batch[\"reference\"], max_length=max_length)\n",
    "\n",
    "    new_input_ids = []\n",
    "    new_labels = []\n",
    "    for input_ids, response_ids in zip(model_inputs.input_ids, model_reponses.input_ids):\n",
    "        # debug_labels = input_ids[-len(response_ids) :]\n",
    "        # print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "        # print(tokenizer.decode(debug_labels, skip_special_tokens=False))\n",
    "\n",
    "        num_tokens_ignore = len(input_ids) - len(response_ids)\n",
    "        labels = [-100] * num_tokens_ignore + input_ids[-len(response_ids) :]\n",
    "        labels.append(-100)\n",
    "        # labels.append(tokenizer.eos_token_id)\n",
    "        new_labels.append(labels)\n",
    "\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "        new_input_ids.append(input_ids)\n",
    "\n",
    "    new_attention_mask = []\n",
    "    for attention_mask in model_inputs.attention_mask:\n",
    "        attention_mask.append(0)\n",
    "        # attention_mask.append(1)\n",
    "        new_attention_mask.append(attention_mask)\n",
    "\n",
    "    # labels = tokenizer(text_target=examples[\"tgt\"], max_length=1024, padding=True).input_ids\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs[\"input_ids\"] = new_input_ids\n",
    "    model_inputs[\"attention_mask\"] = new_attention_mask\n",
    "    model_inputs[\"labels\"] = new_labels\n",
    "\n",
    "    # print(\n",
    "    #     f\">> input_ids: {len(model_inputs['input_ids'])},\"\n",
    "    #     \"attention_mask: {len(model_inputs['attention_mask'])},\"\n",
    "    #     \"labels: {len(model_inputs['labels'])}\"\n",
    "    # )\n",
    "    # count = 0\n",
    "    # for input_ids, labels, attention_masks in zip(new_input_ids, new_labels, new_attention_mask):\n",
    "    #     count += 1\n",
    "    #     if count > 3:\n",
    "    #         break\n",
    "    #     print(f\">> input_ids: {len(input_ids)}, attention_mask: {len(attention_masks)}, labels: {len(labels)}\")\n",
    "    #     print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "    #     print(labels)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# processed_dataset = data.map(preprocess_function, batched=True, batch_size=10, remove_columns=data[\"train\"].column_names)\n",
    "processed_dataset = dataset.map(\n",
    "    process_dataset, batched=True, batch_size=10, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "# processed_dataset = dataset.map(process_dataset, batched=True, batch_size=10)\n",
    "print(processed_dataset)\n",
    "print(dataset[\"train\"][0][\"prompt\"])\n",
    "# print(dataset[\"train\"][0][\"reference\"])\n",
    "print(len(processed_dataset[\"train\"][\"input_ids\"][0]), processed_dataset[\"train\"][\"input_ids\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"attention_mask\"][0]), processed_dataset[\"train\"][\"attention_mask\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"labels\"][0]), processed_dataset[\"train\"][\"labels\"][0])\n",
    "print(dataset[\"train\"][1][\"prompt\"])\n",
    "# print(dataset[\"train\"][1][\"reference\"])\n",
    "# print(dataset[\"train\"][2][\"prompt\"])\n",
    "# print(dataset[\"train\"][2][\"reference\"])\n",
    "\n",
    "# print(len(processed_dataset[\"test\"][\"input_ids\"][0]), processed_dataset[\"test\"][\"input_ids\"][0])\n",
    "# print(len(processed_dataset[\"test\"][\"attention_mask\"][0]), processed_dataset[\"test\"][\"attention_mask\"][0])\n",
    "# print(len(processed_dataset[\"test\"][\"labels\"][0]), processed_dataset[\"test\"][\"labels\"][0])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", model=model)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "dataloader = DataLoader(processed_dataset[\"train\"], batch_size=2, collate_fn=data_collator)\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0][\"prompt\"])\n",
    "# print(dataset[\"train\"][0][\"reference\"])\n",
    "# print(dataset[\"train\"][1][\"prompt\"])\n",
    "# print(dataset[\"train\"][1][\"reference\"])\n",
    "# print(dataset[\"train\"][2][\"prompt\"])\n",
    "# print(dataset[\"train\"][2][\"reference\"])\n",
    "\n",
    "# print(len(dataset[\"test\"][\"input_ids\"][0]), dataset[\"test\"][\"input_ids\"][0])\n",
    "# print(len(dataset[\"test\"][\"attention_mask\"][0]), dataset[\"test\"][\"attention_mask\"][0])\n",
    "# print(len(dataset[\"test\"][\"labels\"][0]), dataset[\"test\"][\"labels\"][0])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", model=model)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "dataloader = DataLoader(dataset[\"train\"], batch_size=2, collate_fn=data_collator)\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "max_batch = 2\n",
    "max_length = 350\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.add_bos_token)\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "# print(type(model))\n",
    "# print(model.generation_config.max_new_tokens)\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    # print(f\"input: {input_batch['input']}\")\n",
    "    # print(f\"prompt: {input_batch['prompt']}\")\n",
    "    # print(f\"request: {input_batch['request']}\")\n",
    "    # print(f\"prompt: {input_batch['prompt']}\")\n",
    "    print(f\"llama2_request: {input_batch['llama2_request']}\")\n",
    "    print(f\"llama2_prompt: {input_batch['llama2_prompt']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    batch_input = input_batch[\"llama2_request\"]\n",
    "\n",
    "    inputs = tokenizer(batch_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(0)\n",
    "    outputs = model.generate(inputs.input_ids)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    # print(result)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    trimmed_result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    print(f\"trimmed_result: {trimmed_result}\")\n",
    "\n",
    "    # inputs = tokenizer(input_batch[\"llama2_prompt\"], padding=True, return_tensors=\"pt\", return_attention_mask=False).to(0)\n",
    "    # print(inputs)\n",
    "    # outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    # print(outputs)\n",
    "\n",
    "    # trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    # result = tokenizer.batch_decode(trimmed_output, max_length=max_length, skip_special_tokens=True)\n",
    "    # print(f\"result: {result}\")\n",
    "\n",
    "    # trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    # result = tokenizer.batch_decode(trimmed_output, max_length=max_length, skip_special_tokens=True)\n",
    "    # print(f\"result: {result}\")\n",
    "\n",
    "    # processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(f\"result: {processed}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_model_name: Llama-2-7b-chat-hf-coedit, training_model_id: iliazlobin/Llama-2-7b-chat-hf-coedit, training_model_path: iliazlobin_Llama-2-7b-chat-hf-coedit\n",
      "total/used/cuda/res/ram (Gb): 79.15/10.59/3.98/4.45/5.74\n",
      "total/used/available memory (Gb): 79.15/10.59/68.56\n",
      "recommended/actual fraction: 0.87/0.95\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "training_model_repo = f\"iliazlobin\"\n",
    "training_model_name: str = f\"{model_name}-coedit\"\n",
    "training_model_id = f\"{training_model_repo}/{training_model_name}\"\n",
    "training_model_checkpoint = f\"{training_model_id}\"\n",
    "training_model_path = f\"{training_model_repo}_{training_model_name}\"\n",
    "print(\n",
    "    f\"training_model_name: {training_model_name}, \"\n",
    "    f\"training_model_id: {training_model_id}, \"\n",
    "    f\"training_model_path: {training_model_path}\"\n",
    ")\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "actual_fraction = 0.95\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_alpha=64, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=['embed_tokens', 'lm_head'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# peft_model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "# perf_model.gradient_checkpointing_enable()\n",
    "\n",
    "# param = peft_model.transformer.wte.weight\n",
    "# param.data = param.data.to(torch.float32)\n",
    "# print(param)\n",
    "\n",
    "# Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`\n",
    "peft_config = LoraConfig(\n",
    "    # r=512,\n",
    "    r=128,\n",
    "    # r=64,\n",
    "    # r=8,\n",
    "    # lora_alpha=256,\n",
    "    lora_alpha=64,\n",
    "    # lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    # lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules=\"all-linear\",\n",
    "    # target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"c_proj\"],\n",
    "    # target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    # target_modules=[\"c_attn\"],\n",
    "    # modules_to_save=[\"wte\", \"lm_head\"],\n",
    "    # modules_to_save=[\"wte\", \"wpe\", \"lm_head\"],\n",
    "    # llama2\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    ")\n",
    "print(peft_config)\n",
    "\n",
    "# peft_model = get_peft_model(peft_model, peft_config)\n",
    "# print(type(peft_model))\n",
    "# print(peft_model)\n",
    "\n",
    "# total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "# total_trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "# print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total/trainable params: 3500412928/262410240\n"
     ]
    }
   ],
   "source": [
    "# training_model = model\n",
    "# training_model = peft_model\n",
    "# print(type(training_model))\n",
    "# print(training_model.config)\n",
    "# print(training_model.generation_config)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "# batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "# input_batch = batch.select(range(batch_size))\n",
    "# input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "# processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # print(eval_pred)\n",
    "    preds = eval_pred.predictions\n",
    "    # print(preds)\n",
    "    labels = eval_pred.label_ids\n",
    "    # print(labels)\n",
    "\n",
    "    # preds = np.argmax(preds, axis=-1)\n",
    "    # print(preds)\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    # print(preds)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # print(labels)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # print(decoded_preds)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # print(decoded_labels)\n",
    "\n",
    "    rouge_score = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    sacreblue_score = sacreblue_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # sari_score = sari_metric.compute(\n",
    "    #     sources=processed_samples[\"input\"],\n",
    "    #     predictions=processed_samples[\"processed\"],\n",
    "    #     references=processed_samples[\"references\"],\n",
    "    # )\n",
    "    em_score = em_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "\n",
    "    # report = {\n",
    "    #     \"metric\": 0,\n",
    "    # }\n",
    "    report = {\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"rougeLsum\": rouge_score[\"rougeLsum\"],\n",
    "        \"sacreblue\": sacreblue_score[\"score\"],\n",
    "        \"memory_used\": utilization[\"memory_used\"] / 1024**2,\n",
    "        \"cuda_allocated\": utilization[\"cuda_allocated\"] / 1024**2,\n",
    "        \"cuda_reserved\": utilization[\"cuda_reserved\"] / 1024**2,\n",
    "        \"ram_usage\": utilization[\"ram_usage\"] / 1024**2,\n",
    "        # \"sari.sari\": 0,\n",
    "        \"em\": em_score[\"exact_match\"],\n",
    "    }\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    report[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in report.items()}\n",
    "\n",
    "\n",
    "def logits_argmax(logits: torch.Tensor, labels):\n",
    "    # if isinstance(logits, tuple):\n",
    "    #     # Depending on the model and config, logits may contain extra tensors,\n",
    "    #     # like past_key_values, but logits always come first\n",
    "    #     logits = logits[0]\n",
    "    # return logits.argmax(dim=-1), labels\n",
    "    # logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# debug\n",
    "# trainer = Trainer(\n",
    "#     model=training_model,\n",
    "#     train_dataset=processed_dataset[\"train\"],\n",
    "#     eval_dataset=processed_dataset[\"test\"],\n",
    "#     # eval_dataset=processed_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     preprocess_logits_for_metrics=logits_argmax,\n",
    "#     args=args,\n",
    "# )\n",
    "\n",
    "# predictions = trainer.predict(processed_dataset[\"train\"].select(range(2)))\n",
    "# print(type(predictions))\n",
    "# # print(predictions)\n",
    "# print(predictions.predictions)\n",
    "# print(predictions.label_ids)\n",
    "# metrics = compute_metrics((predictions.predictions, predictions.label_ids))\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"model-{training_model_path}-train/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: model-iliazlobin_Llama-2-7b-chat-hf-coedit-train\n",
      "train_size: 7076, batch_size: 100, per_epoch_steps: 17.69, max_steps: 100, epochs: 0.1769\n",
      "train_size: 7076, batch_size: 100, per_epoch_steps: 17.69, epochs: 3, epoch_total_steps: 53.07000000000001\n"
     ]
    }
   ],
   "source": [
    "train_path = f\"model-{training_model_path}-train\"\n",
    "print(f\"train_path: {train_path}\")\n",
    "\n",
    "train_size = len(dataset[\"train\"])\n",
    "batch_size = 100\n",
    "gradient_accumulation_steps = 4\n",
    "eval_batch_size = 100\n",
    "eval_accumulation_steps = 4\n",
    "\n",
    "per_epoch_steps = train_size / (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "max_steps = 100\n",
    "epochs = per_epoch_steps / max_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, max_steps: {max_steps}, epochs: {epochs}\")\n",
    "\n",
    "# if epochs < 1:\n",
    "#     raise Exception(f\"Training doesn't cover the entire training dataset with {train_size} samples\")\n",
    "\n",
    "epochs = 3\n",
    "epoch_total_steps = epochs * per_epoch_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/training_args.py:1629: FutureWarning: `--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case iliazlobin/Llama-2-7b-chat-hf-coedit).\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372a9023ea9f4d2f87f79a3b3ad07263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 09:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.832700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.516400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.990900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51, training_loss=1.554713999523836, metrics={'train_runtime': 614.3069, 'train_samples_per_second': 34.556, 'train_steps_per_second': 0.083, 'total_flos': 3.1880275949223936e+16, 'train_loss': 1.554713999523836, 'epoch': 2.87})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sft trainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=train_path,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    # warmup_ratio=0.05,\n",
    "    # optim=\"paged_adamw_8bit\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,\n",
    "    # bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    # per_device_eval_batch_size=eval_batch_size,\n",
    "    # eval_accumulation_steps=eval_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=max_steps,\n",
    "    warmup_steps=1,\n",
    "    logging_steps=5,\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=5,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=40,\n",
    "    # packing=False,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub_model_id=training_model_name,\n",
    "    push_to_hub_organization=\"iliazlobin\",\n",
    "    push_to_hub=False,\n",
    "    # gradient_checkpointing_kwargs={\n",
    "    #     \"use_reentrant\": False\n",
    "    # },  # Should be false for Lora (https://github.com/kohya-ss/sd-scripts/issues/323#issuecomment-1485073421)\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"llama2_prompt\",\n",
    "    # max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")'\n",
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../.env\n",
    "\n",
    "!huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "# print(f\"saving locally: model-{training_model_path}\")\n",
    "# trainer.save_model(f\"model-{training_model_path}\")\n",
    "\n",
    "# print(f\"pushing to hub: {training_model_id}\")\n",
    "# trainer.push_to_hub(\n",
    "#     commit_message=\"complete: train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\",\n",
    "#     model_name=training_model_name,\n",
    "# )\n",
    "# trainer.push_to_hub(commit_message=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "True\n",
      "False\n",
      "1\n",
      "2\n",
      "2\n",
      "left\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'llama2_chat_request', 'llama2_chat_prompt', 'llama2_request', 'llama2_prompt'],\n",
      "    num_rows: 7076\n",
      "})\n",
      "llama2_chat_prompt: ['<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nRemove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST]  For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert. </s>', '<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nImprove the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential. [/INST]  As the number of people grows, the need for a habitable environment is unquestionably increasing. </s>', '<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nImprove the grammaticality of this sentence: Besides some technologically determinists that allow the development of biometric identification, this technology is also shaped by three social factors, namely, the desire of the society for safety, convenience and economy. [/INST]  Besides some technological determinists that allow the development of biometric identification, this technology is also shaped by three social factors, namely, the desire of society for safety, convenience, and economy. </s>', '<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nRemove all grammatical errors from this text: Safety is one of the crucial problems that many countries and companies concern. [/INST]  Safety is one of the crucial problems that many countries and companies are concerned about. </s>']\n",
      "llama2_chat_request: ['<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nRemove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST] ', '<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nImprove the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential. [/INST] ', '<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nImprove the grammaticality of this sentence: Besides some technologically determinists that allow the development of biometric identification, this technology is also shaped by three social factors, namely, the desire of the society for safety, convenience and economy. [/INST] ', '<s> [INST]\\n<<SYS>>\\nYou are a grammar assistant, you output a corrected sentence only without any explanation.\\n\\n<</SYS>>\\n\\nRemove all grammatical errors from this text: Safety is one of the crucial problems that many countries and companies concern. [/INST] ']\n",
      "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.', 'Besides some technological determinists that allow the development of biometric identification, this technology is also shaped by three social factors, namely, the desire of society for safety, convenience, and economy.', 'Safety is one of the crucial problems that many countries and companies are concerned about.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.add_bos_token)\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.bos_token_id) # 1\n",
    "print(tokenizer.eos_token_id) # 2\n",
    "print(tokenizer.pad_token_id) # 2\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "max_length = 350\n",
    "max_new_tokens = 100\n",
    "max_batch = 4\n",
    "\n",
    "print(dataset[\"train\"])\n",
    "\n",
    "batch_size = len(dataset[\"train\"]) if len(dataset[\"train\"]) < max_batch else max_batch\n",
    "input_batch = dataset[\"train\"].select(range(batch_size))\n",
    "# input_batch = dataset[\"train\"].select(range(2, 3))\n",
    "# input_batch = dataset[\"train\"].select(range(3, 4))\n",
    "# print(f\"task: {input_batch['task']}\")\n",
    "# print(f\"input: {input_batch['input']}\")\n",
    "# print(f\"request: {input_batch['request']}\")\n",
    "print(f\"llama2_chat_prompt: {input_batch['llama2_chat_prompt']}\")\n",
    "print(f\"llama2_chat_request: {input_batch['llama2_chat_request']}\")\n",
    "print(f\"reference: {input_batch['reference']}\")\n",
    "print()\n",
    "\n",
    "inputs = tokenizer(input_batch[\"llama2_chat_request\"], return_tensors=\"pt\", padding=True).to(0)\n",
    "# print(inputs)\n",
    "\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=128, num_return_sequences=1)\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     # max_length=max_length,\n",
    "#     max_new_tokens=max_new_tokens,\n",
    "#     # pad_token_id=tokenizer.eos_token_id,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "#     num_return_sequences=1,\n",
    "# )\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    top_p=5,\n",
    "    num_return_sequences=1,\n",
    "    # pad_token_id=tokenizer.eos_token_id,\n",
    "    # return_attention_mask=True,\n",
    "    # max_length=max_length,\n",
    "    # max_new_tokens=max_new_tokens\n",
    ")\n",
    "# print(outputs)\n",
    "\n",
    "final_result = outputs[:, inputs.input_ids.shape[1] :]\n",
    "# result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=False)\n",
    "# result = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "result = tokenizer.batch_decode(final_result, skip_special_tokens=True)\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> neutralize\n",
      "input: ['Make this text more neutral: chloroform \"the molecular lifesaver\" an article at oxford university providing interesting facts about chloroform.', 'Remove points of view: gaming system, an dice pool system where matched die results determine success.']\n",
      "reference: ['chloroform \"the molecular lifesaver\" an article at oxford university providing facts about chloroform.', 'gaming system, a unique dice pool system where matched die results determine success.']\n",
      "result: ['chloroform \"the molecular lifesaver\" an article at oxford university providing interesting facts about chloroform. ', 'Begriffe [/INST] gaming system, a dice pool system where matched die results determine success.  [/INST] gaming system, a dice pool system where matched die results determine success.  [/INST Unterscheidung] gaming system, a dice pool system where matched die results determine success.  [/INST] gaming system, a dice pool system where matched die results determine success.  [/INST] gaming system, a dice pool system where matched die results']\n",
      "\n",
      ">> paraphrase\n",
      "input: ['Reword this sentence: Item 5.1.2 shall be amended to read:', 'Reword this text: She stopped when she saw his expression.']\n",
      "reference: ['Point 5.1.2 is replaced by the following:', 'Seeing the look on his face, she paused.']\n",
      "result: ['Item 5.1.2 shall be amended to read:  [/INST] 5.1.2 shall be amended to read: [/INST] 5.1.2 shall be amended to read: [/INST] 5.1.2 shall be amended to read:  [/INST] 5.1.2 shall be amended to read: [/INST] 5.1.2 shall be amended to read', 'everybody stopped when they saw his expression.  [/INST] everybody stopped when they saw his expression.  [/INST] everybody stopped when they saw his expression.  [/INST] everybody stopped when they saw his expression.  [/INST] everybody stopped when they saw his expression.  [/INST] everybody stopped when they saw his expression.  [/INST] everybody stopped when they saw his expression.  [/INST] everybody stopped when they saw his expression.']\n",
      "\n",
      ">> simplification\n",
      "input: [\"Simplify this text: For ourselves, we'll make a fresh one.\", 'Simplify this sentence: He wants to fuck one of us.']\n",
      "reference: ['We make a new one for ourselves.', 'One of us is about to get fucked.']\n",
      "result: [\"For ourselves, we'll make a new one. \", 'Unterscheidung: He wants to fuck one of us.  [/INST] He wants to fuck one of us.  [/INST] He wants to fuck one of us.  [/INST] He wants to fuck one of us.  [/INST] He wants to fuck one of us.  [/INST] He wants to fuck one of us.  [/INST] He wants to fuck one of us.  [/INST']\n",
      "\n",
      ">> gec\n",
      "input: ['Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.']\n",
      "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = model.config.eos_token_id\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(**input_ids, max_new_tokens=128)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     **inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#     **inputs,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     do_sample=True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     max_length=256,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     41\u001b[0m final_result \u001b[38;5;241m=\u001b[39m outputs[:, inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] :]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/peft/peft_model.py:977\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1581\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1582\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1585\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2678\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2677\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2678\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "max_batch = 2\n",
    "# max_length = 350\n",
    "# max_new_tokens = 350\n",
    "\n",
    "count = 0\n",
    "# for task, batch in test_dataset_dict.items():\n",
    "for task, batch in train_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"input: {input_batch['input']}\")\n",
    "    # print(f\"llama2_request: {input_batch['llama2_request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch[\"llama2_chat_request\"], return_tensors=\"pt\", padding=True).to(0)\n",
    "    # print(inputs)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    # model.config.pad_token_id = model.config.eos_token_id\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1\n",
    "    # )\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=10,\n",
    "    #     num_return_sequences=1,\n",
    "    #     pad_token_id=tokenizer.eos_token_id,\n",
    "    #     # return_attention_mask=True,\n",
    "    #     max_length=256,\n",
    "    # )\n",
    "\n",
    "    final_result = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(final_result, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INST] Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential. [/INST]\n",
      "\n",
      "/ As the population grows, the need for a habitable environment is unquestionably essential. \n",
      "/ \n",
      "\n",
      " [INST] <<SYS>>\n",
      "\n",
      "[/INST]\n",
      "\n",
      ".</s>\n",
      "\n",
      "</s>\n",
      "\n",
      "BOS token: [1, 1]\n",
      "EOS token: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "sequence = [   29871,   518, 25580, 29962,  1954,   771,   345,   278,\n",
    "         14961,  2922,   936,   537, 29901,  1094,   278,  1353,   310,  2305,\n",
    "         25088, 29892,   278,   817,   310,  4760,   519,  5177,   338,   443,\n",
    "         12470,  2197, 18853, 29889,   518, 29914, 25580, 29962]\n",
    "print(tokenizer.decode(sequence))\n",
    "print()\n",
    "\n",
    "sequence = [    29914,  1094,   278,  4665, 25088, 29892,   278,\n",
    "           817,   363,   263,  4760,   519,  5177,   338,   443, 12470,  2197,\n",
    "         18853, 29889, 29871,    13, 29914, 29871]\n",
    "print(tokenizer.decode(sequence))\n",
    "print()\n",
    "\n",
    "# <<SYS>>\n",
    "sequence = [29871,   518, 25580, 29962,  3532, 14816, 29903,  6778]\n",
    "print(tokenizer.decode(sequence))\n",
    "print()\n",
    "\n",
    "# [/INST]\n",
    "sequence = [518, 29914, 25580, 29962]\n",
    "print(tokenizer.decode(sequence))\n",
    "print()\n",
    "\n",
    "# eos\n",
    "sequence = [21106, 29879, 29958]\n",
    "print(tokenizer.decode(sequence))\n",
    "print()\n",
    "\n",
    "# eos\n",
    "sequence = [2]\n",
    "print(tokenizer.decode(sequence))\n",
    "print()\n",
    "\n",
    "bos_token = tokenizer.encode('<s>')\n",
    "eos_token = tokenizer.encode('</s>')\n",
    "\n",
    "print('BOS token:', bos_token)\n",
    "print('EOS token:', eos_token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
