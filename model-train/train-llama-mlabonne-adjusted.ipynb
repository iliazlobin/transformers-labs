{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tunning\n",
    "* https://github.com/mlabonne/llm-course/blob/main/Fine_tune_Llama_2_in_Google_Colab.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml\n",
    "# %pip install tlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n",
      "<class 'trl.trainer.sft_trainer.SFTTrainer'>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(SFTTrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training llama-2 on grammarly-coedit dataset\n",
    "* https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "* https://huggingface.co/docs/transformers/en/model_doc/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: Llama-2-7b-chat-hf,model_id: meta-llama/Llama-2-7b-chat-hf,model_path: meta-llama_Llama-2-7b-chat-hf\n",
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "False\n",
      "False\n",
      "1\n",
      "2\n",
      "2\n",
      "right\n",
      "<class 'transformers.utils.quantization_config.BitsAndBytesConfig'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616d94834d5c462d8a8f652234133dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_new_tokens\": 100,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Total/trainable params: 6738415616/6738415616\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"./model-llama-4bits-coedit\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_name = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
    "model_name = \"Llama-2-7b-chat-hf\"  # fine-tuning for specific chat format\n",
    "model_repo = f\"NousResearch\"\n",
    "# model_repo = f\"meta-llama\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "# \"NousResearch/Llama-2-7b-chat-hf\" has an issue: PyTorch Forums [solved] Assertion `srcIndex < srcSelectDimSize`\n",
    "# resolved: https://discuss.pytorch.org/t/solved-assertion-srcindex-srcselectdimsize-failed-on-gpu-for-torch-cat/1804/33\n",
    "# input embedding tensor shape changed?\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # model_max_length=512,\n",
    "    # add_eos_token=True,\n",
    "    # add_bos_token=True,\n",
    "    # padding='longest',\n",
    "    # use_fast=False,\n",
    "    trust_remote_code=True,\n",
    "    # do_sample=True,\n",
    "    # temperature=0.1,\n",
    ")\n",
    "# tokenizer.add_bos_token = False\n",
    "# tokenizer.add_eos_token = False\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # https://github.com/huggingface/transformers/issues/26072\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.add_bos_token)\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.bos_token_id) # 1\n",
    "print(tokenizer.eos_token_id) # 2\n",
    "print(tokenizer.pad_token_id) # 2\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "print(type(quantization_config))\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "model.generation_config.do_sample = True\n",
    "model.generation_config.max_new_tokens = 100\n",
    "print(model.generation_config)\n",
    "print(model.config)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  376,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n",
      "          3492, 29915,   276,   263, 25437, 20255, 29889,   887,   925, 10683,\n",
      "         10240, 25260,   304,  1207,   963,  1959, 29889,    13,  3492,   437,\n",
      "           451,  3867,   738,  8252, 29889,   887,  1095,   596,  2933,  1492,\n",
      "          1156,   278, 24114, 10541, 29889,    13, 29966,   829, 14816, 29903,\n",
      "          6778,    13,  6113,   263,   610,   481,  1092,   559,   363,   278,\n",
      "         10541, 29901,   306,  1016, 29915, 29873,  1348,   445,   338,   263,\n",
      "         12561, 29889,   518, 29914, 25580, 18017]], device='cuda:0')}\n",
      "tensor([[  376,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n",
      "          3492, 29915,   276,   263, 25437, 20255, 29889,   887,   925, 10683,\n",
      "         10240, 25260,   304,  1207,   963,  1959, 29889,    13,  3492,   437,\n",
      "           451,  3867,   738,  8252, 29889,   887,  1095,   596,  2933,  1492,\n",
      "          1156,   278, 24114, 10541, 29889,    13, 29966,   829, 14816, 29903,\n",
      "          6778,    13,  6113,   263,   610,   481,  1092,   559,   363,   278,\n",
      "         10541, 29901,   306,  1016, 29915, 29873,  1348,   445,   338,   263,\n",
      "         12561, 29889,   518, 29914, 25580, 18017, 29902,  1016, 29915, 29873,\n",
      "          1348,   445,   338,   263, 12561,  1213,     2]], device='cuda:0')\n",
      "final_result: ['I don\\'t think this is a dream.\"']\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skip\n",
    "\n",
    "singnle_input = \"\"\"\"<s>[INST] <<SYS>>\n",
    "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
    "You do not provide any explanation. You end your response right after the corrected sentence.\n",
    "<</SYS>>\n",
    "Write a paraphrase for the sentence: I don't think this is a dream. [/INST]\"\n",
    "\"\"\".strip()\n",
    "\n",
    "batch_input = [\n",
    "    \"\"\"<s>[INST] <<SYS>>\n",
    "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
    "You do not provide any explanation. You end your response right after the corrected sentence.\n",
    "<</SYS>>\n",
    "Make the sentence simple: Hugo Wolf was born in Windischgra Ìˆ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
    "\"\"\".strip(),\n",
    "    \"\"\"<s>[INST] <<SYS>>\n",
    "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
    "You do not provide any explanation. You end your response right after the corrected sentence.\n",
    "<</SYS>>\n",
    "Make the sentence simple: Handzus ÌŒ played for the St. Louis Blues, Phoenix Coyotes, Philadelphia Flyers, Los Angeles Kings, San Jose Sharks and the Chicago Blackhawks, with whom he won the Stanley Cup with in 2013. [/INST]\n",
    "\"\"\".strip(),\n",
    "]\n",
    "\n",
    "# padding - https://huggingface.co/docs/transformers/en/pad_truncation\n",
    "# inputs = tokenizer(input, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "# inputs = tokenizer(batch_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "# inputs = tokenizer(batch_less_input, padding=True, truncation=True, max_length=100, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "# inputs = tokenizer(coedit_input_batch, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "inputs = tokenizer(singnle_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(0)\n",
    "# inputs = tokenizer(coedit_input, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "print(inputs)\n",
    "\n",
    "# print(model)\n",
    "# outputs = model.generate(**inputs)\n",
    "outputs = model.generate(inputs.input_ids)\n",
    "print(outputs)\n",
    "\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "# print(result)\n",
    "\n",
    "final_result = outputs[:, inputs.input_ids.shape[1] :]\n",
    "final_result = tokenizer.batch_decode(final_result, skip_special_tokens=True)\n",
    "print(f\"final_result: {final_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlabonne/guanaco-llama2-1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_model_name: Llama-2-7b-chat-hf-mlabone-guanaco-1k, training_model_id: iliazlobin/Llama-2-7b-chat-hf-mlabone-guanaco-1k, training_model_path: iliazlobin_Llama-2-7b-chat-hf-mlabone-guanaco-1k\n",
      "total/used/cuda/res/ram (Gb): 79.15/5.43/3.99/4.09/3.82\n",
      "total/used/available memory (Gb): 79.15/5.43/73.72\n",
      "recommended/actual fraction: 0.93/0.95\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "training_model_repo = f\"iliazlobin\"\n",
    "training_model_name: str = f\"{model_name}-mlabone-guanaco-1k\"\n",
    "training_model_id = f\"{training_model_repo}/{training_model_name}\"\n",
    "training_model_checkpoint = f\"{training_model_id}\"\n",
    "training_model_path = f\"{training_model_repo}_{training_model_name}\"\n",
    "print(\n",
    "    f\"training_model_name: {training_model_name}, \"\n",
    "    f\"training_model_id: {training_model_id}, \"\n",
    "    f\"training_model_path: {training_model_path}\"\n",
    ")\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "actual_fraction = 0.95\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total/trainable params: 3533967360/33554432\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "# perf_model.gradient_checkpointing_enable()\n",
    "\n",
    "# param = peft_model.transformer.wte.weight\n",
    "# param.data = param.data.to(torch.float32)\n",
    "# print(param)\n",
    "\n",
    "# Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`\n",
    "peft_config = LoraConfig(\n",
    "    # r=512,\n",
    "    # r=128,\n",
    "    r=64,\n",
    "    # r=8,\n",
    "    # lora_alpha=256,\n",
    "    # lora_alpha=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    # lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules=\"all-linear\",\n",
    "    # target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"c_proj\"],\n",
    "    # target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    # target_modules=[\"c_attn\"],\n",
    "    # modules_to_save=[\"wte\", \"lm_head\"],\n",
    "    # modules_to_save=[\"wte\", \"wpe\", \"lm_head\"],\n",
    "    # llama2\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    ")\n",
    "print(peft_config)\n",
    "\n",
    "peft_model = get_peft_model(peft_model, peft_config)\n",
    "print(type(peft_model))\n",
    "print(peft_model)\n",
    "\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_model = model\n",
    "training_model = peft_model\n",
    "# print(type(training_model))\n",
    "# print(training_model.config)\n",
    "# print(training_model.generation_config)\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "# batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "# input_batch = batch.select(range(batch_size))\n",
    "# input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "# processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # print(eval_pred)\n",
    "    preds = eval_pred.predictions\n",
    "    # print(preds)\n",
    "    labels = eval_pred.label_ids\n",
    "    # print(labels)\n",
    "\n",
    "    # preds = np.argmax(preds, axis=-1)\n",
    "    # print(preds)\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    # print(preds)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # print(labels)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # print(decoded_preds)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # print(decoded_labels)\n",
    "\n",
    "    rouge_score = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    sacreblue_score = sacreblue_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # sari_score = sari_metric.compute(\n",
    "    #     sources=processed_samples[\"input\"],\n",
    "    #     predictions=processed_samples[\"processed\"],\n",
    "    #     references=processed_samples[\"references\"],\n",
    "    # )\n",
    "    em_score = em_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "\n",
    "    # report = {\n",
    "    #     \"metric\": 0,\n",
    "    # }\n",
    "    report = {\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"rougeLsum\": rouge_score[\"rougeLsum\"],\n",
    "        \"sacreblue\": sacreblue_score[\"score\"],\n",
    "        \"memory_used\": utilization[\"memory_used\"] / 1024**2,\n",
    "        \"cuda_allocated\": utilization[\"cuda_allocated\"] / 1024**2,\n",
    "        \"cuda_reserved\": utilization[\"cuda_reserved\"] / 1024**2,\n",
    "        \"ram_usage\": utilization[\"ram_usage\"] / 1024**2,\n",
    "        # \"sari.sari\": 0,\n",
    "        \"em\": em_score[\"exact_match\"],\n",
    "    }\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    report[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in report.items()}\n",
    "\n",
    "\n",
    "def logits_argmax(logits: torch.Tensor, labels):\n",
    "    # if isinstance(logits, tuple):\n",
    "    #     # Depending on the model and config, logits may contain extra tensors,\n",
    "    #     # like past_key_values, but logits always come first\n",
    "    #     logits = logits[0]\n",
    "    # return logits.argmax(dim=-1), labels\n",
    "    # logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# debug\n",
    "# trainer = Trainer(\n",
    "#     model=training_model,\n",
    "#     train_dataset=processed_dataset[\"train\"],\n",
    "#     eval_dataset=processed_dataset[\"test\"],\n",
    "#     # eval_dataset=processed_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     preprocess_logits_for_metrics=logits_argmax,\n",
    "#     args=args,\n",
    "# )\n",
    "\n",
    "# predictions = trainer.predict(processed_dataset[\"train\"].select(range(2)))\n",
    "# print(type(predictions))\n",
    "# # print(predictions)\n",
    "# print(predictions.predictions)\n",
    "# print(predictions.label_ids)\n",
    "# metrics = compute_metrics((predictions.predictions, predictions.label_ids))\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"model-{training_model_path}-train/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: model-iliazlobin_Llama-2-7b-chat-hf-mlabone-guanaco-1k-train\n",
      "train_size: 1000, batch_size: 10, per_epoch_steps: 25.0, max_steps: 100, epochs: 0.25\n",
      "train_size: 1000, batch_size: 10, per_epoch_steps: 25.0, epochs: 2, epoch_total_steps: 50.0\n"
     ]
    }
   ],
   "source": [
    "train_path = f\"model-{training_model_path}-train\"\n",
    "print(f\"train_path: {train_path}\")\n",
    "\n",
    "train_size = len(dataset)\n",
    "batch_size = 25\n",
    "gradient_accumulation_steps = 4\n",
    "eval_batch_size = 25\n",
    "eval_accumulation_steps = 4\n",
    "\n",
    "per_epoch_steps = train_size / (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "max_steps = 100\n",
    "epochs = per_epoch_steps / max_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, max_steps: {max_steps}, epochs: {epochs}\")\n",
    "\n",
    "# if epochs < 1:\n",
    "#     raise Exception(f\"Training doesn't cover the entire training dataset with {train_size} samples\")\n",
    "\n",
    "epochs = 2\n",
    "epoch_total_steps = epochs * per_epoch_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/training_args.py:1859: FutureWarning: `--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case iliazlobin/Llama-2-7b-chat-hf-mlabone-guanaco-1k).\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 05:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.425600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=1.5257812976837157, metrics={'train_runtime': 346.8385, 'train_samples_per_second': 5.766, 'train_steps_per_second': 0.144, 'total_flos': 3.541378420260864e+16, 'train_loss': 1.5257812976837157, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sft trainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=train_path,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    # warmup_ratio=0.05,\n",
    "    # optim=\"paged_adamw_8bit\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,\n",
    "    # bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    # per_device_eval_batch_size=eval_batch_size,\n",
    "    # eval_accumulation_steps=eval_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=max_steps,\n",
    "    warmup_steps=1,\n",
    "    logging_steps=5,\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=5,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=40,\n",
    "    # packing=False,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub_model_id=training_model_name,\n",
    "    push_to_hub_organization=\"iliazlobin\",\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # Should be false for Lora (https://github.com/kohya-ss/sd-scripts/issues/323#issuecomment-1485073421)\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=training_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    # max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")'\n",
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../.env\n",
    "\n",
    "!huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "# print(f\"saving locally: model-{training_model_path}\")\n",
    "# trainer.save_model(f\"model-{training_model_path}\")\n",
    "\n",
    "# print(f\"pushing to hub: {training_model_id}\")\n",
    "# trainer.push_to_hub(\n",
    "#     commit_message=\"complete: train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\",\n",
    "#     model_name=training_model_name,\n",
    "# )\n",
    "# trainer.push_to_hub(commit_message=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
      "You do not provide any explanation. You end your response right after the corrected sentence.\n",
      "<</SYS>>\n",
      "Write a paraphrase for the sentence: I don't think this is a dream. [/INST]\"\n",
      "{'input_ids': tensor([[    1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,  3492,\n",
      "         29915,   276,   263, 25437, 20255, 29889,   887,   925, 10683, 10240,\n",
      "         25260,   304,  1207,   963,  1959, 29889,    13,  3492,   437,   451,\n",
      "          3867,   738,  8252, 29889,   887,  1095,   596,  2933,  1492,  1156,\n",
      "           278, 24114, 10541, 29889,    13, 29966,   829, 14816, 29903,  6778,\n",
      "            13,  6113,   263,   610,   481,  1092,   559,   363,   278, 10541,\n",
      "         29901,   306,  1016, 29915, 29873,  1348,   445,   338,   263, 12561,\n",
      "         29889,   518, 29914, 25580, 18017]], device='cuda:0')}\n",
      "tensor([[    1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,  3492,\n",
      "         29915,   276,   263, 25437, 20255, 29889,   887,   925, 10683, 10240,\n",
      "         25260,   304,  1207,   963,  1959, 29889,    13,  3492,   437,   451,\n",
      "          3867,   738,  8252, 29889,   887,  1095,   596,  2933,  1492,  1156,\n",
      "           278, 24114, 10541, 29889,    13, 29966,   829, 14816, 29903,  6778,\n",
      "            13,  6113,   263,   610,   481,  1092,   559,   363,   278, 10541,\n",
      "         29901,   306,  1016, 29915, 29873,  1348,   445,   338,   263, 12561,\n",
      "         29889,   518, 29914, 25580, 18017, 29902, 29915, 29885,   306, 29915,\n",
      "         29881, 29914, 29902, 29908, 29902, 29915, 29885, 29908,    13,    13,\n",
      "         29908, 29902, 29915, 29881, 29908,   306, 29915, 29885,   306, 29908,\n",
      "         29881, 29908, 29881, 29908, 29902, 29915, 29902, 29915,   306, 29915,\n",
      "         29885,    13, 29902, 29915, 29902, 29915,   306, 29915, 29902, 29915,\n",
      "         29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915,\n",
      "         29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915,\n",
      "         29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915,\n",
      "         29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915,\n",
      "         29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915, 29902, 29915,\n",
      "         29902, 29915, 29902, 29915, 29902]], device='cuda:0')\n",
      "final_result: ['I\\'m I\\'d/I\"I\\'m\"\\n\\n\"I\\'d\" I\\'m I\"d\"d\"I\\'I\\' I\\'m\\nI\\'I\\' I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I\\'I']\n"
     ]
    }
   ],
   "source": [
    "single_input = \"\"\"<s>[INST] Tell me about yourself [/INST]\"\n",
    "\"\"\".strip()\n",
    "\n",
    "single_input = \"\"\"<s>[INST] <<SYS>>\n",
    "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
    "You do not provide any explanation. You end your response right after the corrected sentence.\n",
    "<</SYS>>\n",
    "Write a paraphrase for the sentence: I don't think this is a dream. [/INST]\"\n",
    "\"\"\".strip()\n",
    "\n",
    "batch_input = [\n",
    "    \"\"\"<s>[INST] <<SYS>>\n",
    "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
    "You do not provide any explanation. You end your response right after the corrected sentence.\n",
    "<</SYS>>\n",
    "Make the sentence simple: Hugo Wolf was born in Windischgra Ìˆ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
    "\"\"\".strip(),\n",
    "    \"\"\"<s>[INST] <<SYS>>\n",
    "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
    "You do not provide any explanation. You end your response right after the corrected sentence.\n",
    "<</SYS>>\n",
    "Make the sentence simple: Handzus ÌŒ played for the St. Louis Blues, Phoenix Coyotes, Philadelphia Flyers, Los Angeles Kings, San Jose Sharks and the Chicago Blackhawks, with whom he won the Stanley Cup with in 2013. [/INST]\n",
    "\"\"\".strip(),\n",
    "]\n",
    "\n",
    "print(single_input)\n",
    "# print(batch_input)\n",
    "\n",
    "# padding - https://huggingface.co/docs/transformers/en/pad_truncation\n",
    "# inputs = tokenizer(input, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "# inputs = tokenizer(batch_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "# inputs = tokenizer(batch_less_input, padding=True, truncation=True, max_length=100, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "# inputs = tokenizer(coedit_input_batch, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "\n",
    "inputs = tokenizer(single_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "# inputs = tokenizer(coedit_input, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "print(inputs)\n",
    "\n",
    "# outputs = model.generate(**inputs)\n",
    "outputs = model.generate(inputs.input_ids)\n",
    "print(outputs)\n",
    "\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "# print(result)\n",
    "\n",
    "final_result = outputs[:, inputs.input_ids.shape[1] :]\n",
    "final_result = tokenizer.batch_decode(final_result, skip_special_tokens=True)\n",
    "print(f\"final_result: {final_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is a large language model? [/INST] A/INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?INST?\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "2\n",
      "2\n",
      "left\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'llama2_request', 'llama2_prompt'],\n",
      "    num_rows: 7076\n",
      "})\n",
      "llama2_request: ['<s>[INST] <<SYS>> You are a grammar assistant. You just provide the rewritten corrected sentence in your output, that is. You do not provide any explanation. You do not prefix your sentence with any of your comment. You end your response right after the corrected sentence. <</SYS>> Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST]', '<s>[INST] <<SYS>> You are a grammar assistant. You just provide the rewritten corrected sentence in your output, that is. You do not provide any explanation. You do not prefix your sentence with any of your comment. You end your response right after the corrected sentence. <</SYS>> Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential. [/INST]']\n",
      "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.']\n",
      "result: ['For deserts to clean their land. For their deserts. Provide deserts to desertsides to sides. Provides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides to sides', 'As for. environmentally friendly. This is. As for. for. This. for. As. for. This. for. for. This. for. for. for. This. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for. for']\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "max_length = 350\n",
    "max_new_tokens = 100\n",
    "max_batch = 2\n",
    "\n",
    "print(dataset[\"train\"])\n",
    "\n",
    "batch_size = len(dataset[\"train\"]) if len(dataset[\"train\"]) < max_batch else max_batch\n",
    "input_batch = dataset[\"train\"].select(range(batch_size))\n",
    "# print(f\"task: {input_batch['task']}\")\n",
    "# print(f\"input: {input_batch['input']}\")\n",
    "# print(f\"request: {input_batch['request']}\")\n",
    "print(f\"llama2_request: {input_batch['llama2_request']}\")\n",
    "print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "inputs = tokenizer(input_batch[\"llama2_request\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "# print(inputs)\n",
    "\n",
    "model.config.use_cache = False\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=128, num_return_sequences=1)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    # max_length=max_length,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    # pad_token_id=tokenizer.eos_token_id,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=True,\n",
    "#     # max_length=max_length,\n",
    "#     max_new_tokens=max_new_tokens\n",
    "# )\n",
    "# print(outputs)\n",
    "\n",
    "trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "# result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=False)\n",
    "# result = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> paraphrase\n",
      "input: ['Reword this sentence: Item 5.1.2 shall be amended to read:', 'Reword this text: She stopped when she saw his expression.']\n",
      "reference: ['Point 5.1.2 is replaced by the following:', 'Seeing the look on his face, she paused.']\n",
      "result: [' Sure, here is the reworded sentence:\\n\\nItem 5.1.2 shall be amended to read:', '0 She stopped when she saw his expression.']\n",
      "\n",
      ">> clarity\n",
      "input: ['Clarify: This has been widely demonstrated for English using contextualized word representations such as OpenAI GPT (Radford et al., 2018 ), BERT (Devlin et al., 2019 ), or XLNet (Yang et al., 2019b).', 'Use clearer wording: We apply our French language models to complex NLP tasks (natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches.']\n",
      "reference: ['This has been widely demonstrated for English using contextualized word representations such as OpenAI GPT (Radford et al., 2018 ), BERT (Devlin et al., 2019; Yang et al., 2019b).', 'We apply our French language models to diverse NLP tasks (natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches.']\n",
      "result: [' Sure, here is the corrected sentence:\\n\\nThis has been widely demonstrated for English using contextualized word representations such as OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2019), or XLNet (Yang et al., 2019b).', 'revised sentence: We apply our French language models to complex NLP tasks (natural language inference, parsing, word sense disambiguation) and demonstrate that they typically outperform other pre-training approaches.']\n",
      "\n",
      ">> simplification\n",
      "input: [\"Simplify this text: For ourselves, we'll make a fresh one.\", 'Simplify this sentence: He wants to fuck one of us.']\n",
      "reference: ['We make a new one for ourselves.', 'One of us is about to get fucked.']\n",
      "result: [\" For ourselves, we'll make a fresh one.\", 'read \"He wants to have sex with one of us.\"']\n",
      "\n",
      ">> gec\n",
      "input: ['Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.']\n",
      "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.']\n",
      "result: [' Sure, here is the corrected sentence:\\n\\nCountries with a lot of deserts can terraform their deserts to increase their habitable land and use irrigation to provide clean water to the desert.', '0 As the population grows, the need for a habitable environment is unquestionably essential.']\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "max_batch = 2\n",
    "# max_length = 350\n",
    "# max_new_tokens = 350\n",
    "\n",
    "count = 0\n",
    "# for task, batch in test_dataset_dict.items():\n",
    "for task, batch in train_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"input: {input_batch['input']}\")\n",
    "    # print(f\"llama2_request: {input_batch['llama2_request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch[\"llama2_request\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "    # print(inputs)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    # model.config.pad_token_id = model.config.eos_token_id\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1\n",
    "    # )\n",
    "    outputs = model.generate(\n",
    "        **inputs, num_return_sequences=1\n",
    "    )\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=10,\n",
    "    #     num_return_sequences=1,\n",
    "    #     pad_token_id=tokenizer.eos_token_id,\n",
    "    #     # return_attention_mask=True,\n",
    "    #     max_length=256,\n",
    "    # )\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
