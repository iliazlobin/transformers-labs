{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml\n",
    "# %pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gemma-2b\"\n",
    "# model_name = \"gemma-7b-it\"\n",
    "# model_name = \"gemma-7b\"\n",
    "model_repo = f\"google\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gemma-7b,model_id: google/gemma-7b,model_path: google_gemma-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "2\n",
      "1\n",
      "0\n",
      "left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4b9e7a0c2844c2a558a243458873e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.modeling_gemma.GemmaForCausalLM'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
      "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skip\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"gemma-2b\"\n",
    "# model_name = \"gemma-7b-it\"\n",
    "model_name = \"gemma-7b\"\n",
    "model_repo = f\"google\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "# print(tokenizer.add_bos_token)\n",
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# model.generation_config.new_tokens = 350\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set {'neutralize', 'gec', 'clarity', 'coherence', 'simplification', 'paraphrase'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 705\n",
      "})\n",
      "test set {'neutralize', 'gec', 'clarity', 'coherence', 'simplification', 'paraphrase'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 74\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 705\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 74\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'], 'request': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'prompt': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "# print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "# train_ratio = 0.001\n",
    "# test_ratio = 0.0001\n",
    "train_ratio = 0.01\n",
    "test_ratio = 0.001\n",
    "# train_ratio = 0.1\n",
    "# test_ratio = 0.01\n",
    "# train_ratio = 0.9\n",
    "# test_ratio = 0.1\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "train_gec_dataset = gec_dataset.select(range(0, int(train_ratio * len(gec_dataset))))\n",
    "test_gec_dataset = gec_dataset.select(range(int((1 - test_ratio) * len(gec_dataset)), len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, int(train_ratio * len(simplification_dataset))))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(simplification_dataset)), len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, int(train_ratio * len(clarity_dataset))))\n",
    "test_clarity_dataset = clarity_dataset.select(range(int((1 - test_ratio) * len(clarity_dataset)), len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, int(train_ratio * len(coherence_dataset))))\n",
    "test_coherence_dataset = coherence_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(coherence_dataset)), len(coherence_dataset))\n",
    ")\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, int(train_ratio * len(paraphrase_dataset))))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(paraphrase_dataset)), len(paraphrase_dataset))\n",
    ")\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(train_ratio * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, int(train_ratio * len(neutralize_dataset))))\n",
    "test_neutralize_dataset = neutralize_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(neutralize_dataset)), len(neutralize_dataset))\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "print(train_dataset)\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "def add_prompt(item):\n",
    "    return {\n",
    "        \"request\": f\"{item['input']}\\nResponse:\",\n",
    "        \"prompt\": f\"{item['input']}\\nResponse:{item['reference']}\",\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "dataset = dataset.map(add_prompt)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_length train: 101\n",
      "max_input_length test: 150\n"
     ]
    }
   ],
   "source": [
    "# find the longest sequence in the dataset\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"train\"])\n",
    "print(f\"max_input_length train: {max_input_length}\")\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"test\"])\n",
    "print(f\"max_input_length test: {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/neutralize: 112\n",
      "train/gec: 203\n",
      "train/clarity: 12\n",
      "train/coherence: 106\n",
      "train/simplification: 114\n",
      "train/paraphrase: 158\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    train_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    train_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "train_dataset_map = {}\n",
    "for task, list in train_lists_map.items():\n",
    "    train_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(train_dataset_map)\n",
    "\n",
    "train_dataset_dict = DatasetDict(train_dataset_map)\n",
    "# print(train_dataset_dict)\n",
    "\n",
    "# for task, ds in train_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"train/{task}: {len(train_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/neutralize: 12\n",
      "test/gec: 21\n",
      "test/clarity: 2\n",
      "test/coherence: 11\n",
      "test/simplification: 12\n",
      "test/paraphrase: 16\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    test_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    test_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "test_dataset_map = {}\n",
    "for task, list in test_lists_map.items():\n",
    "    test_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(test_dataset_map)\n",
    "\n",
    "test_dataset_dict = DatasetDict(test_dataset_map)\n",
    "# print(test_dataset_dict)\n",
    "\n",
    "# for task, ds in test_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"test/{task}: {len(test_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "False\n",
      "True\n",
      "2\n",
      "1\n",
      "0\n",
      "left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a3d5345fc547b1bb0bea5f81a506c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb3b1cce0574797b0ef1608e3ba48e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 705\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 74\n",
      "    })\n",
      "})\n",
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
      "71 [13681, 832, 92868, 10266, 774, 736, 2793, 235292, 1699, 3287, 235269, 5605, 675, 476, 2940, 576, 105502, 798, 16743, 961, 1024, 21459, 577, 4740, 1024, 177711, 2840, 578, 2177, 34167, 577, 3658, 3903, 2003, 577, 573, 21459, 235265, 108, 3943, 235292, 1636, 3287, 235269, 5605, 675, 476, 2940, 576, 105502, 798, 9315, 1024, 21459, 577, 4740, 1024, 177711, 2840, 578, 1281, 34167, 577, 3658, 3903, 2003, 577, 573, 21459, 235265, 1]\n",
      "71 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "71 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1636, 3287, 235269, 5605, 675, 476, 2940, 576, 105502, 798, 9315, 1024, 21459, 577, 4740, 1024, 177711, 2840, 578, 1281, 34167, 577, 3658, 3903, 2003, 577, 573, 21459, 235265, 1]\n",
      "Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\n",
      "Response:As the number of people grows, the need for a habitable environment is unquestionably increasing.\n",
      "{'input_ids': tensor([[ 13681,    832,  92868,  10266,    774,    736,   2793, 235292,   1699,\n",
      "           3287, 235269,   5605,    675,    476,   2940,    576, 105502,    798,\n",
      "          16743,    961,   1024,  21459,    577,   4740,   1024, 177711,   2840,\n",
      "            578,   2177,  34167,    577,   3658,   3903,   2003,    577,    573,\n",
      "          21459, 235265,    108,   3943, 235292,   1636,   3287, 235269,   5605,\n",
      "            675,    476,   2940,    576, 105502,    798,   9315,   1024,  21459,\n",
      "            577,   4740,   1024, 177711,   2840,    578,   1281,  34167,    577,\n",
      "           3658,   3903,   2003,    577,    573,  21459, 235265,      1],\n",
      "        [     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,  90822,    573,  92868,    684, 235292,   1877,    573,\n",
      "           1758,    576,   1461,  26075, 235269,    573,   1476,    576, 177711,\n",
      "           4473,    603, 135072,   8727, 235265,    108,   3943, 235292,   2169,\n",
      "            573,   1758,    576,   1461,  26075, 235269,    573,   1476,    604,\n",
      "            476, 177711,   4473,    603, 135072,   8185, 235265,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   1636,   3287, 235269,   5605,\n",
      "            675,    476,   2940,    576, 105502,    798,   9315,   1024,  21459,\n",
      "            577,   4740,   1024, 177711,   2840,    578,   1281,  34167,    577,\n",
      "           3658,   3903,   2003,    577,    573,  21459, 235265,      1],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   2169,\n",
      "            573,   1758,    576,   1461,  26075, 235269,    573,   1476,    604,\n",
      "            476, 177711,   4473,    603, 135072,   8185, 235265,      1]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "# tokenizer.add_bos_token=True\n",
    "tokenizer.add_bos_token = False\n",
    "tokenizer.add_eos_token=True\n",
    "# tokenizer.add_eos_token=False\n",
    "print(tokenizer.add_bos_token)\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "max_length = 350\n",
    "max_new_tokens = 50\n",
    "\n",
    "# bos needs to be added\n",
    "# https://github.com/huggingface/transformers/issues/29250\n",
    "# https://github.com/EleutherAI/lm-evaluation-harness/issues/1471\n",
    "\n",
    "\n",
    "def process_dataset(batch):\n",
    "    model_inputs = tokenizer(batch[\"prompt\"], max_length=max_length)\n",
    "    model_reponses = tokenizer(batch[\"reference\"], max_length=max_length)\n",
    "\n",
    "    new_input_ids = []\n",
    "    new_labels = []\n",
    "    for input_ids, response_ids in zip(model_inputs.input_ids, model_reponses.input_ids):\n",
    "        # debug_labels = input_ids[-len(response_ids) :]\n",
    "        # print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "        # print(tokenizer.decode(debug_labels, skip_special_tokens=False))\n",
    "\n",
    "        num_tokens_ignore = len(input_ids) - len(response_ids)\n",
    "        labels = [-100] * (num_tokens_ignore ) + input_ids[-len(response_ids) :]\n",
    "        # labels.append(-100)\n",
    "        # labels.append(tokenizer.eos_token_id)\n",
    "        new_labels.append(labels)\n",
    "\n",
    "        # input_ids.append(tokenizer.eos_token_id)\n",
    "        # input_ids = [tokenizer.bos_token_id] + input_ids\n",
    "        new_input_ids.append(input_ids)\n",
    "\n",
    "    new_attention_mask = []\n",
    "    for attention_mask in model_inputs.attention_mask:\n",
    "        # attention_mask.append(0)\n",
    "        # attention_mask.append(1)\n",
    "        # attention_mask = [0] + attention_mask\n",
    "        new_attention_mask.append(attention_mask)\n",
    "\n",
    "    # labels = tokenizer(text_target=examples[\"tgt\"], max_length=1024, padding=True).input_ids\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs[\"input_ids\"] = new_input_ids\n",
    "    model_inputs[\"attention_mask\"] = new_attention_mask\n",
    "    model_inputs[\"labels\"] = new_labels\n",
    "\n",
    "    # print(\n",
    "    #     f\">> input_ids: {len(model_inputs['input_ids'])},\"\n",
    "    #     \"attention_mask: {len(model_inputs['attention_mask'])},\"\n",
    "    #     \"labels: {len(model_inputs['labels'])}\"\n",
    "    # )\n",
    "    # count = 0\n",
    "    # for input_ids, labels, attention_masks in zip(new_input_ids, new_labels, new_attention_mask):\n",
    "    #     count += 1\n",
    "    #     if count > 3:\n",
    "    #         break\n",
    "    #     print(f\">> input_ids: {len(input_ids)}, attention_mask: {len(attention_masks)}, labels: {len(labels)}\")\n",
    "    #     print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "    #     print(labels)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# processed_dataset = data.map(preprocess_function, batched=True, batch_size=10, remove_columns=data[\"train\"].column_names)\n",
    "processed_dataset = dataset.map(\n",
    "    process_dataset, batched=True, batch_size=10, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "# processed_dataset = dataset.map(process_dataset, batched=True, batch_size=10)\n",
    "print(processed_dataset)\n",
    "print(dataset[\"train\"][0][\"prompt\"])\n",
    "# print(dataset[\"train\"][0][\"reference\"])\n",
    "print(len(processed_dataset[\"train\"][\"input_ids\"][0]), processed_dataset[\"train\"][\"input_ids\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"attention_mask\"][0]), processed_dataset[\"train\"][\"attention_mask\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"labels\"][0]), processed_dataset[\"train\"][\"labels\"][0])\n",
    "print(dataset[\"train\"][1][\"prompt\"])\n",
    "# print(dataset[\"train\"][1][\"reference\"])\n",
    "# print(dataset[\"train\"][2][\"prompt\"])\n",
    "# print(dataset[\"train\"][2][\"reference\"])\n",
    "\n",
    "# print(len(processed_dataset[\"test\"][\"input_ids\"][0]), processed_dataset[\"test\"][\"input_ids\"][0])\n",
    "# print(len(processed_dataset[\"test\"][\"attention_mask\"][0]), processed_dataset[\"test\"][\"attention_mask\"][0])\n",
    "# print(len(processed_dataset[\"test\"][\"labels\"][0]), processed_dataset[\"test\"][\"labels\"][0])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", model=model)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "dataloader = DataLoader(processed_dataset[\"train\"], batch_size=2, collate_fn=data_collator)\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "max_batch = 2\n",
    "max_length = 350\n",
    "max_new_tokens = 50\n",
    "\n",
    "print(type(model))\n",
    "print(model.generation_config.max_new_tokens)\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    # print(f\"input: {input_batch['input']}\")\n",
    "    # print(f\"prompt: {input_batch['prompt']}\")\n",
    "    print(f\"request: {input_batch['request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch[\"request\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    # outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    outputs = model.generate(inputs.input_ids, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    # result = tokenizer.batch_decode(trimmed_output, max_length=max_length, skip_special_tokens=True)\n",
    "    result = tokenizer.batch_decode(trimmed_output, max_length=max_length, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    # processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(f\"result: {processed}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_model_name: gemma-7b-lora64-coedit, training_model_id: iliazlobin/gemma-7b-lora64-coedit, training_model_path: iliazlobin_gemma-7b-lora64-coedit\n",
      "total/used/cuda/res/ram (Gb): 79.15/33.07/31.81/31.81/4.10\n",
      "total/used/available memory (Gb): 79.15/33.07/46.08\n",
      "recommended/actual fraction: 0.58/0.95\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "training_model_repo = f\"iliazlobin\"\n",
    "training_model_name: str = f\"{model_name}-lora64-coedit\"\n",
    "training_model_id = f\"{training_model_repo}/{training_model_name}\"\n",
    "training_model_checkpoint = f\"{training_model_id}\"\n",
    "training_model_path = f\"{training_model_repo}_{training_model_name}\"\n",
    "print(\n",
    "    f\"training_model_name: {training_model_name}, \"\n",
    "    f\"training_model_id: {training_model_id}, \"\n",
    "    f\"training_model_path: {training_model_path}\"\n",
    ")\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "actual_fraction = 0.95\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GemmaForCausalLM(\n",
      "      (model): GemmaModel(\n",
      "        (embed_tokens): ModulesToSaveWrapper(\n",
      "          (original_module): Embedding(256000, 3072, padding_idx=0)\n",
      "          (modules_to_save): ModuleDict(\n",
      "            (default): Embedding(256000, 3072, padding_idx=0)\n",
      "          )\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x GemmaDecoderLayer(\n",
      "            (self_attn): GemmaSdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): GemmaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): GemmaMLP(\n",
      "              (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
      "              (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
      "              (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
      "              (act_fn): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): GemmaRMSNorm()\n",
      "            (post_attention_layernorm): GemmaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): GemmaRMSNorm()\n",
      "      )\n",
      "      (lm_head): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=3072, out_features=256000, bias=False)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=3072, out_features=256000, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total/trainable params: 10161925120/1624244224\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_model = model\n",
    "peft_model = prepare_model_for_kbit_training(peft_model, use_gradient_checkpointing=True)\n",
    "# perf_model.gradient_checkpointing_enable()\n",
    "\n",
    "config = LoraConfig(\n",
    "    # r=512,\n",
    "    # r=128,\n",
    "    r=64,\n",
    "    # r=4,\n",
    "    # lora_alpha=256,\n",
    "    lora_alpha=16,\n",
    "    # lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    # lora_dropout=0.005,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules=[\"q_proj\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(peft_model, config)\n",
    "print(type(peft_model))\n",
    "print(peft_model)\n",
    "\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total/trainable params: 10161925120/1624244224\n"
     ]
    }
   ],
   "source": [
    "# training_model = model\n",
    "training_model = peft_model\n",
    "# print(type(training_model))\n",
    "# print(training_model.config)\n",
    "# print(training_model.generation_config)\n",
    "\n",
    "total_params = sum(p.numel() for p in training_model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in training_model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "# batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "# input_batch = batch.select(range(batch_size))\n",
    "# input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "# processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "max_length = 350\n",
    "max_new_tokens = 50\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # print(eval_pred)\n",
    "    preds = eval_pred.predictions\n",
    "    # print(preds)\n",
    "    labels = eval_pred.label_ids\n",
    "    # print(labels)\n",
    "\n",
    "    # preds = np.argmax(preds, axis=-1)\n",
    "    # print(preds)\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    # print(preds[0])\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # print(labels)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, max_length=max_length, skip_special_tokens=True)\n",
    "    # print(decoded_preds[0])\n",
    "    decoded_labels = tokenizer.batch_decode(labels, max_length=max_length, skip_special_tokens=True)\n",
    "    # print(decoded_labels)\n",
    "\n",
    "    rouge_score = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    sacreblue_score = sacreblue_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # sari_score = sari_metric.compute(\n",
    "    #     sources=processed_samples[\"input\"],\n",
    "    #     predictions=processed_samples[\"processed\"],\n",
    "    #     references=processed_samples[\"references\"],\n",
    "    # )\n",
    "    em_score = em_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "\n",
    "    # report = {\n",
    "    #     \"metric\": 0,\n",
    "    # }\n",
    "    report = {\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"rougeLsum\": rouge_score[\"rougeLsum\"],\n",
    "        \"sacreblue\": sacreblue_score[\"score\"],\n",
    "        \"memory_used\": utilization[\"memory_used\"] / 1024**2,\n",
    "        \"cuda_allocated\": utilization[\"cuda_allocated\"] / 1024**2,\n",
    "        \"cuda_reserved\": utilization[\"cuda_reserved\"] / 1024**2,\n",
    "        \"ram_usage\": utilization[\"ram_usage\"] / 1024**2,\n",
    "        # \"sari.sari\": 0,\n",
    "        \"em\": em_score[\"exact_match\"],\n",
    "    }\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    report[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in report.items()}\n",
    "\n",
    "\n",
    "def logits_argmax(logits: torch.Tensor, labels):\n",
    "    # if isinstance(logits, tuple):\n",
    "    #     # Depending on the model and config, logits may contain extra tensors,\n",
    "    #     # like past_key_values, but logits always come first\n",
    "    #     logits = logits[0]\n",
    "    # return logits.argmax(dim=-1), labels\n",
    "    # logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# debug\n",
    "# trainer = Trainer(\n",
    "#     model=training_model,\n",
    "#     train_dataset=processed_dataset[\"train\"],\n",
    "#     eval_dataset=processed_dataset[\"test\"],\n",
    "#     # eval_dataset=processed_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     preprocess_logits_for_metrics=logits_argmax,\n",
    "#     args=args,\n",
    "# )\n",
    "\n",
    "# print(processed_dataset[\"train\"][0][\"input_ids\"])\n",
    "# print(processed_dataset[\"train\"][0][\"attention_mask\"])\n",
    "# print(processed_dataset[\"train\"][0][\"labels\"])\n",
    "# print(tokenizer.batch_decode([processed_dataset[\"train\"][0][\"input_ids\"]], skip_special_tokens=True))\n",
    "# predictions = trainer.predict(processed_dataset[\"train\"].select(range(1)))\n",
    "# # print(type(predictions))\n",
    "# # print(predictions)\n",
    "# # print(predictions.predictions)\n",
    "# # print(predictions.label_ids)\n",
    "# # metrics = compute_metrics((predictions.predictions, predictions.label_ids))\n",
    "# # eval_pred = (\n",
    "# #     predictions= predictions.predictions,\n",
    "# #     label_ids= predictions.label_ids,\n",
    "# # )\n",
    "\n",
    "# from collections import namedtuple\n",
    "# EvalPred = namedtuple('EvalPred', ['predictions', 'label_ids'])\n",
    "# eval_pred = EvalPred(predictions=predictions.predictions, label_ids=predictions.label_ids)\n",
    "\n",
    "# metrics = compute_metrics(eval_pred)\n",
    "# # print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"model-{training_model_path}-train/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: model-iliazlobin_gemma-7b-lora64-coedit-train\n",
      "train_size: 705, batch_size: 10, per_epoch_steps: 17.625, max_steps: 100, epochs: 0.17625\n",
      "train_size: 705, batch_size: 10, per_epoch_steps: 17.625, epochs: 2, epoch_total_steps: 35.25\n"
     ]
    }
   ],
   "source": [
    "train_path = f\"model-{training_model_path}-train\"\n",
    "print(f\"train_path: {train_path}\")\n",
    "\n",
    "train_size = len(processed_dataset[\"train\"])\n",
    "batch_size = 10\n",
    "gradient_accumulation_steps = 4\n",
    "eval_batch_size = 20\n",
    "eval_accumulation_steps = 4\n",
    "\n",
    "per_epoch_steps = train_size / (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "max_steps = 100\n",
    "epochs = per_epoch_steps / max_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, max_steps: {max_steps}, epochs: {epochs}\")\n",
    "\n",
    "# if epochs < 1:\n",
    "#     raise Exception(f\"Training doesn't cover the entire training dataset with {train_size} samples\")\n",
    "\n",
    "epochs = 2\n",
    "epoch_total_steps = epochs * per_epoch_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/training_args.py:1859: FutureWarning: `--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case iliazlobin/gemma-7b-lora64-coedit).\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 01:03, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.624800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34, training_loss=2.6334788238300995, metrics={'train_runtime': 65.2914, 'train_samples_per_second': 21.595, 'train_steps_per_second': 0.521, 'total_flos': 6350064667238400.0, 'train_loss': 2.6334788238300995, 'epoch': 1.92})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer, TrainingArguments\n",
    "\n",
    "# WEIGHT_DECAY\n",
    "# As a rule of thumb, the more training examples you have, the weaker this term should be.\n",
    "# The more parameters you have (i.e., deeper net, larger filters, larger InnerProduct layers etc.) the higher this term should be.\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=train_path,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    # warmup_ratio=0.05,\n",
    "    # optim=\"paged_adamw_8bit\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    # per_device_eval_batch_size=eval_batch_size,\n",
    "    # eval_accumulation_steps=eval_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=max_steps,\n",
    "    warmup_steps=1,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=5,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub_model_id=training_model_name,\n",
    "    push_to_hub_organization=\"iliazlobin\",\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # Should be false for Lora (https://github.com/kohya-ss/sd-scripts/issues/323#issuecomment-1485073421)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=training_model,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    # eval_dataset=processed_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=logits_argmax,\n",
    "    args=args,\n",
    ")\n",
    "print(type(trainer))\n",
    "\n",
    "model.config.use_cache = False\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")'\n",
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../.env\n",
    "\n",
    "!huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "# print(f\"saving locally: model-{training_model_path}\")\n",
    "# trainer.save_model(f\"model-{training_model_path}\")\n",
    "\n",
    "# print(f\"pushing to hub: {training_model_id}\")\n",
    "# trainer.push_to_hub(\n",
    "#     commit_message=\"complete: train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\",\n",
    "#     model_name=training_model_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"add_bos_token\": true,\n",
      "  \"add_eos_token\": true,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "    num_rows: 705\n",
      "})\n",
      "request: ['Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\\nResponse:', 'Improve the grammaticality of this sentence: Besides some technologically determinists that allow the development of biometric identification, this technology is also shaped by three social factors, namely, the desire of the society for safety, convenience and economy.\\nResponse:']\n",
      "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.', 'Besides some technological determinists that allow the development of biometric identification, this technology is also shaped by three social factors, namely, the desire of society for safety, convenience, and economy.']\n",
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "False\n",
      "False\n",
      "2\n",
      "1\n",
      "0\n",
      "left\n",
      "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      0,  13681,\n",
      "            832,  92868,  10266,    774,    736,   2793, 235292,   1699,   3287,\n",
      "         235269,   5605,    675,    476,   2940,    576, 105502,    798,  16743,\n",
      "            961,   1024,  21459,    577,   4740,   1024, 177711,   2840,    578,\n",
      "           2177,  34167,    577,   3658,   3903,   2003,    577,    573,  21459,\n",
      "         235265,    108,   3943, 235292],\n",
      "        [     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,  90822,    573,\n",
      "          92868,    684, 235292,   1877,    573,   1758,    576,   1461,  26075,\n",
      "         235269,    573,   1476,    576, 177711,   4473,    603, 135072,   8727,\n",
      "         235265,    108,   3943, 235292],\n",
      "        [ 90822,    573,  92868,    684,    576,    736,  13060, 235292,  28842,\n",
      "           1009, 174192,   3172,   3767, 235256,    674,   2765,    573,   3505,\n",
      "            576, 170741,  17971, 235269,    736,   6178,    603,   1170,  24048,\n",
      "            731,   2149,   3127,   7549, 235269,  24435, 235269,    573,  12005,\n",
      "            576,    573,   8059,    604,   6514, 235269,  26343,    578,   9797,\n",
      "         235265,    108,   3943, 235292]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n",
      "tensor([[     0,      0,      0,      0,      0,      0,      0,      0,  13681,\n",
      "            832,  92868,  10266,    774,    736,   2793, 235292,   1699,   3287,\n",
      "         235269,   5605,    675,    476,   2940,    576, 105502,    798,  16743,\n",
      "            961,   1024,  21459,    577,   4740,   1024, 177711,   2840,    578,\n",
      "           2177,  34167,    577,   3658,   3903,   2003,    577,    573,  21459,\n",
      "         235265,    108,   3943, 235292,   1636,   3287, 235269,   5605,    675,\n",
      "            476,  21459,    577,   3658,   3903,   2003,    577,   1024,  21459,\n",
      "         235265,      1,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,  90822,    573,\n",
      "          92868,    684, 235292,   1877,    573,   1758,    576,   1461,  26075,\n",
      "         235269,    573,   1476,    576, 177711,   4473,    603, 135072,   8727,\n",
      "         235265,    108,   3943, 235292,   2169,    573,   1758,    576,   1461,\n",
      "         235269,    573,   1758,    576,   1461,    603, 135072, 195939, 195939,\n",
      "         195939, 195939, 195939, 195939, 195939, 195939, 195939, 195939, 195939,\n",
      "         195939, 195939, 195939, 195939, 195939, 195939, 195939, 195939, 195939,\n",
      "         195939, 195939, 195939, 195939, 195939, 195939, 195939, 195939, 195939,\n",
      "         195939, 195939, 195939, 195939, 195939, 195939, 195939, 195939, 195939],\n",
      "        [ 90822,    573,  92868,    684,    576,    736,  13060, 235292,  28842,\n",
      "           1009, 174192,   3172,   3767, 235256,    674,   2765,    573,   3505,\n",
      "            576, 170741,  17971, 235269,    736,   6178,    603,   1170,  24048,\n",
      "            731,   2149,   3127,   7549, 235269,  24435, 235269,    573,  12005,\n",
      "            576,    573,   8059,    604,   6514, 235269,  26343,    578,   9797,\n",
      "         235265,    108,   3943, 235292,  49171,    573,   8059,    604,    948,\n",
      "           8563,    573,   8059,    577,    614,  24048,    731,   2149,   3127,\n",
      "           7549, 235265,      1,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0]],\n",
      "       device='cuda:0')\n",
      "result: ['For example, countries with a desert to provide clean water to their desert.', 'As the number of people, the number of people is unquestionably dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises dises', 'Besides the society for which allows the society to be shaped by three social factors.']\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "model.generation_config.add_bos_token = True\n",
    "model.generation_config.add_eos_token = True\n",
    "print(model.generation_config)\n",
    "\n",
    "max_length = 350\n",
    "max_new_tokens = 50\n",
    "max_batch = 3\n",
    "\n",
    "print(dataset[\"train\"])\n",
    "\n",
    "batch_size = len(dataset[\"train\"]) if len(dataset[\"train\"]) < max_batch else max_batch\n",
    "input_batch = dataset[\"train\"].select(range(batch_size))\n",
    "# print(f\"task: {input_batch['task']}\")\n",
    "# print(f\"input: {input_batch['input']}\")\n",
    "print(f\"request: {input_batch['request']}\")\n",
    "print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# tokenizer.add_bos_token=True\n",
    "tokenizer.add_bos_token=False\n",
    "# tokenizer.add_eos_token=True\n",
    "tokenizer.add_eos_token=False\n",
    "print(tokenizer.add_bos_token)\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "# max_length = 350\n",
    "# model_inputs = tokenizer(dataset[\"train\"][0][\"request\"], max_length=max_length)\n",
    "# print(model_inputs)\n",
    "\n",
    "inputs = tokenizer(input_batch[\"request\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "print(inputs)\n",
    "\n",
    "model.config.use_cache = False\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=128, num_return_sequences=1)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    # max_length=max_length,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    # pad_token_id=tokenizer.eos_token_id,\n",
    "    # bos_token_id=tokenizer.bos_token_id,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "# # outputs = model.generate(\n",
    "# #     **inputs,\n",
    "# #     do_sample=True,\n",
    "# #     top_k=10,\n",
    "# #     num_return_sequences=1,\n",
    "# #     bos_token_id=tokenizer.bos_token_id,\n",
    "# #     eos_token_id=tokenizer.eos_token_id,\n",
    "# #     pad_token_id=tokenizer.pad_token_id,\n",
    "# #     # max_length=max_length,\n",
    "# #     max_new_tokens=max_new_tokens\n",
    "# # )\n",
    "print(outputs)\n",
    "\n",
    "trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "# result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=False)\n",
    "# result = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "# result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.<eos>\n",
      "<bos>Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response:\n",
      "For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.<eos>\n",
      "As the number of people grows, the need for a habitable environment is unquestionably increasing.<eos>\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "sequence = [     2,  13681,    832,  92868,  10266,    774,    736,   2793, 235292,\n",
    "           1699,   3287, 235269,   5605,    675,    476,   2940,    576, 105502,\n",
    "            798,  16743,    961,   1024,  21459,    577,   4740,   1024, 177711,\n",
    "           2840,    578,   2177,  34167,    577,   3658,   3903,   2003,    577,\n",
    "            573,  21459, 235265,    108,   3943, 235292,   1636,   3287, 235269,\n",
    "           5605,    675,    476,   2940,    576, 105502,    798,   9315,   1024,\n",
    "          21459,    577,   4740,   1024, 177711,   2840,    578,   1281,  34167,\n",
    "            577,   3658,   3903,   2003,    577,    573,  21459, 235265,      1]\n",
    "print(tokenizer.decode(sequence))\n",
    "# input\n",
    "sequence = [     2,  13681,    832,  92868,  10266,    774,    736,   2793, 235292,\n",
    "           1699,   3287, 235269,   5605,    675,    476,   2940,    576, 105502,\n",
    "            798,  16743,    961,   1024,  21459,    577,   4740,   1024, 177711,\n",
    "           2840,    578,   2177,  34167,    577,   3658,   3903,   2003,    577,\n",
    "            573,  21459, 235265,    108,   3943, 235292]\n",
    "print(tokenizer.decode(sequence))\n",
    "# # output\n",
    "# sequence = [     2,  13681,    832,  92868,  10266,    774,    736,   2793, 235292,\n",
    "#            1699,   3287, 235269,   5605,    675,    476,   2940,    576, 105502,\n",
    "#             798,  16743,    961,   1024,  21459,    577,   4740,   1024, 177711,\n",
    "#            2840,    578,   2177,  34167,    577,   3658,   3903,   2003,    577,\n",
    "#             573,  21459, 235265,    108,   3943, 235292,   1636, 146392, 217512]\n",
    "# print(tokenizer.decode(sequence))\n",
    "# labels\n",
    "sequence = [1636,   3287, 235269,\n",
    "           5605,    675,    476,   2940,    576, 105502,    798,   9315,   1024,\n",
    "          21459,    577,   4740,   1024, 177711,   2840,    578,   1281,  34167,\n",
    "            577,   3658,   3903,   2003,    577,    573,  21459, 235265,      1]\n",
    "print(tokenizer.decode(sequence))\n",
    "sequence = [2169,    573,   1758,    576,   1461,  26075, 235269,    573,   1476,\n",
    "            604,    476, 177711,   4473,    603, 135072,   8185, 235265,      1]\n",
    "print(tokenizer.decode(sequence))\n",
    "# sequence = [27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
    "#           1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
    "#            430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
    "#           1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
    "#            198, 31077,    25,   220]\n",
    "# print(tokenizer.decode(token_ids=sequence))\n",
    "# sequence = [25, 220, 25]\n",
    "# print(tokenizer.decode(sequence))\n",
    "# sequence = [1114]\n",
    "# print(tokenizer.decode(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> neutralize\n",
      "request: ['Make this text more neutral: chloroform \"the molecular lifesaver\" an article at oxford university providing interesting facts about chloroform.\\nResponse:', 'Remove points of view: gaming system, an dice pool system where matched die results determine success.\\nResponse:']\n",
      "reference: ['chloroform \"the molecular lifesaver\" an article at oxford university providing facts about chloroform.', 'gaming system, a unique dice pool system where matched die results determine success.']\n",
      "result: ['', '']\n",
      "\n",
      ">> coherence\n",
      "request: ['Fix coherence in this sentence: Guy Leech is still a celebrity today. Guy Leech regularly appears on television as an advocate for health and fitness.\\nResponse:', 'Fix coherence in this text: Injury, bad luck and inconsistency prevented him from ever winning the series. He was certainly good enough.\\nResponse:']\n",
      "reference: ['Guy Leech is still a celebrity today and regularly appears on television as an advocate for health and fitness.', 'Injury, bad luck and inconsistency prevented him from ever winning the series, although he was certainly good enough.']\n",
      "result: ['', '']\n",
      "\n",
      ">> clarity\n",
      "request: ['Clarify: This has been widely demonstrated for English using contextualized word representations such as OpenAI GPT (Radford et al., 2018 ), BERT (Devlin et al., 2019 ), or XLNet (Yang et al., 2019b).\\nResponse:', 'Use clearer wording: We apply our French language models to complex NLP tasks (natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches.\\nResponse:']\n",
      "reference: ['This has been widely demonstrated for English using contextualized word representations such as OpenAI GPT (Radford et al., 2018 ), BERT (Devlin et al., 2019; Yang et al., 2019b).', 'We apply our French language models to diverse NLP tasks (natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches.']\n",
      "result: ['', '']\n",
      "\n",
      ">> simplification\n",
      "request: [\"Simplify this text: For ourselves, we'll make a fresh one.\\nResponse:\", 'Simplify this sentence: He wants to fuck one of us.\\nResponse:']\n",
      "reference: ['We make a new one for ourselves.', 'One of us is about to get fucked.']\n",
      "result: ['', '']\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "max_batch = 2\n",
    "max_length = 350\n",
    "max_new_tokens = 350\n",
    "\n",
    "count = 0\n",
    "# for task, batch in test_dataset_dict.items():\n",
    "for task, batch in train_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"request: {input_batch['request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch[\"request\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "    # print(inputs)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    # model.config.pad_token_id = model.config.eos_token_id\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs, max_new_tokens=max_new_tokens, num_return_sequences=1\n",
    "    # )\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        # pad_token_id=tokenizer.eos_token_id,\n",
    "        # return_attention_mask=True,\n",
    "        # max_length=256,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
