{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trl.trainer.sft_trainer.SFTTrainer'>\n"
     ]
    }
   ],
   "source": [
    "# from trl import SFTTrainer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(SFTTrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training llama-2 on grammarly-coedit dataset\n",
    "* https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "* https://huggingface.co/docs/transformers/en/model_doc/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.utils.quantization_config.BitsAndBytesConfig'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d190b2b3a6664e7aa9a756055c543448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"./model-llama-4bits-coedit\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_name = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import BitsAndBytesConfig, GPTQConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print(type(quantization_config))\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name,\n",
    "    padding_side=\"left\",\n",
    "    # model_max_length=512,\n",
    "    add_eos_token=True,\n",
    "    # add_bos_token=True,\n",
    "    # padding='longest',\n",
    "    # use_fast=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# input_text = \"\"\"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs\n",
    "# Response:\n",
    "# \"\"\".strip()\n",
    "# tokens = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "# print(tokens.input_ids)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(model.config)\n",
    "\n",
    "# from auto_gptq import exllama_set_max_input_length\n",
    "# model = exllama_set_max_input_length(model, max_input_length=2400)\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an original English text. Correct English Grammar and spelling mistakes in the text.\n",
      "\n",
      "### Input:\n",
      "Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs\n",
      "\n",
      "### Response:\n",
      "Fix grammar: In the case of young people the best way is to study as hard as possible to get better grades so in the future they will have better chances to find better jobs.\n",
      "\n",
      "### Explanation:\n",
      "In the case of young people the best way is to study as hard as possible to get better grades so in the future they will have better chances to find better jobs.\n",
      "\n",
      "### Input:\n",
      "Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find\n"
     ]
    }
   ],
   "source": [
    "# input_text = \"Fix grammatical errors in this sentence: I goes to school every day.\"\n",
    "# input_text = \"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\"\n",
    "input_text = \"\"\"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs\n",
    "Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "tokens = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**tokens, max_new_tokens=128)\n",
    "# outputs = model.generate(\n",
    "#     **input_ids,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=False,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "2\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 69071\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1712\n",
      "    })\n",
      "})\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_data = load_dataset(\"grammarly/coedit\", split=\"train[:100]\")\n",
    "test_data = load_dataset(\"grammarly/coedit\", split=\"train[:10]\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_data, \"test\": test_data})\n",
    "print(dataset)\n",
    "print(len(dataset))\n",
    "\n",
    "full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "print(full_dataset)\n",
    "print(len(full_dataset))\n",
    "\n",
    "# data[\"test\"] = test_data\n",
    "# data = load_dataset(\"grammarly/coedit\")\n",
    "# data[\"test\"] = data[\"validation\"]\n",
    "# del data[\"validation\"]\n",
    "\n",
    "# print(len(train_data))\n",
    "# print(len(data))\n",
    "\n",
    "# print(data)\n",
    "# print(data[\"train\"])\n",
    "# print(data[\"train\"][0]['src'])\n",
    "# print(data[\"test\"])\n",
    "# print(data[\"test\"][0]['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832a844904ea487a922692742c8a6072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d955fa1dfbb548baa0ec4cb596306fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src', 'tgt', 'text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src', 'tgt', 'text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "2\n",
      "### Instruction: Below is an original English text. Correct English Grammar and spelling mistakes in the text.\n",
      "\n",
      "\n",
      "### Input:\n",
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "\n",
      "\n",
      "### Response:\n",
      "For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
      "### Instruction: Below is an original English text. Correct English Grammar and spelling mistakes in the text.\n",
      "\n",
      "\n",
      "### Input:\n",
      "Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\n",
      "\n",
      "\n",
      "### Response:\n",
      "As the number of people grows, the need for a habitable environment is unquestionably increasing.\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Below is an original English text. Correct English Grammar and spelling mistakes in the text.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(input: str, response: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "\n",
    "### Input:\n",
    "{input.strip()}\n",
    "\n",
    "\n",
    "### Response:\n",
    "{response}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def process_data(item):\n",
    "    return {\n",
    "        \"text\": generate_prompt(item[\"src\"], item[\"tgt\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "text_dataset = dataset.map(process_data, remove_columns=[\"task\", \"_id\"])\n",
    "print(text_dataset)\n",
    "print(len(text_dataset))\n",
    "print(text_dataset[\"train\"][0][\"text\"])\n",
    "print(text_dataset[\"train\"][1][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e07a7f36065432eb6e2a2951a9f61d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5696fa685ecd421fbd0010cd181bfd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "[1, 835, 2799, 4080, 29901, 13866, 338, 385, 2441, 4223, 1426, 29889, 28518, 4223, 16878, 3034, 322, 805, 7807, 28947, 297, 278, 1426, 29889, 13, 13, 13, 2277, 29937, 10567, 29901, 13, 15941, 599, 14961, 2922, 936, 4436, 515, 445, 1426, 29901, 1152, 1342, 29892, 10916, 411, 263, 3287, 310, 18197, 29879, 508, 15087, 689, 1009, 18197, 304, 7910, 1009, 4760, 519, 2982, 322, 773, 3805, 8966, 362, 304, 3867, 5941, 4094, 304, 278, 18197, 29889, 13, 13, 13, 2277, 29937, 13291, 29901, 13, 2831, 1342, 29892, 10916, 411, 263, 3287, 310, 18197, 29879, 508, 4327, 1009, 18197, 304, 7910, 1009, 4760, 519, 2982, 322, 671, 3805, 8966, 362, 304, 3867, 5941, 4094, 304, 278, 18197, 29889, 2]\n",
      "[1, 835, 2799, 4080, 29901, 13866, 338, 385, 2441, 4223, 1426, 29889, 28518, 4223, 16878, 3034, 322, 805, 7807, 28947, 297, 278, 1426, 29889, 13, 13, 13, 2277, 29937, 10567, 29901, 13, 1888, 771, 345, 278, 14961, 2922, 936, 537, 29901, 1094, 278, 1353, 310, 2305, 25088, 29892, 278, 817, 310, 4760, 519, 5177, 338, 443, 12470, 2197, 18853, 29889, 13, 13, 13, 2277, 29937, 13291, 29901, 13, 2887, 278, 1353, 310, 2305, 25088, 29892, 278, 817, 363, 263, 4760, 519, 5177, 338, 443, 12470, 2197, 10231, 29889, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"])\n",
    "    # labels = tokenizer(text_target=examples[\"tgt\"], max_length=1024, padding=True).input_ids\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# processed_data = data.map(preprocess_function, batched=True, batch_size=10, remove_columns=data[\"train\"].column_names)\n",
    "processed_dataset = text_dataset.map(\n",
    "    preprocess_function, batched=True, batch_size=100, remove_columns=text_dataset[\"train\"].column_names\n",
    ")\n",
    "print(processed_dataset)\n",
    "print(processed_dataset[\"train\"][0][\"input_ids\"])\n",
    "print(processed_dataset[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total/used/cuda/res/ram (Gb): 10.00/3.51/0.00/0.00/23.28\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "# available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "# recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "# actual_fraction = 0.90\n",
    "# torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "# print(\n",
    "#     f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f} | \"\n",
    "#     f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\"\n",
    "# )\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.empty(utilization['total_memory'] // 2, dtype=torch.int8, device='cuda')\n",
    "# print_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8388608 || all params: 3508801536 || trainable%: 0.23907331075678143\n",
      "total/used/cuda/res/ram (Gb): 10.00/7.62/4.24/5.33/14.40\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "perf_model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(perf_model)\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.training_args.TrainingArguments'>\n",
      "<class 'transformers.trainer.Trainer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#     result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "#     result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "#     return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    return {\"k\": 2}\n",
    "\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model-llama-4bits-train\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=20,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=0.2,\n",
    "    warmup_ratio=0.05,\n",
    "    save_strategy=\"epoch\",\n",
    "    group_by_length=True,\n",
    "    save_safetensors=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "print(type(training_args))\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=perf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(type(trainer))\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"model-llama-4bits-coedit\")\n",
    "# trainer.push_to_hub(\"iliazlobin/bart-large-coedit\", token=os.getenv(\"HUGGING_FACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"_name_or_path\": \"iliazlobin/bart-grammarly\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_name = \"iliazlobin/bart-large-coedit\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "# tokenizer = BartTokenizer.from_pretrained('iliazlobin/bart-grammarly')\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "print(model.config)\n",
    "\n",
    "model.save_pretrained(\"model-bart-large-coedit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('./bart-grammarly-model', device_map=0)\n",
    "print(type(model))\n",
    "# print(model.config)\n",
    "\n",
    "checkpoint = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They also feel that these cars are less attractive compared to conventional cars too.\n"
     ]
    }
   ],
   "source": [
    "# input_text = \"Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\"\n",
    "# input_text = \"Fix the grammatical mistakes: One of the reasons for its success was that laptop catered to people's needs greatly.\"\n",
    "# input_text = \"Fix grammaticality in this sentence: The low technological standard is one of the significant factors which hamper engineering design process.\"\n",
    "input_text = \"Fix all grammatical errors: They also feels that these cars are less attractive compared to conventional cars too.\"\n",
    "tokens = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**tokens, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
