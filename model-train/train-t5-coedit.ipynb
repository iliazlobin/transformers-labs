{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BART\n",
    "* https://huggingface.co/facebook/bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: t5-large,model_id: google-t5/t5-large,model_path: google-t5_t5-large\n",
      "<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
      "T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-large\"\n",
    "model_repo = f\"google-t5\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_id)\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"right\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=0)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly/coedit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set {'gec', 'clarity', 'coherence', 'simplification', 'neutralize', 'paraphrase'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 705\n",
      "})\n",
      "test set {'gec', 'clarity', 'coherence', 'simplification', 'neutralize', 'paraphrase'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 74\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references'],\n",
      "        num_rows: 705\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references'],\n",
      "        num_rows: 74\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "# print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "train_ratio = 0.01\n",
    "test_ratio = 0.001\n",
    "# train_ratio = 0.1\n",
    "# test_ratio = 0.01\n",
    "# train_ratio = 0.9\n",
    "# test_ratio = 0.1\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "train_gec_dataset = gec_dataset.select(range(0, int(train_ratio * len(gec_dataset))))\n",
    "test_gec_dataset = gec_dataset.select(range(int((1 - test_ratio) * len(gec_dataset)), len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, int(train_ratio * len(simplification_dataset))))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(simplification_dataset)), len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, int(train_ratio * len(clarity_dataset))))\n",
    "test_clarity_dataset = clarity_dataset.select(range(int((1 - test_ratio) * len(clarity_dataset)), len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, int(train_ratio * len(coherence_dataset))))\n",
    "test_coherence_dataset = coherence_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(coherence_dataset)), len(coherence_dataset))\n",
    ")\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, int(train_ratio * len(paraphrase_dataset))))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(paraphrase_dataset)), len(paraphrase_dataset))\n",
    ")\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(train_ratio * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, int(train_ratio * len(neutralize_dataset))))\n",
    "test_neutralize_dataset = neutralize_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(neutralize_dataset)), len(neutralize_dataset))\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "print(train_dataset)\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "# def add_prompt(item):\n",
    "    # return {\n",
    "    #     \"request\": f\"{item['input']}\\nResponse:\",\n",
    "    #     \"prompt\": f\"{item['input']}\\nResponse:{item['reference']}\",\n",
    "    # }\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "# dataset = dataset.map(add_prompt)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_length train: 117\n",
      "max_input_length test: 160\n"
     ]
    }
   ],
   "source": [
    "# find the longest sequence in the dataset\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"train\"])\n",
    "print(f\"max_input_length train: {max_input_length}\")\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"test\"])\n",
    "print(f\"max_input_length test: {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/gec: 203\n",
      "train/clarity: 12\n",
      "train/coherence: 106\n",
      "train/simplification: 114\n",
      "train/neutralize: 112\n",
      "train/paraphrase: 158\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    train_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    train_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "train_dataset_map = {}\n",
    "for task, l in train_lists_map.items():\n",
    "    train_dataset_map[task] = Dataset.from_list(l)\n",
    "# print(train_dataset_map)\n",
    "\n",
    "train_dataset_dict = DatasetDict(train_dataset_map)\n",
    "# print(train_dataset_dict)\n",
    "\n",
    "# for task, ds in train_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"train/{task}: {len(train_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/gec: 21\n",
      "test/clarity: 2\n",
      "test/coherence: 11\n",
      "test/simplification: 12\n",
      "test/neutralize: 12\n",
      "test/paraphrase: 16\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    test_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    test_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "test_dataset_map = {}\n",
    "for task, l in test_lists_map.items():\n",
    "    test_dataset_map[task] = Dataset.from_list(l)\n",
    "# print(test_dataset_map)\n",
    "\n",
    "test_dataset_dict = DatasetDict(test_dataset_map)\n",
    "# print(test_dataset_dict)\n",
    "\n",
    "# for task, ds in test_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"test/{task}: {len(test_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_token_id: 1\n",
      "pad_token_id: 0\n",
      "padding_side: right\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 705\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 74\n",
      "    })\n",
      "})\n",
      "55 [10002, 66, 3, 5096, 4992, 138, 6854, 45, 48, 1499, 10, 242, 677, 6, 1440, 28, 3, 9, 418, 13, 9980, 7, 54, 3, 12829, 2032, 70, 9980, 12, 993, 70, 7386, 179, 1322, 11, 338, 21455, 12, 370, 1349, 387, 12, 8, 9980, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "55 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "44 [242, 677, 6, 1440, 28, 3, 9, 418, 13, 9980, 7, 54, 3343, 70, 9980, 12, 993, 70, 7386, 179, 1322, 11, 169, 21455, 12, 370, 1349, 387, 12, 8, 9980, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'input_ids': tensor([[10002,    66,     3,  5096,  4992,   138,  6854,    45,    48,  1499,\n",
      "            10,   242,   677,     6,  1440,    28,     3,     9,   418,    13,\n",
      "          9980,     7,    54,     3, 12829,  2032,    70,  9980,    12,   993,\n",
      "            70,  7386,   179,  1322,    11,   338, 21455,    12,   370,  1349,\n",
      "           387,    12,     8,  9980,     5,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [20825,     8,     3,  5096,  4992, 10355,    10,   282,     8,   381,\n",
      "            13,   151, 13919,     6,     8,   174,    13,  7386,   179,  1164,\n",
      "            19,    73,  7771,  1575,  3834,  1832,     5,     1,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  242,   677,     6,  1440,    28,     3,     9,   418,    13,  9980,\n",
      "             7,    54,  3343,    70,  9980,    12,   993,    70,  7386,   179,\n",
      "          1322,    11,   169, 21455,    12,   370,  1349,   387,    12,     8,\n",
      "          9980,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  282,     8,   381,    13,   151, 13919,     6,     8,   174,    21,\n",
      "             3,     9,  7386,   179,  1164,    19,    73,  7771,  1575,  3834,\n",
      "          3094,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'decoder_input_ids': tensor([[    0,   242,   677,     6,  1440,    28,     3,     9,   418,    13,\n",
      "          9980,     7,    54,  3343,    70,  9980,    12,   993,    70,  7386,\n",
      "           179,  1322,    11,   169, 21455,    12,   370,  1349,   387,    12,\n",
      "             8,  9980,     5,     1,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    0,   282,     8,   381,    13,   151, 13919,     6,     8,   174,\n",
      "            21,     3,     9,  7386,   179,  1164,    19,    73,  7771,  1575,\n",
      "          3834,  3094,     5,     1,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"right\"\n",
    "print(f\"eos_token_id: {tokenizer.eos_token_id}\")\n",
    "print(f\"pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"padding_side: {tokenizer.padding_side}\")\n",
    "\n",
    "max_length = 350\n",
    "\n",
    "def process_dataset(batch):\n",
    "    model_inputs = tokenizer(batch[\"input\"], max_length=max_length, padding=True)\n",
    "    labels = tokenizer(text_target=batch[\"reference\"], max_length=max_length, padding=True).input_ids\n",
    "    # model_inputs = tokenizer(batch[\"input\"], padding=True)\n",
    "    # labels = tokenizer(text_target=batch[\"reference\"], padding=True).input_ids\n",
    "    # model_inputs = tokenizer(batch[\"input\"])\n",
    "    # labels = tokenizer(text_target=batch[\"reference\"]).input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# processed_dataset = data.map(preprocess_function, batched=True, batch_size=10, remove_columns=data[\"train\"].column_names)\n",
    "processed_dataset = dataset.map(process_dataset, batched=True, batch_size=10, remove_columns=dataset[\"train\"].column_names)\n",
    "# processed_dataset = dataset.map(process_dataset, batched=True, batch_size=10)\n",
    "print(processed_dataset)\n",
    "print(len(processed_dataset[\"train\"][\"input_ids\"][0]), processed_dataset[\"train\"][\"input_ids\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"attention_mask\"][0]), processed_dataset[\"train\"][\"attention_mask\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"labels\"][0]), processed_dataset[\"train\"][\"labels\"][0])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", model=model)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "dataloader = DataLoader(processed_dataset[\"train\"], batch_size=2, collate_fn=data_collator)\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "max_batch = 2\n",
    "max_length = 350\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"input: {input_batch['input']}\")\n",
    "\n",
    "    input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_model_name: t5-large-coedit, training_model_id: iliazlobin/t5-large-coedit, training_model_path: iliazlobin_t5-large-coedit\n",
      "total/used/cuda/res/ram (Gb): 10.00/3.67/2.75/2.75/12.30\n",
      "total/used/available memory (Gb): 10.00/3.67/6.33\n",
      "recommended/actual fraction: 0.63/0.95\n",
      "Total/trainable params: 737668096/737668096\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "training_model_repo = f\"iliazlobin\"\n",
    "training_model_name: str = f\"{model_name}-coedit\"\n",
    "training_model_id = f\"{training_model_repo}/{training_model_name}\"\n",
    "training_model_checkpoint = f\"{training_model_id}\"\n",
    "training_model_path = f\"{training_model_repo}_{training_model_name}\"\n",
    "print(\n",
    "    f\"training_model_name: {training_model_name}, \"\n",
    "    f\"training_model_id: {training_model_id}, \"\n",
    "    f\"training_model_path: {training_model_path}\"\n",
    ")\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "actual_fraction = 0.95\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = model\n",
    "# print(type(training_model))\n",
    "# print(training_model.config)\n",
    "\n",
    "total_params = sum(p.numel() for p in training_model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in training_model.parameters() if p.requires_grad)\n",
    "# print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "max_length = 350\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # print(logits)\n",
    "    # predictions = logits[0].argmax(dim=-1)\n",
    "    # predictions = torch.argmax(logits[0], dim=-1)\n",
    "    # predictions = np.argmax(logits[0])\n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    # predictions = np.where(predictions != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, max_length=max_length, skip_special_tokens=True)\n",
    "    print(decoded_preds)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, max_length=max_length, skip_special_tokens=True)\n",
    "    print(decoded_labels)\n",
    "\n",
    "    rouge_score = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    sacreblue_score = sacreblue_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # sari_score = sari_metric.compute(\n",
    "    #     sources=processed_samples[\"input\"],\n",
    "    #     predictions=processed_samples[\"processed\"],\n",
    "    #     references=processed_samples[\"references\"],\n",
    "    # )\n",
    "    em_score = em_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    report = {\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"rougeLsum\": rouge_score[\"rougeLsum\"],\n",
    "        \"sacreblue\": sacreblue_score[\"score\"],\n",
    "        \"memory_used\": utilization[\"memory_used\"] / 1024**2,\n",
    "        \"cuda_allocated\": utilization[\"cuda_allocated\"] / 1024**2,\n",
    "        \"cuda_reserved\": utilization[\"cuda_reserved\"] / 1024**2,\n",
    "        \"ram_usage\": utilization[\"ram_usage\"] / 1024**2,\n",
    "        # \"sari.sari\": 0,\n",
    "        \"em\": em_score[\"exact_match\"],\n",
    "    }\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    report[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in report.items()}\n",
    "\n",
    "# debug\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=training_model,\n",
    "#     train_dataset=processed_dataset[\"train\"],\n",
    "#     eval_dataset=processed_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     # preprocess_logits_for_metrics=logits_argmax,\n",
    "#     args=args,\n",
    "# )\n",
    "# predictions = trainer.predict(processed_dataset[\"train\"].select(range(2)))\n",
    "# print(type(predictions))\n",
    "# print(predictions)\n",
    "# metrics = compute_metrics((predictions.predictions, predictions.label_ids))\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"model-{training_model_path}-train/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: model-iliazlobin_t5-large-coedit-train\n",
      "train_size: 705, batch_size: 1, per_epoch_steps: 176.25, max_steps: 108, epochs: 0.6127659574468085\n",
      "train_size: 705, batch_size: 1, per_epoch_steps: 176.25, epochs: 1, epoch_total_steps: 176.25\n"
     ]
    }
   ],
   "source": [
    "train_path = f\"model-{training_model_path}-train\"\n",
    "print(f\"train_path: {train_path}\")\n",
    "\n",
    "train_size = len(processed_dataset[\"train\"])\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "eval_batch_size = 1\n",
    "eval_accumulation_steps = 4\n",
    "\n",
    "per_epoch_steps = train_size / (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "max_steps = 108\n",
    "epochs = max_steps / per_epoch_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, max_steps: {max_steps}, epochs: {epochs}\")\n",
    "\n",
    "# if epochs < 1:\n",
    "#     raise Exception(f\"Training doesn't cover the entire training dataset with {train_size} samples\")\n",
    "\n",
    "epochs = 1\n",
    "epoch_total_steps = epochs * per_epoch_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/training_args.py:1859: FutureWarning: `--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case iliazlobin/t5-large-coedit).\n",
      "  warnings.warn(\n",
      "/home/izlobin/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer_seq2seq.Seq2SeqTrainer'>\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 10.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 9.48 GiB is allocated by PyTorch, and 18.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# trainer.train(resume_from_checkpoint=True)\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2181\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 2181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2182\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/optimizer.py:149\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:176\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    173\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     adamw(\n\u001b[1;32m    188\u001b[0m         params_with_grad,\n\u001b[1;32m    189\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:123\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    117\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    118\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    127\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    128\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    129\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 10.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 9.48 GiB is allocated by PyTorch, and 18.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=train_path,\n",
    "    learning_rate=2e-5,\n",
    "    # learning_rate=5.6e-5,\n",
    "    weight_decay=0.01,\n",
    "    # warmup_ratio=0.05,\n",
    "    # optim=\"paged_adamw_8bit\",\n",
    "    # optim=\"paged_adamw_32bit\",\n",
    "    fp16=False,\n",
    "    fp16_full_eval=True,\n",
    "    predict_with_generate=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    eval_accumulation_steps=eval_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=max_steps,\n",
    "    warmup_steps=1,\n",
    "    logging_steps=20,\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=5,\n",
    "    # save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub_model_id=training_model_name,\n",
    "    push_to_hub_organization=\"iliazlobin\",\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # Should be false for Lora (https://github.com/kohya-ss/sd-scripts/issues/323#issuecomment-1485073421)\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=training_model,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # preprocess_logits_for_metrics=logits_argmax,\n",
    "    args=args,\n",
    ")\n",
    "print(type(trainer))\n",
    "\n",
    "model.config.use_cache = False\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")'\n",
      "<class 'transformers.trainer_seq2seq.Seq2SeqTrainer'>\n",
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../.env\n",
    "\n",
    "!huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "# print(f\"saving locally: {model-{training_model_path}}\")\n",
    "# trainer.save_model(f\"model-{training_model_path}\")\n",
    "\n",
    "# print(f\"pushing to hub: {training_model_id}\")\n",
    "# # trainer.save_model(f\"model-{training_model_path}\")\n",
    "# trainer.push_to_hub(commit_message=\"test\", model_name=training_model_name)\n",
    "# trainer.push_to_hub(commit_message=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> gec\n",
      "input: ['Fix grammar in the sentence: Noice', 'Fix disfluencies in the sentence: Taking into considerations the facts that your staff fails to provide any information regarding the possibility to have my kitchen equipments fixed or replaced as well as your poor response to your customer concerns, I would like to received a full refund for the faulty microwave purchasing from your shops.']\n",
      "result: [':: No the sentence::: Fix grammar in the sentence:::::: No the sentence:: the sentence: Fix grammar in the sentence: Noice:: Fix grammar in the sentence: Noice::: Fix grammar in the sentence: Noice: Noice::: No: Noice::: No:::: No:ice: No: Noice: Noice', 'False claims in the sentence:']\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "max_batch = 2\n",
    "max_length = 350\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"input: {input_batch['input']}\")\n",
    "\n",
    "    input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: model-iliazlobin_bart-large-coedit-train,model_id: iliazlobin/model-iliazlobin_bart-large-coedit-train,model_path: iliazlobin_model-iliazlobin_bart-large-coedit-train\n",
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "BartConfig {\n",
      "  \"_name_or_path\": \"iliazlobin/model-iliazlobin_bart-large-coedit-train\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/iliazlobin/bart-large-coedit/commit/c2e558803e815433a4dcae0a9f84a3332c0804f7', commit_message='Upload config', commit_description='', oid='c2e558803e815433a4dcae0a9f84a3332c0804f7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BartForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"model-iliazlobin_bart-large-coedit-train\"\n",
    "model_repo = f\"iliazlobin\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "tokenizer.push_to_hub(repo_id=\"iliazlobin/bart-large-coedit\")\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "print(model.config)\n",
    "# print(model)\n",
    "\n",
    "# !huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "# print(training_model_path)\n",
    "# print(type(trainer))\n",
    "# print(type(trainer.model))\n",
    "\n",
    "model.push_to_hub(commit_message=\"train complete on coedit dataset\", repo_id=\"iliazlobin/bart-large-coedit\")\n",
    "model.generation_config.push_to_hub(repo_id=\"iliazlobin/bart-large-coedit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
