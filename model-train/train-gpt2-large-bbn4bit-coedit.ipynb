{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"openai-community/gpt2-large\"\n",
    "model_name = \"openai-community/gpt2-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "print(type(tokenizer))\n",
    "\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # padding_side=\"right\",\n",
    "    # model_max_length=512,\n",
    "    # pad_token=\"!\",\n",
    "    # add_eos_token=True,\n",
    "    # add_bos_token=True,\n",
    "    # padding='longest',\n",
    "    use_fast=True,\n",
    "    # trust_remote_code=True,\n",
    ")\n",
    "print(type(tokenizer))\n",
    "\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# print(tokenizer.eos_token_id)\n",
    "# print(tokenizer.pad_token_id)\n",
    "# print(tokenizer.special_tokens_map)\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens(\n",
    "#     {\n",
    "#         \"pad_token\": \"<|endoftext|>\",\n",
    "#         \"eos_token\": \"<|endoftext|>\",\n",
    "#         \"bos_token\": \"<|endoftext|>\",\n",
    "#         \"unk_token\": \"<|endoftext|>\",\n",
    "#     }\n",
    "# )\n",
    "\n",
    "tokenizer.add_eos_token = True\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "input_text = \"\"\"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs\n",
    "Response:\n",
    "\"\"\".strip()\n",
    "tokens = tokenizer(input_text, add_special_tokens=True)\n",
    "print(tokens.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "input_text = \"Fix grammatical errors in this sentence: I goes to school every day.\"\n",
    "# input_text = \"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\"\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "# outputs = model.generate(\n",
    "#     **input_ids,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=False,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "full_ds = load_dataset(\"Abirate/english_quotes\")\n",
    "print(full_ds)\n",
    "train_ds = load_dataset(\"Abirate/english_quotes\", split=\"train[:30%]\")\n",
    "test_ds = load_dataset(\"Abirate/english_quotes\", split=\"train[-05%:]\")\n",
    "\n",
    "processed_train_ds = train_ds.map(\n",
    "    lambda samples: tokenizer(samples[\"quote\"], max_length=512), batched=True\n",
    ")\n",
    "processed_test_ds = test_ds.map(\n",
    "    lambda samples: tokenizer(samples[\"quote\"], max_length=512), batched=True\n",
    ")\n",
    "print(processed_train_ds)\n",
    "print(processed_test_ds)\n",
    "\n",
    "# test_data = train_test_split(\n",
    "#     test_size=0.1,\n",
    "# )\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_data = load_dataset(\"grammarly/coedit\", split=\"train\")\n",
    "# test_data = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "train_data = load_dataset(\"grammarly/coedit\", split=\"train[:1000]\")\n",
    "test_data = load_dataset(\"grammarly/coedit\", split=\"train[:100]\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_data, \"test\": test_data})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src', 'tgt', 'prompt'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src', 'tgt', 'prompt'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "2\n",
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response: For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
      "Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\n",
      "Response: As the number of people grows, the need for a habitable environment is unquestionably increasing.\n"
     ]
    }
   ],
   "source": [
    "def generate_grammar_prompt(src: str, tgt: str) -> str:\n",
    "    return f\"\"\"{src}\n",
    "Response: {tgt}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def prompt_dataset(item):\n",
    "    return {\n",
    "        \"prompt\": generate_grammar_prompt(item[\"src\"], item[\"tgt\"]),\n",
    "        # \"labels\": item[\"tgt\"],\n",
    "    }\n",
    "\n",
    "\n",
    "prompted_dataset = dataset.map(prompt_dataset, remove_columns=[\"task\", \"_id\"])\n",
    "print(prompted_dataset)\n",
    "print(len(prompted_dataset))\n",
    "print(prompted_dataset[\"train\"][0][\"prompt\"])\n",
    "print(prompted_dataset[\"train\"][1][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "[27914, 477, 14599, 44935, 8563, 422, 428, 2420, 25, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 1059, 430, 687, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 1262, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 198, 31077, 25, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 6121, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 779, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 50256]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 6121, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 779, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 50256]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "def tokenize(samples):\n",
    "    # model_input = tokenizer(examples[\"prompt\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs = tokenizer(samples[\"prompt\"])\n",
    "    model_reponses = tokenizer(samples[\"tgt\"])\n",
    "\n",
    "    new_input_ids = []\n",
    "    new_labels = []\n",
    "    for input_ids, response_ids in zip(model_inputs.input_ids, model_reponses.input_ids):\n",
    "        # debug_labels = input_ids[-len(response_ids) :]\n",
    "        # print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "        # print(tokenizer.decode(debug_labels, skip_special_tokens=False))\n",
    "\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "        new_input_ids.append(input_ids)\n",
    "\n",
    "        num_tokens_ignore = len(input_ids) - len(response_ids)\n",
    "        labels = [-100] * num_tokens_ignore + input_ids[-len(response_ids) :]\n",
    "        new_labels.append(labels)\n",
    "\n",
    "    new_attention_mask = []\n",
    "    for attention_mask in model_inputs.attention_mask:\n",
    "        attention_mask.append(1)\n",
    "        new_attention_mask.append(attention_mask)\n",
    "\n",
    "    # labels = tokenizer(text_target=examples[\"tgt\"], max_length=1024, padding=True).input_ids\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs[\"input_ids\"] = new_input_ids\n",
    "    model_inputs[\"attention_mask\"] = new_attention_mask\n",
    "    model_inputs[\"labels\"] = new_labels\n",
    "\n",
    "    # print(\n",
    "    #     f\">> input_ids: {len(model_inputs['input_ids'])},\"\n",
    "    #     \"attention_mask: {len(model_inputs['attention_mask'])},\"\n",
    "    #     \"labels: {len(model_inputs['labels'])}\"\n",
    "    # )\n",
    "    # count = 0\n",
    "    # for input_ids, labels, attention_masks in zip(new_input_ids, new_labels, new_attention_mask):\n",
    "    #     count += 1\n",
    "    #     if count > 3:\n",
    "    #         break\n",
    "    #     print(f\">> input_ids: {len(input_ids)}, attention_mask: {len(attention_masks)}, labels: {len(labels)}\")\n",
    "    #     print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "    #     print(labels)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_dataset = prompted_dataset.map(tokenize, batched=True, remove_columns=[\"tgt\", \"src\", \"prompt\"])\n",
    "print(processed_dataset)\n",
    "print(processed_dataset[\"train\"][\"input_ids\"][0])\n",
    "print(processed_dataset[\"train\"][\"attention_mask\"][0])\n",
    "print(processed_dataset[\"train\"][\"labels\"][0])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# dataloader = DataLoader(processed_dataset[\"train\"], batch_size=4, collate_fn=data_collator)\n",
    "# for batch in dataloader:\n",
    "# print(batch)\n",
    "# break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total/used/cuda/res/ram (Gb): 10.00/1.82/0.51/0.57/14.12\n",
      "total/used/available memory (Gb): 10.00/1.82/8.18\n",
      "recommended/actual fraction: 0.82/0.90\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "actual_fraction = 0.90\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "trainable params: 23592960 || all params: 443728640 || trainable%: 5.316979314204285\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "perf_model = model\n",
    "perf_model = prepare_model_for_kbit_training(perf_model, use_gradient_checkpointing=True)\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=32,\n",
    "    # target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "perf_model = get_peft_model(perf_model, config)\n",
    "print(type(perf_model))\n",
    "print_trainable_parameters(perf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'model-gpt2-large-bnb4bit-coedit-train/runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.770100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.468100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.7852902460098267, metrics={'train_runtime': 80.1026, 'train_samples_per_second': 9.987, 'train_steps_per_second': 1.248, 'total_flos': 237645717657600.0, 'train_loss': 0.7852902460098267, 'epoch': 0.8})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "training_model = perf_model\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"model-gpt2-large-bnb4bit-coedit-train\",\n",
    "    learning_rate=2e-4,\n",
    "    # warmup_ratio=0.05,\n",
    "    # optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    warmup_steps=2,\n",
    "    # save_steps=50,\n",
    "    max_steps=100,\n",
    "    # num_train_epochs=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=10,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=training_model,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    # eval_dataset=processed_dataset[\"test\"],\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(type(trainer))\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "# trainer.save_model(f\"model-{model_alias}-bnb4bit-coedit-test\")\n",
    "trained_model = trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
      "          1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
      "           430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
      "          1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
      "           198, 31077,    25],\n",
      "        [22743, 23491,    25,   679,  3708,   257,  4998,  1097,    13,   198,\n",
      "         31077,    25, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256],\n",
      "        [22743, 23491,    25,   679,  3708,   257,  4998,  1097,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/izlobin/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' For example, countries with a lot of deserts can terraform their deserts to increase their habitable land and use irrigation to provide clean water to the desert.', 'A remarkable car.', '\\n\\nHe drives a amazing car.']\n"
     ]
    }
   ],
   "source": [
    "# input_text = \"Fix grammatical errors in this sentence: I goes to school every day.\"\n",
    "# input_text = \"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\"\n",
    "\n",
    "# input_text = f\"\"\"Fix grammar: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "# Response:\n",
    "# \"\"\".strip()\n",
    "# input_text = f\"\"\"Fix grammar: Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "# Response: For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
    "# \"\"\".strip()\n",
    "# For example, countries with a lot of deserts can be terraformed to increase their habitable land and to use the irrigation system to provide clean water to the desert\n",
    "# from scipy import special\n",
    "\n",
    "\n",
    "# input_text = f\"\"\"Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "# Response:\n",
    "# \"\"\".strip()\n",
    "# input_text = f\"\"\"Fix grammar: He drive a amazing car.\n",
    "# Response:\n",
    "# \"\"\".strip()\n",
    "input_texts = [\n",
    "    f\"\"\"Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "Response:\n",
    "\"\"\".strip(),\n",
    "    f\"\"\"Fix grammar: He drive a amazing car.\n",
    "Response:\n",
    "\"\"\".strip(),\n",
    "    f\"\"\"Fix grammar: He drive a amazing car.\"\"\",\n",
    "]\n",
    "# input_texts = [\n",
    "#     \"Fix grammar:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\",\n",
    "#     \"Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian Hall, still a vibrant community center/beer garden started by Czech immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, continuously occupied Latin music store in New York City,  as census sites to the National Register of Historic Places.\",\n",
    "# ]\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "print(inputs)\n",
    "\n",
    "trained_model.config.use_cache = False\n",
    "# trained_model.config.pad_token_id = trained_model.config.eos_token_id\n",
    "# outputs = trained_model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = trained_model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "outputs = trained_model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "# outputs = trained_model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=True,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "trimmed_outputs = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "print(trimmed_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f981574cd40444529b4f77b8c67e35ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/19705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5078, num_samples: 100, selected: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a576588a9fe74e25b05470f842af14bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f21c80c5a14d32b908818cea94c595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/19705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5106, num_samples: 100, selected: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc39b1e17fa48b8b5da90ffefd286d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5cafba116e4244a1daeef2f872a94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/19705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 1676, num_samples: 100, selected: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec82bb54a363451ea0a8f7006894dcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fluency': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "}), 'clarity': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "}), 'coherence': Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "})}\n",
      "Fix grammar:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre? Response: \n",
      " We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      " We don't have enough good Open Source games â€” it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n",
      "Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian Hall, still a vibrant community center/beer garden started by Czech immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, continuously occupied Latin music store in New York City,  as census sites to the National Register of Historic Places. Response: \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 10\n",
    "total_samples = 100\n",
    "samples_map = {\n",
    "    \"fluency\": get_iterater_samples_with_instruction(label=\"fluency\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "    \"clarity\": get_iterater_samples_with_instruction(label=\"clarity\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "    \"coherence\": get_iterater_samples_with_instruction(label=\"coherence\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "}\n",
    "print(samples_map)\n",
    "print(samples_map[\"fluency\"][\"task\"][0])\n",
    "print(samples_map[\"fluency\"][\"source\"][0])\n",
    "print(samples_map[\"fluency\"][\"reference\"][0])\n",
    "print(samples_map[\"fluency\"][\"task\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "max_samples: 5078, num_samples: 10, selected: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e97471ff5c34f2694fe411ae0148f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5106, num_samples: 10, selected: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059e5499c9784768a1a00b577c3ae95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 1676, num_samples: 10, selected: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97321a4a5c44707a383c1c31dcd01cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total/used/cuda/res/ram (Gb): 10.00/4.00/0.73/2.83/16.04\n",
      "Processing 10 samples for fluency\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Fix grammar errors:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre? Response: \n",
      " We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decbffd3e0534577ab030ef457da4ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/10 | total/used/cuda/res/ram (Gb): 10.00/4.03/0.73/2.83/16.03 | batch/sps: 5/1.18\n",
      "5-9/10 | total/used/cuda/res/ram (Gb): 10.00/4.03/0.73/2.83/16.01 | batch/sps: 5/1.38\n",
      "Finished processing 10 samples for fluency.\n",
      "10 | total/used/cuda/res/ram (Gb): 10.00/4.03/0.73/2.83/16.01 | sps: 0.82\n",
      "Processing 10 samples for clarity\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Make this sentence more readable:  Dr. Farley Stillwell appears in Spider-Man: The Animated  , voiced by Michael Rye. Response: \n",
      " Dr. Farley Stillwell appears in Spider-Man: The Animated  , voiced by Michael Rye.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a3544ef1844c28b3ab3fbf5ddbb5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/10 | total/used/cuda/res/ram (Gb): 10.00/4.01/0.73/2.83/16.02 | batch/sps: 5/1.17\n",
      "5-9/10 | total/used/cuda/res/ram (Gb): 10.00/4.06/0.73/2.83/16.03 | batch/sps: 5/1.38\n",
      "Finished processing 10 samples for clarity.\n",
      "10 | total/used/cuda/res/ram (Gb): 10.00/4.06/0.73/2.83/16.03 | sps: 0.88\n",
      "Processing 10 samples for coherence\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Improve the coherence of the text:  The study also exhibits three methodological challenges with respect to assessing of interventions : a) the estimation of true infection dates , b) the usage of several indicators and c) the influence of test volume . In conclusion, the effectiveness of most German interventions remains questionable . Response: \n",
      " The study also exhibits three methodological challenges with respect to assessing of interventions : a) the estimation of true infection dates , b) the usage of several indicators and c) the influence of test volume . In conclusion, the effectiveness of most German interventions remains questionable .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacc530e753b4af79f9090143f7413f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4/10 | total/used/cuda/res/ram (Gb): 10.00/4.02/0.73/2.83/16.12 | batch/sps: 5/1.55\n",
      "5-9/10 | total/used/cuda/res/ram (Gb): 10.00/4.01/0.73/2.83/16.12 | batch/sps: 5/1.91\n",
      "Finished processing 10 samples for coherence.\n",
      "10 | total/used/cuda/res/ram (Gb): 10.00/4.01/0.73/2.83/16.12 | sps: 1.08\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'processed'],\n",
      "    num_rows: 10\n",
      "})\n",
      "[\" We don't have enough good Open Source games, as it's a waste of resources \"\n",
      " 'to pour all the resources into one. As for the World of Warcraft, I would '\n",
      " 'argue that it is a waste of resources to pour all the resources into one '\n",
      " 'game.',\n",
      " '']\n",
      "CPU times: user 30.2 s, sys: 2.57 s, total: 32.8 s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "model = trained_model\n",
    "model_name = \"iliazlobin/gpt2-large-bnb4bit-coedit\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "batch_size = 5\n",
    "total_samples = 10\n",
    "samples_map = {\n",
    "    \"fluency\": get_iterater_samples_with_instruction(label=\"fluency\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "    \"clarity\": get_iterater_samples_with_instruction(label=\"clarity\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "    \"coherence\": get_iterater_samples_with_instruction(label=\"coherence\", num_samples=total_samples, post_instruction=\"Response: \"),\n",
    "}\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"task\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    # print(batch[\"task\"])\n",
    "    inputs = tokenizer(batch[\"task\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    # print(inputs)\n",
    "    # inputs = tokenizer(batch['task'], return_tensors=\"pt\").inputs.to(device)\n",
    "    # inputs = tokenizer(item['task'], return_tensors=\"pt\").inputs\n",
    "\n",
    "    # outputs = model.generate(inputs, max_length=512)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=128,\n",
    "    )\n",
    "    # print(outputs)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    processed = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    # print(processed)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for category, samples in samples_map.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {category}\")\n",
    "    print(samples)\n",
    "    print(samples[\"task\"][0])\n",
    "    print(samples[\"source\"][0])\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {category}.\")\n",
    "    print(processed_samples)\n",
    "    print(processed_samples[\"reference\"][0])\n",
    "    print(processed_samples[\"processed\"][0])\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[category] = {\n",
    "        # \"category\": category,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"sps\": sps,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_fluency_samples = processed_samples_map[\"fluency\"][\"samples\"]\n",
    "\n",
    "pprint(processed_fluency_samples)\n",
    "pprint(processed_fluency_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: iliazlobin/gpt2-large-bnb4bit-coedit\n",
      "model_alias: iliazlobin_gpt2-large-bnb4bit-coedit\n",
      "Total/trainable params: 443728640/23592960\n",
      "{'model': 'iliazlobin/gpt2-large-bnb4bit-coedit', 'hardware': 'HomeDesktop (RTX3080)', 'total_params': 443728640, 'fluency': {'category': 'fluency', 'total_samples': 10, 'sps': 0.8247413760947063, 'scores': {'rouge': {'rouge1': 0.42422309885257353, 'rouge2': 0.3108144350176073, 'rougeL': 0.3976087074227543, 'rougeLsum': 0.4029114324560925}, 'sacreblue': {'score': 25.24792575861845, 'counts': [141, 101, 77, 59], 'totals': [345, 336, 327, 318], 'precisions': [40.869565217391305, 30.05952380952381, 23.547400611620795, 18.553459119496857], 'bp': 0.9327992966640325, 'sys_len': 345, 'ref_len': 369}, 'sari': {'sari': 14.228726961829649}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 4325888000, 'cuda_allocated': 789037568, 'cuda_reserved': 3036676096, 'ram_usage': 17190219776}}, 'clarity': {'category': 'clarity', 'total_samples': 10, 'sps': 0.8821406862780048, 'scores': {'rouge': {'rouge1': 0.7044986395300419, 'rouge2': 0.5981344696969697, 'rougeL': 0.6779026933972834, 'rougeLsum': 0.6845886459226753}, 'sacreblue': {'score': 52.242270637753926, 'counts': [220, 177, 152, 130], 'totals': [334, 324, 314, 304], 'precisions': [65.86826347305389, 54.629629629629626, 48.40764331210191, 42.76315789473684], 'bp': 1.0, 'sys_len': 334, 'ref_len': 298}, 'sari': {'sari': 34.342356799399795}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 4359049216, 'cuda_allocated': 789037568, 'cuda_reserved': 3036676096, 'ram_usage': 17215336448}}, 'coherence': {'category': 'coherence', 'total_samples': 10, 'sps': 1.0818681389118088, 'scores': {'rouge': {'rouge1': 0.5259657651944001, 'rouge2': 0.4561673987776929, 'rougeL': 0.5216496566330084, 'rougeLsum': 0.5271451283215989}, 'sacreblue': {'score': 41.65480326350308, 'counts': [172, 147, 132, 119], 'totals': [255, 247, 239, 231], 'precisions': [67.45098039215686, 59.51417004048583, 55.23012552301255, 51.515151515151516], 'bp': 0.7165313105737893, 'sys_len': 255, 'ref_len': 340}, 'sari': {'sari': 31.83103927604012}, 'em': {'exact_match': 0.0}}, 'utilization': {'total_memory': 10736893952, 'memory_used': 4308979712, 'cuda_allocated': 789037568, 'cuda_reserved': 3036676096, 'ram_usage': 17307279360}}}\n"
     ]
    }
   ],
   "source": [
    "hardware = \"HomeDesktop (RTX3080)\"\n",
    "\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"model_alias: {model_alias}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "all_flats = []\n",
    "all_scores = []\n",
    "if os.path.exists(\"results/all-scores.csv\"):\n",
    "    all_scores = pd.read_csv(\"results/all-scores.csv\").to_dict(\"records\")\n",
    "\n",
    "all_fulls = []\n",
    "\n",
    "for category, obj in processed_samples_map.items():\n",
    "    samples = obj[\"samples\"]\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    all_saved_samples = samples.remove_columns([\"references\"])\n",
    "    saved_samples = all_saved_samples[:100] if len(all_saved_samples) > 100 else all_saved_samples\n",
    "    flats_frame = pd.DataFrame.from_records(saved_samples)\n",
    "    flats_frame.to_json(f\"samples/{model_alias}_{category}.json\", orient=\"records\")\n",
    "\n",
    "    scores = calculate_scores(samples)\n",
    "    # pprint(scores)\n",
    "\n",
    "    score_paths = [\n",
    "        \"rouge.rouge1\",\n",
    "        # \"rouge.rouge2\",\n",
    "        # \"rouge.rougeL\",\n",
    "        # \"rouge.rougeLsum\",\n",
    "        \"sacreblue.score\",\n",
    "        \"sari.sari\",\n",
    "        \"em.exact_match\",\n",
    "    ]\n",
    "\n",
    "    normalized_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        for k2, v2 in v.items():\n",
    "            if not isinstance(v2, list):\n",
    "                # normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "                path = f\"{k}.{k2}\"\n",
    "                if path in score_paths:\n",
    "                    normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "    # pprint(normalized_scores)\n",
    "\n",
    "    normalized_utilization = {}\n",
    "    for k, v in obj[\"utilization\"].items():\n",
    "        if not isinstance(v, list):\n",
    "            normalized_utilization[f\"utilization.{k}\"] = v\n",
    "    # print(normalized_utilization)\n",
    "\n",
    "    flat_dict = {\n",
    "        \"model\": model_name,\n",
    "        \"hardware\": hardware,\n",
    "        \"total_params\": total_params,\n",
    "        \"category\": category,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"sps\": obj[\"sps\"],\n",
    "    }\n",
    "    flat_dict.update(normalized_scores)\n",
    "    flat_dict.update(normalized_utilization)\n",
    "    # pprint(frame)\n",
    "\n",
    "    all_flats.append(flat_dict)\n",
    "    all_scores.append(flat_dict)\n",
    "\n",
    "    fulls_frame = {\n",
    "        \"category\": category,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"sps\": obj[\"sps\"],\n",
    "    }\n",
    "    fulls_frame.update(\n",
    "        {\n",
    "            \"scores\": scores,\n",
    "            \"utilization\": obj[\"utilization\"],\n",
    "        }\n",
    "    )\n",
    "    all_fulls.append(fulls_frame)\n",
    "\n",
    "flats_frame = pd.DataFrame.from_records(all_flats)\n",
    "flats_frame.to_csv(f\"results/{model_alias}.csv\", index=False)\n",
    "\n",
    "scores_frame = pd.DataFrame.from_records(all_scores)\n",
    "scores_frame.to_csv(f\"results/all-scores.csv\", index=False)\n",
    "\n",
    "fulls_dict = {\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": hardware,\n",
    "    \"total_params\": total_params,\n",
    "}\n",
    "for full in all_fulls:\n",
    "    fulls_dict[full[\"category\"]] = full\n",
    "\n",
    "print(fulls_dict)\n",
    "fulls_frame = pd.DataFrame.from_records([fulls_dict])\n",
    "fulls_frame.to_json(f\"results/{model_alias}.json\", orient=\"records\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
