{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlabonne/llm-course/blob/main/Fine_tune_Llama_2_in_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSHlAbqzDFDq"
      },
      "source": [
        "# Fine-tune Llama 2 in Google Colab\n",
        "> ðŸ—£ï¸ Large Language Model Course\n",
        "\n",
        "â¤ï¸ Created by [@maximelabonne](https://twitter.com/maximelabonne), based on Younes Belkada's [GitHub Gist](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da). Special thanks to Tolga HOÅžGÃ–R for his solution to empty the VRAM.\n",
        "\n",
        "This notebook runs on a T4 GPU. (Last update: 01 Aug 2023)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "auto-gptq 0.7.1 requires accelerate>=0.26.0, but you have accelerate 0.21.0 which is incompatible.\n",
            "auto-gptq 0.7.1 requires peft>=0.5.0, but you have peft 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set {'clarity', 'neutralize', 'gec', 'simplification', 'coherence', 'paraphrase'}\n",
            "Dataset({\n",
            "    features: ['task', 'input', 'reference', 'references'],\n",
            "    num_rows: 1413\n",
            "})\n",
            "test set {'clarity', 'neutralize', 'gec', 'simplification', 'coherence', 'paraphrase'}\n",
            "Dataset({\n",
            "    features: ['task', 'input', 'reference', 'references'],\n",
            "    num_rows: 144\n",
            "})\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0701450f494149cebcb8497892bfb2c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1413 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1475998c738d41efb434051c3197c1cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'llama2_request', 'llama2_prompt'],\n",
            "        num_rows: 1413\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'llama2_request', 'llama2_prompt'],\n",
            "        num_rows: 144\n",
            "    })\n",
            "})\n",
            "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'], 'request': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'prompt': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'llama2_request': '<s> [INST] <<SYS>> You are a grammar assistant, you output a correct sentence only without any explanation. <</SYS>> Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST]', 'llama2_prompt': '<s> [INST] <<SYS>> You are a grammar assistant, you output a correct sentence only without any explanation. <</SYS>> Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST] For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert. </s>'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import DatasetDict, load_dataset\n",
        "\n",
        "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
        "# print(full_dataset)\n",
        "\n",
        "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
        "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
        "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
        "\n",
        "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
        "# print(all_dataset)\n",
        "\n",
        "# print()\n",
        "# print(f\"train set {set(all_dataset['task'])}\")\n",
        "# print(f\"total len: {len(all_dataset)}\")\n",
        "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
        "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
        "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
        "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
        "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
        "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
        "# print()\n",
        "\n",
        "# train_ratio = 0.01\n",
        "# test_ratio = 0.001\n",
        "# train_ratio = 0.1\n",
        "# test_ratio = 0.01\n",
        "train_ratio = 0.02\n",
        "test_ratio = 0.002\n",
        "# train_ratio = 0.9\n",
        "# test_ratio = 0.1\n",
        "\n",
        "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
        "train_gec_dataset = gec_dataset.select(range(0, int(train_ratio * len(gec_dataset))))\n",
        "test_gec_dataset = gec_dataset.select(range(int((1 - test_ratio) * len(gec_dataset)), len(gec_dataset)))\n",
        "\n",
        "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
        "train_simplification_dataset = simplification_dataset.select(range(0, int(train_ratio * len(simplification_dataset))))\n",
        "test_simplification_dataset = simplification_dataset.select(\n",
        "    range(int((1 - test_ratio) * len(simplification_dataset)), len(simplification_dataset))\n",
        ")\n",
        "\n",
        "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
        "train_clarity_dataset = clarity_dataset.select(range(0, int(train_ratio * len(clarity_dataset))))\n",
        "test_clarity_dataset = clarity_dataset.select(range(int((1 - test_ratio) * len(clarity_dataset)), len(clarity_dataset)))\n",
        "\n",
        "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
        "train_coherence_dataset = coherence_dataset.select(range(0, int(train_ratio * len(coherence_dataset))))\n",
        "test_coherence_dataset = coherence_dataset.select(\n",
        "    range(int((1 - test_ratio) * len(coherence_dataset)), len(coherence_dataset))\n",
        ")\n",
        "\n",
        "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
        "train_paraphrase_dataset = paraphrase_dataset.select(range(0, int(train_ratio * len(paraphrase_dataset))))\n",
        "test_paraphrase_dataset = paraphrase_dataset.select(\n",
        "    range(int((1 - test_ratio) * len(paraphrase_dataset)), len(paraphrase_dataset))\n",
        ")\n",
        "\n",
        "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
        "neutralize_dataset_split = int(train_ratio * len(neutralize_dataset))\n",
        "train_neutralize_dataset = neutralize_dataset.select(range(0, int(train_ratio * len(neutralize_dataset))))\n",
        "test_neutralize_dataset = neutralize_dataset.select(\n",
        "    range(int((1 - test_ratio) * len(neutralize_dataset)), len(neutralize_dataset))\n",
        ")\n",
        "\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "train_dataset = concatenate_datasets(\n",
        "    [\n",
        "        train_gec_dataset,\n",
        "        train_simplification_dataset,\n",
        "        train_clarity_dataset,\n",
        "        train_coherence_dataset,\n",
        "        train_paraphrase_dataset,\n",
        "        train_neutralize_dataset,\n",
        "    ]\n",
        ")\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda item: {\n",
        "        \"input\": item[\"src\"],\n",
        "        \"reference\": item[\"tgt\"],\n",
        "        \"references\": [item[\"tgt\"]],\n",
        "    },\n",
        "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
        ")\n",
        "print(f\"train set {set(train_dataset['task'])}\")\n",
        "print(train_dataset)\n",
        "\n",
        "test_dataset = concatenate_datasets(\n",
        "    [\n",
        "        test_gec_dataset,\n",
        "        test_simplification_dataset,\n",
        "        test_clarity_dataset,\n",
        "        test_coherence_dataset,\n",
        "        test_paraphrase_dataset,\n",
        "        test_neutralize_dataset,\n",
        "    ]\n",
        ")\n",
        "test_dataset = test_dataset.map(\n",
        "    lambda item: {\n",
        "        \"input\": item[\"src\"],\n",
        "        \"reference\": item[\"tgt\"],\n",
        "        \"references\": [item[\"tgt\"]],\n",
        "    },\n",
        "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
        ")\n",
        "print(f\"test set {set(test_dataset['task'])}\")\n",
        "print(test_dataset)\n",
        "\n",
        "\n",
        "def llama2_prompt(item, response=True):\n",
        "    prompt = f\"\"\"<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>> {item['input']} [/INST]\n",
        "\"\"\".strip()\n",
        "    if response:\n",
        "        prompt = f\"\"\"{prompt} {item['reference']} </s>\n",
        "\"\"\".strip()\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def add_prompt(item):\n",
        "    return {\n",
        "        \"request\": f\"{item['input']}\\nResponse:\",\n",
        "        \"prompt\": f\"{item['input']}\\nResponse:{item['reference']}\",\n",
        "        \"llama2_request\": llama2_prompt(item, response=False),\n",
        "        \"llama2_prompt\": llama2_prompt(item),\n",
        "    }\n",
        "\n",
        "\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "# dataset = dataset.rename_column(\"task\", \"label\")\n",
        "dataset = dataset.map(add_prompt)\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "# new_model = \"llama-2-7b-miniguanaco\"\n",
        "new_model = \"llama-2-7b-coedit\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 100\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 100\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52c4bf7418f74bc79a8c12fe35901974",
            "c5e609d111b34d408a53a4cd71bb43d5",
            "0e0a20b5ed7a44e9834022e7eba2194d",
            "b5627331e78e4eb28765ed20f32cf403",
            "8084d4cb267f4a52b3d80ec34d291190",
            "a8dcdf1f7ab64242acb057e8b54ebf79",
            "1ca492fddbaa4ea7a3226649154e01fd",
            "a8eda8bfe08e4152a80c63830138c96d",
            "1f258eacd6d0472385d41523b65dea8b",
            "228b1bcf604f454f8060a250b58008a1",
            "90b281e9c5ed4e77ab93e5879d0b15a3"
          ]
        },
        "id": "OJXpOgBFuSrc",
        "outputId": "8d06ed40-ea32-4d85-8665-413bde069607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Your GPU supports bfloat16: accelerate training with bf16=True\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38152a8a4cc84a6892518b4fb1e63cd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaConfig {\n",
            "  \"_name_or_path\": \"NousResearch/Llama-2-7b-chat-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": false,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load dataset (you can process it here)\n",
        "# dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "print(model.config)\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell me about yourself\n",
            "<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>>\n",
            "Make the sentence simple: Hugo Wolf was born in Windischgra Ìˆ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
            "{'input_ids': tensor([[    1, 24948,   592,  1048,  7535]], device='cuda:0')}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    1, 24948,   592,  1048,  7535, 29892,   322,  2020,   366,   864,\n",
            "           304,   367,   263,   760,   310,   445,  2060, 29973,    13, 19838]],\n",
            "       device='cuda:0')\n",
            "final_result: [', and why you want to be a part of this project?\\n Unterscheidung']\n"
          ]
        }
      ],
      "source": [
        "# %%script echo skip\n",
        "\n",
        "raw_input = \"Tell me about yourself\".strip()\n",
        "print(raw_input)\n",
        "\n",
        "single_input = \"\"\"<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>>\n",
        "Make the sentence simple: Hugo Wolf was born in Windischgra Ìˆ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
        "\"\"\".strip()\n",
        "print(single_input)\n",
        "\n",
        "batch_input = [\n",
        "    \"\"\"<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>>\n",
        "Make the sentence simple: Hugo Wolf was born in Windischgra Ìˆ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
        "\"\"\".strip(),\n",
        "    \"\"\"<s> [INST] <<SYS>> You are a grammar assistant, you output a corrected sentence only without any explanation. <</SYS>>\n",
        "Make the sentence simple: Handzus ÌŒ played for the St. Louis Blues, Phoenix Coyotes, Philadelphia Flyers, Los Angeles Kings, San Jose Sharks and the Chicago Blackhawks, with whom he won the Stanley Cup with in 2013. [/INST]\n",
        "\"\"\".strip(),\n",
        "]\n",
        "\n",
        "# padding - https://huggingface.co/docs/transformers/en/pad_truncation\n",
        "# inputs = tokenizer(single_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
        "inputs = tokenizer(raw_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
        "print(inputs)\n",
        "\n",
        "outputs = model.generate(inputs.input_ids)\n",
        "print(outputs)\n",
        "\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "# print(result)\n",
        "\n",
        "final_result = outputs[:, inputs.input_ids.shape[1] :]\n",
        "final_result = tokenizer.batch_decode(final_result, skip_special_tokens=True)\n",
        "print(f\"final_result: {final_result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'llama2_request', 'llama2_prompt'],\n",
            "    num_rows: 1413\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfbe10528cd849a59fbb58d634ec5f5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1413 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 01:15, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"llama2_prompt\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crj9svNe4hU5"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir results/runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
            "2\n",
            "2\n",
            "right\n",
            "Dataset({\n",
            "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt', 'llama2_request', 'llama2_prompt'],\n",
            "    num_rows: 1413\n",
            "})\n",
            "llama2_request: ['<s> [INST] <<SYS>> You are a grammar assistant. You just provide the rewritten corrected sentence in your output, that is. You do not provide any explanation. You do not prefix your sentence with any of your comment. You end your response right after the corrected sentence. <</SYS>> Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST]']\n",
            "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.']\n",
            "{'input_ids': tensor([[    1,     1,   518, 25580, 29962,  3532, 14816, 29903,  6778,   887,\n",
            "           526,   263, 25437, 20255, 29889,   887,   925,  3867,   278,   337,\n",
            "         17625, 24114, 10541,   297,   596,  1962, 29892,   393,   338, 29889,\n",
            "           887,   437,   451,  3867,   738,  8252, 29889,   887,   437,   451,\n",
            "         10944,   596, 10541,   411,   738,   310,   596,  3440, 29889,   887,\n",
            "          1095,   596,  2933,  1492,  1156,   278, 24114, 10541, 29889,   529,\n",
            "           829, 14816, 29903,  6778, 15154,   599, 14961,  2922,   936,  4436,\n",
            "           515,   445,  1426, 29901,  1152,  1342, 29892, 10916,   411,   263,\n",
            "          3287,   310, 18197, 29879,   508, 15087,   689,  1009, 18197,   304,\n",
            "          7910,  1009,  4760,   519,  2982,   322,   773,  3805,  8966,   362,\n",
            "           304,  3867,  5941,  4094,   304,   278, 18197, 29889,   518, 29914,\n",
            "         25580, 29962]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    1,     1,   518, 25580, 29962,  3532, 14816, 29903,  6778,   887,\n",
            "           526,   263, 25437, 20255, 29889,   887,   925,  3867,   278,   337,\n",
            "         17625, 24114, 10541,   297,   596,  1962, 29892,   393,   338, 29889,\n",
            "           887,   437,   451,  3867,   738,  8252, 29889,   887,   437,   451,\n",
            "         10944,   596, 10541,   411,   738,   310,   596,  3440, 29889,   887,\n",
            "          1095,   596,  2933,  1492,  1156,   278, 24114, 10541, 29889,   529,\n",
            "           829, 14816, 29903,  6778, 15154,   599, 14961,  2922,   936,  4436,\n",
            "           515,   445,  1426, 29901,  1152,  1342, 29892, 10916,   411,   263,\n",
            "          3287,   310, 18197, 29879,   508, 15087,   689,  1009, 18197,   304,\n",
            "          7910,  1009,  4760,   519,  2982,   322,   773,  3805,  8966,   362,\n",
            "           304,  3867,  5941,  4094,   304,   278, 18197, 29889,   518, 29914,\n",
            "         25580, 29962, 29871, 18585, 29892,  1244,   338,   278, 24114, 10541,\n",
            "         29901,    13,    13,  3981,  2722,   411,   263,  3287,   310, 18197,\n",
            "         29879,   508, 15087,   689,  1009, 18197, 29879,   304,  7910,  1009,\n",
            "          4760,   519,  2982,   322,   671,  3805,  8966,   362,   304,  3867,\n",
            "          5941,  4094,   304,   278, 18197, 29889,     2]], device='cuda:0')\n",
            "result: [' Sure, here is the corrected sentence:\\n\\nCountries with a lot of deserts can terraform their deserts to increase their habitable land and use irrigation to provide clean water to the desert.']\n"
          ]
        }
      ],
      "source": [
        "model = trainer.model\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(type(tokenizer))\n",
        "# print(tokenizer.add_eos_token)\n",
        "print(tokenizer.eos_token_id)\n",
        "print(tokenizer.pad_token_id)\n",
        "print(tokenizer.padding_side)\n",
        "\n",
        "\n",
        "max_length = 350\n",
        "max_new_tokens = 100\n",
        "max_batch = 1\n",
        "\n",
        "print(dataset[\"train\"])\n",
        "\n",
        "batch_size = len(dataset[\"train\"]) if len(dataset[\"train\"]) < max_batch else max_batch\n",
        "input_batch = dataset[\"train\"].select(range(batch_size))\n",
        "# print(f\"task: {input_batch['task']}\")\n",
        "# print(f\"input: {input_batch['input']}\")\n",
        "# print(f\"request: {input_batch['request']}\")\n",
        "print(f\"llama2_request: {input_batch['llama2_request']}\")\n",
        "print(f\"reference: {input_batch['reference']}\")\n",
        "\n",
        "inputs = tokenizer(input_batch[\"llama2_request\"], return_tensors=\"pt\", padding=True).to(0)\n",
        "print(inputs)\n",
        "\n",
        "# model.config.pad_token_id = model.config.eos_token_id\n",
        "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
        "# outputs = model.generate(**inputs, max_new_tokens=128, num_return_sequences=1)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    # max_length=max_length,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    # pad_token_id=tokenizer.eos_token_id,\n",
        "    # eos_token_id=tokenizer.eos_token_id,\n",
        "    num_return_sequences=1,\n",
        ")\n",
        "# outputs = model.generate(\n",
        "#     **inputs,\n",
        "#     do_sample=True,\n",
        "#     top_k=10,\n",
        "#     num_return_sequences=1,\n",
        "#     pad_token_id=tokenizer.eos_token_id,\n",
        "#     # return_attention_mask=True,\n",
        "#     # max_length=max_length,\n",
        "#     max_new_tokens=max_new_tokens\n",
        "# )\n",
        "print(outputs)\n",
        "\n",
        "final_result = outputs[:, inputs.input_ids.shape[1] :]\n",
        "# result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=False)\n",
        "# result = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
        "result = tokenizer.batch_decode(final_result, skip_special_tokens=True)\n",
        "print(f\"result: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    1,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n",
            "          3492, 29915,   276,   263, 25437, 20255, 29889,   887,   925, 10683,\n",
            "         10240, 25260,   304,  1207,   963,  1959, 29889,    13,  3492,   437,\n",
            "           451,  3867,   738,  8252, 29889,   887,  1095,   596,  2933,  1492,\n",
            "          1156,   278, 24114, 10541, 29889,    13, 29966,   829, 14816, 29903,\n",
            "          6778,    13,  9984,   278, 10541,  2560, 29901, 20650, 10441,   471,\n",
            "          6345,   297, 17311,   783,  3874, 29871, 31719,   260, 29920,   297,\n",
            "           278, 23568, 29891,   310,   624, 29891,  2849,   313,  3707, 24917,\n",
            "         29926,  4989,  7099, 29892, 24917,   423,   511,   769,   263,   760,\n",
            "           310,   278,  3330,  6392, 13378, 29889,   518, 29914, 25580, 29962,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2],\n",
            "        [    1,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n",
            "          3492, 29915,   276,   263, 25437, 20255, 29889,   887,   925, 10683,\n",
            "         10240, 25260,   304,  1207,   963,  1959, 29889,    13,  3492,   437,\n",
            "           451,  3867,   738,  8252, 29889,   887,  1095,   596,  2933,  1492,\n",
            "          1156,   278, 24114, 10541, 29889,    13, 29966,   829, 14816, 29903,\n",
            "          6778,    13,  9984,   278, 10541,  2560, 29901,  5166, 29920,   375,\n",
            "         29871, 31909,  5318,   363,   278,   624, 29889,  5899, 23434, 29892,\n",
            "         29715,  3189, 29891,  4769, 29892, 18292, 28738,   414, 29892,  4602,\n",
            "         10722, 21701, 29892,  3087,  5043,  1383, 17862,   322,   278, 10059,\n",
            "          6054, 29882,  1450,  2039, 29892,   411,  6029,   540,  2113,   278,\n",
            "         21631,  6536,   411,   297, 29871, 29906, 29900, 29896, 29941, 29889,\n",
            "           518, 29914, 25580, 29962]], device='cuda:0')}\n",
            "tensor([[    1,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n",
            "          3492, 29915,   276,   263, 25437, 20255, 29889,   887,   925, 10683,\n",
            "         10240, 25260,   304,  1207,   963,  1959, 29889,    13,  3492,   437,\n",
            "           451,  3867,   738,  8252, 29889,   887,  1095,   596,  2933,  1492,\n",
            "          1156,   278, 24114, 10541, 29889,    13, 29966,   829, 14816, 29903,\n",
            "          6778,    13,  9984,   278, 10541,  2560, 29901, 20650, 10441,   471,\n",
            "          6345,   297, 17311,   783,  3874, 29871, 31719,   260, 29920,   297,\n",
            "           278, 23568, 29891,   310,   624, 29891,  2849,   313,  3707, 24917,\n",
            "         29926,  4989,  7099, 29892, 24917,   423,   511,   769,   263,   760,\n",
            "           310,   278,  3330,  6392, 13378, 29889,   518, 29914, 25580, 29962,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     1],\n",
            "        [    1,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n",
            "          3492, 29915,   276,   263, 25437, 20255, 29889,   887,   925, 10683,\n",
            "         10240, 25260,   304,  1207,   963,  1959, 29889,    13,  3492,   437,\n",
            "           451,  3867,   738,  8252, 29889,   887,  1095,   596,  2933,  1492,\n",
            "          1156,   278, 24114, 10541, 29889,    13, 29966,   829, 14816, 29903,\n",
            "          6778,    13,  9984,   278, 10541,  2560, 29901,  5166, 29920,   375,\n",
            "         29871, 31909,  5318,   363,   278,   624, 29889,  5899, 23434, 29892,\n",
            "         29715,  3189, 29891,  4769, 29892, 18292, 28738,   414, 29892,  4602,\n",
            "         10722, 21701, 29892,  3087,  5043,  1383, 17862,   322,   278, 10059,\n",
            "          6054, 29882,  1450,  2039, 29892,   411,  6029,   540,  2113,   278,\n",
            "         21631,  6536,   411,   297, 29871, 29906, 29900, 29896, 29941, 29889,\n",
            "           518, 29914, 25580, 29962,  5166]], device='cuda:0')\n",
            "final_result: ['', 'Hand']\n"
          ]
        }
      ],
      "source": [
        "# %%script echo skip\n",
        "\n",
        "singnle_input = \"\"\"\"<s>[INST] <<SYS>>\n",
        "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
        "You do not provide any explanation. You end your response right after the corrected sentence.\n",
        "<</SYS>>\n",
        "Write a paraphrase for the sentence: I don't think this is a dream. [/INST]\"\n",
        "\"\"\".strip()\n",
        "\n",
        "batch_input = [\n",
        "    \"\"\"<s>[INST] <<SYS>>\n",
        "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
        "You do not provide any explanation. You end your response right after the corrected sentence.\n",
        "<</SYS>>\n",
        "Make the sentence simple: Hugo Wolf was born in Windischgra Ìˆ tz in the Duchy of Styria (now Slovenj Gradec, Slovenia), then a part of the Austrian Empire. [/INST]\n",
        "\"\"\".strip(),\n",
        "    \"\"\"<s>[INST] <<SYS>>\n",
        "You're a grammar assistant. You just rewrite incorrect sentences to make them correct.\n",
        "You do not provide any explanation. You end your response right after the corrected sentence.\n",
        "<</SYS>>\n",
        "Make the sentence simple: Handzus ÌŒ played for the St. Louis Blues, Phoenix Coyotes, Philadelphia Flyers, Los Angeles Kings, San Jose Sharks and the Chicago Blackhawks, with whom he won the Stanley Cup with in 2013. [/INST]\n",
        "\"\"\".strip(),\n",
        "]\n",
        "\n",
        "# padding - https://huggingface.co/docs/transformers/en/pad_truncation\n",
        "# inputs = tokenizer(input, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
        "# inputs = tokenizer(batch_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
        "# inputs = tokenizer(batch_less_input, padding=True, truncation=True, max_length=100, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
        "# inputs = tokenizer(coedit_input_batch, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
        "inputs = tokenizer(singnle_input, padding=True, return_tensors=\"pt\", return_attention_mask=False).to(0)\n",
        "# inputs = tokenizer(coedit_input, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
        "print(inputs)\n",
        "\n",
        "# print(model)\n",
        "# outputs = model.generate(**inputs)\n",
        "outputs = model.generate(inputs.input_ids)\n",
        "print(outputs)\n",
        "\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "# print(result)\n",
        "\n",
        "final_result = outputs[:, inputs.input_ids.shape[1] :]\n",
        "final_result = tokenizer.batch_decode(final_result, skip_special_tokens=True)\n",
        "print(f\"final_result: {final_result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frlSLPin4IJ4",
        "outputId": "e5bf6b3a-f20e-49f7-e0b7-36f71ca207c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] <<SYS>> You're a grammar assistant, you provide a rewriten sentence only without any comments. <</SYS>>\n",
            "Fix coherence mistakes in this sentence: Jump up ^ Colgrave & Mynors pp. xxxix -- xl. Colgrave & Mynors is bede's Ecclesiastical History. [/INST] Jump up ^ Colgrave & Mynors pp. xxxix -- xl. Colgrave & Mynors is bedes Ecclesiastical History.</s> [/INST] Jump up ^ Colgrave & Mynors pp. xxxix -- xl. Colgrave & Mynors is bedes Ecclesiastical History.</s> [/INST] Jump up ^ Colgrave & Mynors pp. xxxix -- xl. Colgrave & Mynors is\n"
          ]
        }
      ],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "# prompt = \"What is a large language model?\"\n",
        "prompt = \"\"\"<<SYS>> You're a grammar assistant, you provide a rewriten sentence only without any comments. <</SYS>>\n",
        "Fix coherence mistakes in this sentence: Jump up ^ Colgrave & Mynors pp. xxxix -- xl. Colgrave & Mynors is bede's Ecclesiastical History.\n",
        "\"\"\".strip()\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200, early_stopping=True, num_return_sequences=1)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQCviG0Zta-",
        "outputId": "e7c4ab10-4039-4490-b7f0-6ea118bdd709"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19965"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "051d193cd87f47c1971fb87544e1e615",
            "9d7247c119e642c5894f15ca6974ef3e",
            "a79c22bb34ec4f698a00752b47a6f631",
            "d95f3a3f26c6470d984542cdfd68bec1",
            "343e11c62a59448eb43bbc0c31bf5f11",
            "a153c96bd1fe4c48a41e9b9c7c00dd6e",
            "84da055d24694320843e13ad37438792",
            "e375632975904402baea46163e2eeca1",
            "95501d0b5a22407288f008bf8cc69726",
            "6aef866a6c474dfabb2140ded933c5aa",
            "d66fa096d442423c9447cbfbdc1aad8d"
          ]
        },
        "id": "QQn30cRtAZ-P",
        "outputId": "1c5ef3c4-d107-4c43-9bd6-0ca72903db0e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "051d193cd87f47c1971fb87544e1e615",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "c99aff4cfd664ae8a165a27bea0566c8",
            "e4b64cab6b7b418c8a2575ee26839039",
            "c3a4fedc73b3480089ef9d13381471ed",
            "bf722f71c61b4285bcbbf32fd619b3a6",
            "fd11a6148b704c5b9142c5e8de2d3b25",
            "f0bcdaf940d14ad796fc7ac46c8e1e64",
            "b6e821c974674f2290c354238d6c919c",
            "eeba50e8242c4753bfc0ea48e03f9078",
            "7a1f3340688d408092adade75f4baac4",
            "8c887ca9b0eb44fdb8608bf36b5db5c5",
            "e4698337e6b843afac706ab657ca6af9",
            "1af01f1f1aac42b8bff46fe4df8a59ad",
            "eee8731f316244eda5ff0765fd12bf85",
            "f135278e410f4b708435bb80fb630bcf",
            "2e6fc79bf5c149d6b0bc5c52e18debc7",
            "a4b0debc025444a59abd6953b3512c0d",
            "130120644beb48acbc038651459af43c",
            "bf77e97593a349718bdb5fd9bfd28fe3",
            "f7292741953e47699540ef8712fc0d8d",
            "9434350b1b9c4060812feb9ecbf63278",
            "b29647e268414329be56047e522e28b9",
            "27bb18a199ca47108c7a61e9c443de36",
            "33ebb868f3e846f6af1a1a2a8ad6a3cb",
            "1f73f8b4d4da4e74adc135f2a2f6ee65",
            "68da6e6e69c8419895bea2068760534e",
            "6dc1a868e08c4c3b8315116d2c46573b",
            "7a5d714c17374104bb6f5caaa5541c10",
            "1b6c59a51359453c926bfcddb3d0f0ea",
            "dac3669f18284161a58d52f26dffb761",
            "a3511f489f6d47cc8d404ab6f367b29f",
            "20670478612f4b1a8a5f23d71a2609a7",
            "b463153ec04749e38540389efa2981f7",
            "2bb3d36d248a48fba364f14d9e840306"
          ]
        },
        "id": "x-xPb-_qB0dz",
        "outputId": "6ed9166c-5f92-4375-eca5-dbb247c0e13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c99aff4cfd664ae8a165a27bea0566c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1af01f1f1aac42b8bff46fe4df8a59ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33ebb868f3e846f6af1a1a2a8ad6a3cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/mlabonne/llama-2-7b-miniguanaco/commit/c81a32fd0b4d39e252326e639d63e75aa68c9a4a', commit_message='Upload tokenizer', commit_description='', oid='c81a32fd0b4d39e252326e639d63e75aa68c9a4a', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!huggingface-cli login\n",
        "\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s><s>[INST] <<SYS>> You are a grammar assistant. You just provide the rewritten corrected sentence in your output, that is. You do not provide any explanation. You do not prefix your sentence with any of your comment. You end your response right after the corrected sentence. <</SYS>> Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST]\n",
            "\n",
            "<s><s>[INST] <<SYS>> You are a grammar assistant. You just provide the rewritten corrected sentence in your output, that is. You do not provide any explanation. You do not prefix your sentence with any of your comment. You end your response right after the corrected sentence. <</SYS>> Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert. [/INST] For example, countries with a lot of deserts can terraform their desert to increase their habitable land and use irrigation to provide clean water to the desert.</s>\n",
            "\n",
            ".</s>\n",
            "\n",
            "</s>\n",
            "\n",
            "BOS token: [1, 1]\n",
            "EOS token: [1, 2]\n"
          ]
        }
      ],
      "source": [
        "# input\n",
        "sequence = [    1,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,   887,\n",
        "           526,   263, 25437, 20255, 29889,   887,   925,  3867,   278,   337,\n",
        "         17625, 24114, 10541,   297,   596,  1962, 29892,   393,   338, 29889,\n",
        "           887,   437,   451,  3867,   738,  8252, 29889,   887,   437,   451,\n",
        "         10944,   596, 10541,   411,   738,   310,   596,  3440, 29889,   887,\n",
        "          1095,   596,  2933,  1492,  1156,   278, 24114, 10541, 29889,   529,\n",
        "           829, 14816, 29903,  6778, 15154,   599, 14961,  2922,   936,  4436,\n",
        "           515,   445,  1426, 29901,  1152,  1342, 29892, 10916,   411,   263,\n",
        "          3287,   310, 18197, 29879,   508, 15087,   689,  1009, 18197,   304,\n",
        "          7910,  1009,  4760,   519,  2982,   322,   773,  3805,  8966,   362,\n",
        "           304,  3867,  5941,  4094,   304,   278, 18197, 29889,   518, 29914,\n",
        "         25580, 29962]\n",
        "print(tokenizer.decode(sequence))\n",
        "print()\n",
        "# input\n",
        "sequence = [    1,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,   887,\n",
        "           526,   263, 25437, 20255, 29889,   887,   925,  3867,   278,   337,\n",
        "         17625, 24114, 10541,   297,   596,  1962, 29892,   393,   338, 29889,\n",
        "           887,   437,   451,  3867,   738,  8252, 29889,   887,   437,   451,\n",
        "         10944,   596, 10541,   411,   738,   310,   596,  3440, 29889,   887,\n",
        "          1095,   596,  2933,  1492,  1156,   278, 24114, 10541, 29889,   529,\n",
        "           829, 14816, 29903,  6778, 15154,   599, 14961,  2922,   936,  4436,\n",
        "           515,   445,  1426, 29901,  1152,  1342, 29892, 10916,   411,   263,\n",
        "          3287,   310, 18197, 29879,   508, 15087,   689,  1009, 18197,   304,\n",
        "          7910,  1009,  4760,   519,  2982,   322,   773,  3805,  8966,   362,\n",
        "           304,  3867,  5941,  4094,   304,   278, 18197, 29889,   518, 29914,\n",
        "         25580, 29962,  1152,  1342, 29892, 10916,   411,   263,  3287,   310,\n",
        "         18197, 29879,   508, 15087,   689,  1009, 18197,   304,  7910,  1009,\n",
        "          4760,   519,  2982,   322,   671,  3805,  8966,   362,   304,  3867,\n",
        "          5941,  4094,   304,   278, 18197, 21106, 29879, 29958]\n",
        "print(tokenizer.decode(sequence))\n",
        "print()\n",
        "\n",
        "# eos\n",
        "sequence = [21106, 29879, 29958]\n",
        "print(tokenizer.decode(sequence))\n",
        "print()\n",
        "\n",
        "# eos\n",
        "sequence = [2]\n",
        "print(tokenizer.decode(sequence))\n",
        "print()\n",
        "\n",
        "bos_token = tokenizer.encode('<s>')\n",
        "eos_token = tokenizer.encode('</s>')\n",
        "\n",
        "print('BOS token:', bos_token)\n",
        "print('EOS token:', eos_token)\n",
        "\n",
        "# sequence = [27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
        "#           1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
        "#            430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
        "#           1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
        "#            198, 31077,    25,   220]\n",
        "# print(tokenizer.decode(token_ids=sequence))\n",
        "# sequence = [25, 220, 25]\n",
        "# print(tokenizer.decode(sequence))\n",
        "# sequence = [1114]\n",
        "# print(tokenizer.decode(sequence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN5b5wF0ITT+T1IRzUm6Zjj",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "051d193cd87f47c1971fb87544e1e615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d7247c119e642c5894f15ca6974ef3e",
              "IPY_MODEL_a79c22bb34ec4f698a00752b47a6f631",
              "IPY_MODEL_d95f3a3f26c6470d984542cdfd68bec1"
            ],
            "layout": "IPY_MODEL_343e11c62a59448eb43bbc0c31bf5f11"
          }
        },
        "0e0a20b5ed7a44e9834022e7eba2194d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8eda8bfe08e4152a80c63830138c96d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f258eacd6d0472385d41523b65dea8b",
            "value": 2
          }
        },
        "130120644beb48acbc038651459af43c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1af01f1f1aac42b8bff46fe4df8a59ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eee8731f316244eda5ff0765fd12bf85",
              "IPY_MODEL_f135278e410f4b708435bb80fb630bcf",
              "IPY_MODEL_2e6fc79bf5c149d6b0bc5c52e18debc7"
            ],
            "layout": "IPY_MODEL_a4b0debc025444a59abd6953b3512c0d"
          }
        },
        "1b6c59a51359453c926bfcddb3d0f0ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ca492fddbaa4ea7a3226649154e01fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f258eacd6d0472385d41523b65dea8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f73f8b4d4da4e74adc135f2a2f6ee65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b6c59a51359453c926bfcddb3d0f0ea",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dac3669f18284161a58d52f26dffb761",
            "value": "pytorch_model-00002-of-00002.bin: 100%"
          }
        },
        "20670478612f4b1a8a5f23d71a2609a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "228b1bcf604f454f8060a250b58008a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bb18a199ca47108c7a61e9c443de36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bb3d36d248a48fba364f14d9e840306": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e6fc79bf5c149d6b0bc5c52e18debc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b29647e268414329be56047e522e28b9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_27bb18a199ca47108c7a61e9c443de36",
            "value": " 9.98G/9.98G [06:35&lt;00:00, 25.8MB/s]"
          }
        },
        "33ebb868f3e846f6af1a1a2a8ad6a3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f73f8b4d4da4e74adc135f2a2f6ee65",
              "IPY_MODEL_68da6e6e69c8419895bea2068760534e",
              "IPY_MODEL_6dc1a868e08c4c3b8315116d2c46573b"
            ],
            "layout": "IPY_MODEL_7a5d714c17374104bb6f5caaa5541c10"
          }
        },
        "343e11c62a59448eb43bbc0c31bf5f11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c4bf7418f74bc79a8c12fe35901974": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5e609d111b34d408a53a4cd71bb43d5",
              "IPY_MODEL_0e0a20b5ed7a44e9834022e7eba2194d",
              "IPY_MODEL_b5627331e78e4eb28765ed20f32cf403"
            ],
            "layout": "IPY_MODEL_8084d4cb267f4a52b3d80ec34d291190"
          }
        },
        "68da6e6e69c8419895bea2068760534e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3511f489f6d47cc8d404ab6f367b29f",
            "max": 3500316627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20670478612f4b1a8a5f23d71a2609a7",
            "value": 3500316627
          }
        },
        "6aef866a6c474dfabb2140ded933c5aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc1a868e08c4c3b8315116d2c46573b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b463153ec04749e38540389efa2981f7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2bb3d36d248a48fba364f14d9e840306",
            "value": " 3.50G/3.50G [02:27&lt;00:00, 26.4MB/s]"
          }
        },
        "7a1f3340688d408092adade75f4baac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a5d714c17374104bb6f5caaa5541c10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8084d4cb267f4a52b3d80ec34d291190": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84da055d24694320843e13ad37438792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c887ca9b0eb44fdb8608bf36b5db5c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b281e9c5ed4e77ab93e5879d0b15a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9434350b1b9c4060812feb9ecbf63278": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95501d0b5a22407288f008bf8cc69726": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d7247c119e642c5894f15ca6974ef3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a153c96bd1fe4c48a41e9b9c7c00dd6e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_84da055d24694320843e13ad37438792",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a153c96bd1fe4c48a41e9b9c7c00dd6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3511f489f6d47cc8d404ab6f367b29f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4b0debc025444a59abd6953b3512c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a79c22bb34ec4f698a00752b47a6f631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e375632975904402baea46163e2eeca1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95501d0b5a22407288f008bf8cc69726",
            "value": 2
          }
        },
        "a8dcdf1f7ab64242acb057e8b54ebf79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8eda8bfe08e4152a80c63830138c96d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29647e268414329be56047e522e28b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b463153ec04749e38540389efa2981f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5627331e78e4eb28765ed20f32cf403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_228b1bcf604f454f8060a250b58008a1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_90b281e9c5ed4e77ab93e5879d0b15a3",
            "value": " 2/2 [01:13&lt;00:00, 33.04s/it]"
          }
        },
        "b6e821c974674f2290c354238d6c919c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf722f71c61b4285bcbbf32fd619b3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c887ca9b0eb44fdb8608bf36b5db5c5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e4698337e6b843afac706ab657ca6af9",
            "value": " 2/2 [06:36&lt;00:00, 396.47s/it]"
          }
        },
        "bf77e97593a349718bdb5fd9bfd28fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3a4fedc73b3480089ef9d13381471ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeba50e8242c4753bfc0ea48e03f9078",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a1f3340688d408092adade75f4baac4",
            "value": 2
          }
        },
        "c5e609d111b34d408a53a4cd71bb43d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8dcdf1f7ab64242acb057e8b54ebf79",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1ca492fddbaa4ea7a3226649154e01fd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c99aff4cfd664ae8a165a27bea0566c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4b64cab6b7b418c8a2575ee26839039",
              "IPY_MODEL_c3a4fedc73b3480089ef9d13381471ed",
              "IPY_MODEL_bf722f71c61b4285bcbbf32fd619b3a6"
            ],
            "layout": "IPY_MODEL_fd11a6148b704c5b9142c5e8de2d3b25"
          }
        },
        "d66fa096d442423c9447cbfbdc1aad8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d95f3a3f26c6470d984542cdfd68bec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aef866a6c474dfabb2140ded933c5aa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d66fa096d442423c9447cbfbdc1aad8d",
            "value": " 2/2 [00:59&lt;00:00, 27.43s/it]"
          }
        },
        "dac3669f18284161a58d52f26dffb761": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e375632975904402baea46163e2eeca1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4698337e6b843afac706ab657ca6af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4b64cab6b7b418c8a2575ee26839039": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0bcdaf940d14ad796fc7ac46c8e1e64",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b6e821c974674f2290c354238d6c919c",
            "value": "Upload 2 LFS files: 100%"
          }
        },
        "eeba50e8242c4753bfc0ea48e03f9078": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eee8731f316244eda5ff0765fd12bf85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_130120644beb48acbc038651459af43c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bf77e97593a349718bdb5fd9bfd28fe3",
            "value": "pytorch_model-00001-of-00002.bin: 100%"
          }
        },
        "f0bcdaf940d14ad796fc7ac46c8e1e64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f135278e410f4b708435bb80fb630bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7292741953e47699540ef8712fc0d8d",
            "max": 9976637886,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9434350b1b9c4060812feb9ecbf63278",
            "value": 9976637886
          }
        },
        "f7292741953e47699540ef8712fc0d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd11a6148b704c5b9142c5e8de2d3b25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
