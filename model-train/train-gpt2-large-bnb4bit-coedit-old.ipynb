{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml\n",
    "# %pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gpt2-large,model_id: openai-community/gpt2-large,model_path: openai-community_gpt2-large\n",
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_new_tokens\": 350,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"padding_side\": \"left\"\n",
      "}\n",
      "\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1280)\n",
      "    (wpe): Embedding(1024, 1280)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-35): 36 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "model_repo = f\"openai-community\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "model.generation_config.max_new_tokens = 350\n",
    "# model.generation_config.new_tokens = 350\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set {'neutralize', 'paraphrase', 'coherence', 'clarity', 'gec', 'simplification'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 14155\n",
      "})\n",
      "test set {'neutralize', 'paraphrase', 'coherence', 'clarity', 'gec', 'simplification'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 711\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 14155\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 711\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'], 'request': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'prompt': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse: For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "# print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "# train_ratio = 0.01\n",
    "# test_ratio = 0.001\n",
    "train_ratio = 0.2\n",
    "test_ratio = 0.01\n",
    "# train_ratio = 0.9\n",
    "# test_ratio = 0.1\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "train_gec_dataset = gec_dataset.select(range(0, int(train_ratio * len(gec_dataset))))\n",
    "test_gec_dataset = gec_dataset.select(range(int((1 - test_ratio) * len(gec_dataset)), len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, int(train_ratio * len(simplification_dataset))))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(simplification_dataset)), len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, int(train_ratio * len(clarity_dataset))))\n",
    "test_clarity_dataset = clarity_dataset.select(range(int((1 - test_ratio) * len(clarity_dataset)), len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, int(train_ratio * len(coherence_dataset))))\n",
    "test_coherence_dataset = coherence_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(coherence_dataset)), len(coherence_dataset))\n",
    ")\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, int(train_ratio * len(paraphrase_dataset))))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(paraphrase_dataset)), len(paraphrase_dataset))\n",
    ")\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(train_ratio * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, int(train_ratio * len(neutralize_dataset))))\n",
    "test_neutralize_dataset = neutralize_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(neutralize_dataset)), len(neutralize_dataset))\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "print(train_dataset)\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "def add_prompt(item):\n",
    "    return {\n",
    "        \"request\": f\"{item['input']}\\nResponse:\",\n",
    "        \"prompt\": f\"{item['input']}\\nResponse: {item['reference']}\",\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "dataset = dataset.map(add_prompt)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_length train: 144\n",
      "max_input_length test: 239\n"
     ]
    }
   ],
   "source": [
    "# find the longest sequence in the dataset\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"train\"])\n",
    "print(f\"max_input_length train: {max_input_length}\")\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"test\"])\n",
    "print(f\"max_input_length test: {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/neutralize: 2254\n",
      "train/paraphrase: 3179\n",
      "train/coherence: 2123\n",
      "train/clarity: 250\n",
      "train/gec: 4061\n",
      "train/simplification: 2288\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    train_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    train_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "train_dataset_map = {}\n",
    "for task, list in train_lists_map.items():\n",
    "    train_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(train_dataset_map)\n",
    "\n",
    "train_dataset_dict = DatasetDict(train_dataset_map)\n",
    "# print(train_dataset_dict)\n",
    "\n",
    "# for task, ds in train_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"train/{task}: {len(train_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/neutralize: 113\n",
      "test/paraphrase: 159\n",
      "test/coherence: 107\n",
      "test/clarity: 13\n",
      "test/gec: 204\n",
      "test/simplification: 115\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    test_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    test_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "test_dataset_map = {}\n",
    "for task, list in test_lists_map.items():\n",
    "    test_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(test_dataset_map)\n",
    "\n",
    "test_dataset_dict = DatasetDict(test_dataset_map)\n",
    "# print(test_dataset_dict)\n",
    "\n",
    "# for task, ds in test_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"test/{task}: {len(test_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "50256\n",
      "left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687d318e42e549c3b512cb6545be4b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cc5564e71c47bb8186f6ea3bf193f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14155\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 711\n",
      "    })\n",
      "})\n",
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response: For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
      "73 [27914, 477, 14599, 44935, 8563, 422, 428, 2420, 25, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 1059, 430, 687, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 1262, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 198, 31077, 25, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 6121, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 779, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 50256]\n",
      "73 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "73 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 6121, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 779, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, -100]\n",
      "Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\n",
      "Response: As the number of people grows, the need for a habitable environment is unquestionably increasing.\n",
      "Improve the grammaticality of this sentence: Besides some technologically determinists that allow the development of biometric identification, this technology is also shaped by three social factors, namely, the desire of the society for safety, convenience and economy.\n",
      "Response: Besides some technological determinists that allow the development of biometric identification, this technology is also shaped by three social factors, namely, the desire of society for safety, convenience, and economy.\n",
      "{'input_ids': tensor([[27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
      "          1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
      "           430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
      "          1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
      "           198, 31077,    25,  1114,  1672,    11,  2678,   351,   257,  1256,\n",
      "           286, 45288,   460,  6121,   511, 10326,   284,  2620,   511, 49055,\n",
      "          1956,   290,   779, 35425,   284,  2148,  3424,  1660,   284,   262,\n",
      "         10326,    13, 50256],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 47531,   262,\n",
      "         14599, 44935,   414,    25,  1081,   262,  1271,   286,   661, 13676,\n",
      "            11,   262,   761,   286, 49055,  2858,   318, 38766,  1346,  6393,\n",
      "            13,   198, 31077,    25,  1081,   262,  1271,   286,   661, 13676,\n",
      "            11,   262,   761,   329,   257, 49055,  2858,   318, 38766,  1346,\n",
      "          3649,    13, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  1114,  1672,    11,  2678,   351,   257,  1256,\n",
      "           286, 45288,   460,  6121,   511, 10326,   284,  2620,   511, 49055,\n",
      "          1956,   290,   779, 35425,   284,  2148,  3424,  1660,   284,   262,\n",
      "         10326,    13,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  1081,   262,  1271,   286,   661, 13676,\n",
      "            11,   262,   761,   329,   257, 49055,  2858,   318, 38766,  1346,\n",
      "          3649,    13,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "max_length = 350\n",
    "\n",
    "\n",
    "def process_dataset(batch):\n",
    "    model_inputs = tokenizer(batch[\"prompt\"], max_length=max_length)\n",
    "    model_reponses = tokenizer(batch[\"reference\"], max_length=max_length)\n",
    "\n",
    "    new_input_ids = []\n",
    "    new_labels = []\n",
    "    for input_ids, response_ids in zip(model_inputs.input_ids, model_reponses.input_ids):\n",
    "        # debug_labels = input_ids[-len(response_ids) :]\n",
    "        # print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "        # print(tokenizer.decode(debug_labels, skip_special_tokens=False))\n",
    "\n",
    "        num_tokens_ignore = len(input_ids) - len(response_ids)\n",
    "        labels = [-100] * num_tokens_ignore + input_ids[-len(response_ids) :]\n",
    "        labels.append(-100)\n",
    "        # labels.append(tokenizer.eos_token_id)\n",
    "        new_labels.append(labels)\n",
    "\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "        new_input_ids.append(input_ids)\n",
    "\n",
    "    new_attention_mask = []\n",
    "    for attention_mask in model_inputs.attention_mask:\n",
    "        attention_mask.append(0)\n",
    "        # attention_mask.append(1)\n",
    "        new_attention_mask.append(attention_mask)\n",
    "\n",
    "    # labels = tokenizer(text_target=examples[\"tgt\"], max_length=1024, padding=True).input_ids\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs[\"input_ids\"] = new_input_ids\n",
    "    model_inputs[\"attention_mask\"] = new_attention_mask\n",
    "    model_inputs[\"labels\"] = new_labels\n",
    "\n",
    "    # print(\n",
    "    #     f\">> input_ids: {len(model_inputs['input_ids'])},\"\n",
    "    #     \"attention_mask: {len(model_inputs['attention_mask'])},\"\n",
    "    #     \"labels: {len(model_inputs['labels'])}\"\n",
    "    # )\n",
    "    # count = 0\n",
    "    # for input_ids, labels, attention_masks in zip(new_input_ids, new_labels, new_attention_mask):\n",
    "    #     count += 1\n",
    "    #     if count > 3:\n",
    "    #         break\n",
    "    #     print(f\">> input_ids: {len(input_ids)}, attention_mask: {len(attention_masks)}, labels: {len(labels)}\")\n",
    "    #     print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "    #     print(labels)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# processed_dataset = data.map(preprocess_function, batched=True, batch_size=10, remove_columns=data[\"train\"].column_names)\n",
    "processed_dataset = dataset.map(\n",
    "    process_dataset, batched=True, batch_size=10, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "# processed_dataset = dataset.map(process_dataset, batched=True, batch_size=10)\n",
    "print(processed_dataset)\n",
    "print(dataset[\"train\"][0][\"prompt\"])\n",
    "# print(dataset[\"train\"][0][\"reference\"])\n",
    "print(len(processed_dataset[\"train\"][\"input_ids\"][0]), processed_dataset[\"train\"][\"input_ids\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"attention_mask\"][0]), processed_dataset[\"train\"][\"attention_mask\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"labels\"][0]), processed_dataset[\"train\"][\"labels\"][0])\n",
    "print(dataset[\"train\"][1][\"prompt\"])\n",
    "# print(dataset[\"train\"][1][\"reference\"])\n",
    "print(dataset[\"train\"][2][\"prompt\"])\n",
    "# print(dataset[\"train\"][2][\"reference\"])\n",
    "\n",
    "# print(len(processed_dataset[\"test\"][\"input_ids\"][0]), processed_dataset[\"test\"][\"input_ids\"][0])\n",
    "# print(len(processed_dataset[\"test\"][\"attention_mask\"][0]), processed_dataset[\"test\"][\"attention_mask\"][0])\n",
    "# print(len(processed_dataset[\"test\"][\"labels\"][0]), processed_dataset[\"test\"][\"labels\"][0])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", model=model)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "dataloader = DataLoader(processed_dataset[\"train\"], batch_size=2, collate_fn=data_collator)\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "350\n",
      "\n",
      ">> neutralize\n",
      "input: [\"Remove POV: this ecotopia, like callenbach's, is characterized culturally by its environmental sensibilities and focus on 'quality of life', and economically by its focus on renewable resources such as hydropower and forestry.\", 'Make this paragraph more neutral: raymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.']\n",
      "request: [\"Remove POV: this ecotopia, like callenbach's, is characterized culturally by its environmental sensibilities and focus on 'quality of life', and economically by its focus on renewable resources such as hydropower and forestry.\\nResponse:\", 'Make this paragraph more neutral: raymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nResponse:']\n",
      "reference: [\"this ecotopia, like callenbach's, is characterized culturally by its environmental sensibilities and focus on 'quality of life', and economically by its focus on resources such as hydropower and forestry.\", 'raymond delves into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.']\n",
      "result: [\" the ecotopia is a concept that has been used to describe a variety of ecotopias, including the 'ecotopia of the future' and the 'ecotopia of the present'. The ecotopia is a concept that is often used to describe a variety of ecotopias, including the 'ecotopia of the future' and the 'ecotopia of the present'. The ecotopia is a concept that is often used to describe a variety of ecotopias, including the 'ecotopia of the future' and the 'ecotopia of the present'. The ecotopia is a concept that is often used to describe a variety of ecotopias, including the 'ecotopia of the future' and the 'ecotopia of the present'. The ecotopia is a concept that is often used to describe a variety of ecotopias, including the 'ecotopia of the future' and the 'ecotopia of the present'. The ecotopia is a concept that is often used to describe a variety of ecotopias, including the 'ecotopia of the future' and the 'ecotopia of the present'. The ecotopia is a concept that is often used to describe a variety of ecotopias, including the 'ecotopia of the future' and the 'ecotopia of the present'. The ecotopia is a concept that is often used to describe a variety of ecotopias, including the 'ecotopia of the future' and the 'ecotopia of the present'. The ecotopia is a concept that is often used to describe a variety of ecotopias, including\", '\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.\\nRaymond delves deeply into the contrast between the stated aims of open source and observed behaviors, and also explores the underlying motivations of people involved in the open source movement.']\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skip\n",
    "\n",
    "max_batch = 2\n",
    "# max_length = 350\n",
    "\n",
    "print(type(model))\n",
    "print(model.generation_config.max_new_tokens)\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"input: {input_batch['input']}\")\n",
    "    print(f\"request: {input_batch['request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "    # print(f\"prompt: {input_batch['prompt']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch[\"request\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    # outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
    "    # outputs = model.generate(inputs.input_ids)\n",
    "    outputs = model.generate(**inputs, num_return_sequences=1)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    # processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(f\"result: {processed}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_model_name: gpt2-large-bnb4-coedit, training_model_id: iliazlobin/gpt2-large-bnb4-coedit, training_model_path: iliazlobin_gpt2-large-bnb4-coedit\n",
      "total/used/cuda/res/ram (Gb): 79.15/2.36/0.51/0.99/6.97\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "training_model_repo = f\"iliazlobin\"\n",
    "training_model_name: str = f\"{model_name}-bnb4-coedit\"\n",
    "training_model_id = f\"{training_model_repo}/{training_model_name}\"\n",
    "training_model_checkpoint = f\"{training_model_id}\"\n",
    "training_model_path = f\"{training_model_repo}_{training_model_name}\"\n",
    "print(\n",
    "    f\"training_model_name: {training_model_name}, \"\n",
    "    f\"training_model_id: {training_model_id}, \"\n",
    "    f\"training_model_path: {training_model_path}\"\n",
    ")\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "# actual_fraction = 0.95\n",
    "# torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "# print(\n",
    "#     f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "#     f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    "# )\n",
    "# print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 1280)\n",
      "        (wpe): Embedding(1024, 1280)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-35): 36 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=128, out_features=3840, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "              (c_proj): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total/trainable params: 443728640/23592960\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "perf_model = model\n",
    "perf_model = prepare_model_for_kbit_training(perf_model, use_gradient_checkpointing=True)\n",
    "# perf_model.gradient_checkpointing_enable()\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=128,\n",
    "    # r=64,\n",
    "    # r=8,\n",
    "    lora_alpha=32,\n",
    "    # target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "perf_model = get_peft_model(perf_model, config)\n",
    "print(type(perf_model))\n",
    "print(perf_model)\n",
    "\n",
    "total_params = sum(p.numel() for p in perf_model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in perf_model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = perf_model\n",
    "# print(type(training_model))\n",
    "# print(training_model.config)\n",
    "# print(training_model.generation_config)\n",
    "\n",
    "# total_params = sum(p.numel() for p in training_model.parameters())\n",
    "# total_trainable_params = sum(p.numel() for p in training_model.parameters() if p.requires_grad)\n",
    "# print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "# batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "# input_batch = batch.select(range(batch_size))\n",
    "# input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "# processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # print(eval_pred)\n",
    "    preds = eval_pred.predictions\n",
    "    # print(preds)\n",
    "    labels = eval_pred.label_ids\n",
    "    # print(labels)\n",
    "\n",
    "    # preds = np.argmax(preds, axis=-1)\n",
    "    # print(preds)\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    # print(preds)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # print(labels)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # print(decoded_preds)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # print(decoded_labels)\n",
    "\n",
    "    rouge_score = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    sacreblue_score = sacreblue_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # sari_score = sari_metric.compute(\n",
    "    #     sources=processed_samples[\"input\"],\n",
    "    #     predictions=processed_samples[\"processed\"],\n",
    "    #     references=processed_samples[\"references\"],\n",
    "    # )\n",
    "    em_score = em_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "\n",
    "    # report = {\n",
    "    #     \"metric\": 0,\n",
    "    # }\n",
    "    report = {\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"rougeLsum\": rouge_score[\"rougeLsum\"],\n",
    "        \"sacreblue\": sacreblue_score[\"score\"],\n",
    "        \"memory_used\": utilization[\"memory_used\"] / 1024**2,\n",
    "        \"cuda_allocated\": utilization[\"cuda_allocated\"] / 1024**2,\n",
    "        \"cuda_reserved\": utilization[\"cuda_reserved\"] / 1024**2,\n",
    "        \"ram_usage\": utilization[\"ram_usage\"] / 1024**2,\n",
    "        # \"sari.sari\": 0,\n",
    "        \"em\": em_score[\"exact_match\"],\n",
    "    }\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    report[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in report.items()}\n",
    "\n",
    "\n",
    "def logits_argmax(logits: torch.Tensor, labels):\n",
    "    # if isinstance(logits, tuple):\n",
    "    #     # Depending on the model and config, logits may contain extra tensors,\n",
    "    #     # like past_key_values, but logits always come first\n",
    "    #     logits = logits[0]\n",
    "    # return logits.argmax(dim=-1), labels\n",
    "    # logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# debug\n",
    "# trainer = Trainer(\n",
    "#     model=training_model,\n",
    "#     train_dataset=processed_dataset[\"train\"],\n",
    "#     eval_dataset=processed_dataset[\"test\"],\n",
    "#     # eval_dataset=processed_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     preprocess_logits_for_metrics=logits_argmax,\n",
    "#     args=args,\n",
    "# )\n",
    "\n",
    "# predictions = trainer.predict(processed_dataset[\"train\"].select(range(2)))\n",
    "# print(type(predictions))\n",
    "# # print(predictions)\n",
    "# print(predictions.predictions)\n",
    "# print(predictions.label_ids)\n",
    "# metrics = compute_metrics((predictions.predictions, predictions.label_ids))\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"model-{training_model_path}-train/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: model-iliazlobin_gpt2-large-bnb4-coedit-train\n",
      "train_size: 14155, batch_size: 150, per_epoch_steps: 23.591666666666665, max_steps: 100, epochs: 0.23591666666666666\n",
      "train_size: 14155, batch_size: 150, per_epoch_steps: 23.591666666666665, epochs: 2, epoch_total_steps: 47.18333333333333\n"
     ]
    }
   ],
   "source": [
    "train_path = f\"model-{training_model_path}-train\"\n",
    "print(f\"train_path: {train_path}\")\n",
    "\n",
    "train_size = len(processed_dataset[\"train\"])\n",
    "batch_size = 150\n",
    "gradient_accumulation_steps = 4\n",
    "eval_batch_size = 150\n",
    "eval_accumulation_steps = 4\n",
    "\n",
    "per_epoch_steps = train_size / (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "max_steps = 100\n",
    "epochs = per_epoch_steps / max_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, max_steps: {max_steps}, epochs: {epochs}\")\n",
    "\n",
    "# if epochs < 1:\n",
    "#     raise Exception(f\"Training doesn't cover the entire training dataset with {train_size} samples\")\n",
    "\n",
    "epochs = 2\n",
    "epoch_total_steps = epochs * per_epoch_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/training_args.py:1859: FutureWarning: `--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case iliazlobin/gpt2-large-bnb4-coedit).\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 07:14, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Sacreblue</th>\n",
       "      <th>Memory Used</th>\n",
       "      <th>Cuda Allocated</th>\n",
       "      <th>Cuda Reserved</th>\n",
       "      <th>Ram Usage</th>\n",
       "      <th>Em</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.884000</td>\n",
       "      <td>6.767493</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.743700</td>\n",
       "      <td>47681.500000</td>\n",
       "      <td>763.967300</td>\n",
       "      <td>46148.000000</td>\n",
       "      <td>6923.835900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>242.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.471928</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>60875.500000</td>\n",
       "      <td>763.844200</td>\n",
       "      <td>59342.000000</td>\n",
       "      <td>7013.156200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>242.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>6.331500</td>\n",
       "      <td>6.318489</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.722600</td>\n",
       "      <td>60875.500000</td>\n",
       "      <td>763.847200</td>\n",
       "      <td>59342.000000</td>\n",
       "      <td>7059.656200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>242.285500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=46, training_loss=6.621178419693656, metrics={'train_runtime': 442.7169, 'train_samples_per_second': 63.946, 'train_steps_per_second': 0.104, 'total_flos': 1.71515998568448e+16, 'train_loss': 6.621178419693656, 'epoch': 1.94})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=train_path,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    # warmup_ratio=0.05,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    eval_accumulation_steps=eval_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=max_steps,\n",
    "    warmup_steps=1,\n",
    "    logging_steps=15,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=15,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=15,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub_model_id=training_model_name,\n",
    "    push_to_hub_organization=\"iliazlobin\",\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # Should be false for Lora (https://github.com/kohya-ss/sd-scripts/issues/323#issuecomment-1485073421)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=training_model,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=logits_argmax,\n",
    "    args=args,\n",
    ")\n",
    "print(type(trainer))\n",
    "\n",
    "model.config.use_cache = False\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")'\n",
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "saving locally: model-iliazlobin_gpt2-large-bnb4-coedit\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../.env\n",
    "\n",
    "!huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "print(f\"saving locally: model-{training_model_path}\")\n",
    "# trainer.save_model(f\"model-{training_model_path}\")\n",
    "\n",
    "# print(f\"pushing to hub: {training_model_id}\")\n",
    "# trainer.push_to_hub(\n",
    "#     commit_message=\"complete: train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\",\n",
    "#     model_name=training_model_name,\n",
    "# )\n",
    "# trainer.push_to_hub(commit_message=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "50256\n",
      "50256\n",
      "left\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "    num_rows: 14155\n",
      "})\n",
      "task: ['gec', 'gec']\n",
      "request: ['Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\\nResponse:']\n",
      "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.']\n",
      "{'input_ids': tensor([[27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
      "          1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
      "           430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
      "          1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
      "           198, 31077,    25],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 47531,   262, 14599,\n",
      "         44935,   414,    25,  1081,   262,  1271,   286,   661, 13676,    11,\n",
      "           262,   761,   286, 49055,  2858,   318, 38766,  1346,  6393,    13,\n",
      "           198, 31077,    25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "tensor([[27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
      "          1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
      "           430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
      "          1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
      "           198, 31077,    25,   770,   318,   257,   845,   922,  1808,    13,\n",
      "           383,  3280,   318,   326,   262, 10326,   318,   257,   845,   922,\n",
      "          1295,   284,  1059,   430,   687,    13,   383,   691,  1917,   318,\n",
      "           326,   262, 10326,   318,   407,   257,   922,  1295,   284,  1059,\n",
      "           430,   687,    13,   383,   691,   835,   284,  1059,   430,   687,\n",
      "           257, 10326,   318,   284,   779,   257,  1256,   286,  1660,    13,\n",
      "           383,   691,   835,   284,   779,   257,  1256,   286,  1660,   318,\n",
      "           284,   779, 35425,    13,   383,   691,   835,   284,   779, 35425,\n",
      "           318,   284,   779,   257,  1256,   286,  1660,    13,   383,   691,\n",
      "           835,   284,   779,  1660,   318,   284,   779, 35425,    13,   383,\n",
      "           691,   835,   284,   779, 35425,   318,   284,   779,   257,  1256,\n",
      "           286,  1660,    13,   383,   691,   835,   284,   779,  1660,   318,\n",
      "           284,   779, 35425,    13,   383,   691,   835,   284,   779, 35425,\n",
      "           318,   284,   779, 35425,    13,   383,   691,   835,   284,   779,\n",
      "         35425,   318,   284,   779, 35425,    13,   383,   691,   835,   284,\n",
      "           779, 35425,   318,   284,   779, 35425,    13,   383,   691,   835,\n",
      "           284,   779, 35425,   318,   284,   779, 35425,    13,   383,   691,\n",
      "           835,   284,   779, 35425,   318,   284,   779, 35425,    13,   383,\n",
      "           691,   835,   284,   779, 35425,   318,   284,   779, 35425,    13,\n",
      "           383,   691,   835,   284,   779, 35425,   318,   284,   779, 35425,\n",
      "            13,   383,   691,   835,   284,   779, 35425,   318,   284,   779,\n",
      "         35425,    13,   383,   691,   835,   284,   779, 35425,   318,   284,\n",
      "           779, 35425,    13,   383,   691,   835,   284,   779, 35425,   318,\n",
      "           284,   779, 35425,    13,   383,   691,   835,   284,   779, 35425,\n",
      "           318,   284,   779, 35425,    13,   383,   691,   835,   284,   779,\n",
      "         35425,   318,   284,   779, 35425,    13,   383,   691,   835,   284,\n",
      "           779, 35425,   318,   284,   779, 35425,    13,   383,   691,   835,\n",
      "           284,   779, 35425,   318,   284,   779, 35425,    13,   383,   691,\n",
      "           835,   284,   779, 35425,   318,   284,   779, 35425,    13,   383,\n",
      "           691,   835,   284,   779, 35425,   318,   284,   779, 35425,    13,\n",
      "           383,   691,   835,   284,   779, 35425,   318,   284,   779, 35425,\n",
      "            13,   383,   691,   835,   284,   779, 35425,   318,   284,   779,\n",
      "         35425,    13,   383,   691,   835,   284,   779, 35425,   318,   284,\n",
      "           779, 35425,    13,   383,   691,   835,   284,   779, 35425,   318,\n",
      "           284,   779, 35425,    13,   383,   691,   835,   284,   779, 35425,\n",
      "           318,   284,   779],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 47531,   262, 14599,\n",
      "         44935,   414,    25,  1081,   262,  1271,   286,   661, 13676,    11,\n",
      "           262,   761,   286, 49055,  2858,   318, 38766,  1346,  6393,    13,\n",
      "           198, 31077,    25,   383,  1917,   286,   262, 14599, 44935,   414,\n",
      "           318,   407,   262,  1917,   286,   262,  1271,   286,   661,    13,\n",
      "           383,  1917,   318,   262,   761,   286, 49055,  2858,    13,   198,\n",
      "           464,  1917,   286,   262, 14599, 44935,   414,   318,   407,   262,\n",
      "          1917,   286,   262,  1271,   286,   661,    13,   383,  1917,   318,\n",
      "           262,   761,   286, 49055,  2858,    13,   198,   464,  1917,   286,\n",
      "           262, 14599, 44935,   414,   318,   407,   262,  1917,   286,   262,\n",
      "          1271,   286,   661,    13,   383,  1917,   318,   262,   761,   286,\n",
      "         49055,  2858,    13,   198,   464,  1917,   286,   262, 14599, 44935,\n",
      "           414,   318,   407,   262,  1917,   286,   262,  1271,   286,   661,\n",
      "            13,   383,  1917,   318,   262,   761,   286, 49055,  2858,    13,\n",
      "           198,   198,   464,  1917,   286,   262, 14599, 44935,   414,   318,\n",
      "           407,   262,  1917,   286,   262,  1271,   286,   661,    13,   383,\n",
      "          1917,   318,   262,   761,   286, 49055,  2858,    13,   198,   198,\n",
      "           464,  1917,   286,   262, 14599, 44935,   414,   318,   407,   262,\n",
      "          1917,   286,   262,  1271,   286,   661,    13,   383,  1917,   318,\n",
      "           262,   761,   286, 49055,  2858,    13,   198,   198,   464,  1917,\n",
      "           286,   262, 14599, 44935,   414,   318,   407,   262,  1917,   286,\n",
      "           262,  1271,   286,   661,    13,   383,  1917,   318,   262,   761,\n",
      "           286, 49055,  2858,    13,   198,   198,   464,  1917,   286,   262,\n",
      "         14599, 44935,   414,   318,   407,   262,  1917,   286,   262,  1271,\n",
      "           286,   661,    13,   383,  1917,   318,   262,   761,   286, 49055,\n",
      "          2858,    13,   198,   198,   464,  1917,   286,   262, 14599, 44935,\n",
      "           414,   318,   407,   262,  1917,   286,   262,  1271,   286,   661,\n",
      "            13,   383,  1917,   318,   262,   761,   286, 49055,  2858,    13,\n",
      "           198,   198,   464,  1917,   286,   262, 14599, 44935,   414,   318,\n",
      "           407,   262,  1917,   286,   262,  1271,   286,   661,    13,   383,\n",
      "          1917,   318,   262,   761,   286, 49055,  2858,    13,   198,   198,\n",
      "           464,  1917,   286,   262, 14599, 44935,   414,   318,   407,   262,\n",
      "          1917,   286,   262,  1271,   286,   661,    13,   383,  1917,   318,\n",
      "           262,   761,   286, 49055,  2858,    13,   198,   198,   464,  1917,\n",
      "           286,   262, 14599, 44935,   414,   318,   407,   262,  1917,   286,\n",
      "           262,  1271,   286,   661,    13,   383,  1917,   318,   262,   761,\n",
      "           286, 49055,  2858,    13,   198,   198,   464,  1917,   286,   262,\n",
      "         14599, 44935,   414,   318,   407,   262,  1917,   286,   262,  1271,\n",
      "           286,   661,    13]], device='cuda:0')\n",
      "result: ['Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse: This is a very good question. The answer is that the desert is a very good place to terraform. The only problem is that the desert is not a good place to terraform. The only way to terraform a desert is to use a lot of water. The only way to use a lot of water is to use irrigation. The only way to use irrigation is to use a lot of water. The only way to use water is to use irrigation. The only way to use irrigation is to use a lot of water. The only way to use water is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use irrigation. The only way to use irrigation is to use', '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\\nResponse: The problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people. The problem is the need of habitable environment.\\n\\nThe problem of the grammaticality is not the problem of the number of people.']\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "max_length = 350\n",
    "max_new_tokens = 100\n",
    "max_batch = 2\n",
    "\n",
    "print(dataset[\"train\"])\n",
    "\n",
    "batch_size = len(dataset[\"train\"]) if len(dataset[\"train\"]) < max_batch else max_batch\n",
    "input_batch = dataset[\"train\"].select(range(batch_size))\n",
    "print(f\"task: {input_batch['task']}\")\n",
    "# print(f\"input: {input_batch['input']}\")\n",
    "print(f\"request: {input_batch['request']}\")\n",
    "print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "inputs = tokenizer(input_batch[\"request\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "print(inputs)\n",
    "\n",
    "model.config.use_cache = False\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=128, num_return_sequences=1)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    # max_new_tokens=max_new_tokens,\n",
    "    # pad_token_id=tokenizer.eos_token_id,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=True,\n",
    "#     # max_length=max_length,\n",
    "#     max_new_tokens=max_new_tokens\n",
    "# )\n",
    "print(outputs)\n",
    "\n",
    "trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "# result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=False)\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "# result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> neutralize\n",
      "request: ['Make this text more neutral: chloroform \"the molecular lifesaver\" an article at oxford university providing interesting facts about chloroform.\\nResponse:', 'Remove points of view: gaming system, an dice pool system where matched die results determine success.\\nResponse:']\n",
      "reference: ['chloroform \"the molecular lifesaver\" an article at oxford university providing facts about chloroform.', 'gaming system, a unique dice pool system where matched die results determine success.']\n",
      "tensor([[12050,   428,  2420,   517,  8500,    25, 20207,    78,   687,   366,\n",
      "          1169, 18955, 25622,  8770,     1,   281,  2708,   379, 12018,  3841,\n",
      "          6403,  4955,  3499,  6419,   546, 20207,    78,   687,    13,   198,\n",
      "         31077,    25,   314,  1101,   407,  1654,   644,   345,  1612,   416,\n",
      "           366,  1169, 18955, 25622,  8770,  1911,   314,   892,   345,  1612,\n",
      "           366,  1169,  1204,    12, 29336,  2563,  1911,   198,    40,  1101,\n",
      "           407,  1654,   644,   345,  1612,   416,   366,  1169,  1204,    12,\n",
      "         29336,  2563,  1911,   314,   892,   345,  1612,   366,  1169,  1204,\n",
      "            12, 29336,  2563,  1911, 14883, 23520,   198,    40,  1101,   407,\n",
      "          1654,   644,   345,  1612,   416,   366,  1169,  1204,    12, 29336,\n",
      "          2563,  1911,   314,   892,   345,  1612,   366,  1169,  1204,    12,\n",
      "         29336,  2563,  1911, 14883, 23520,   198,    40,  1101,   407,  1654,\n",
      "           644,   345,  1612,   416,   366,  1169,  1204,    12, 29336,  2563,\n",
      "          1911,   314,   892,   345,  1612,   366,  1169,  1204,    12, 29336,\n",
      "          2563,  1911, 14883, 23520,   198,    40,  1101,   407,  1654,   644,\n",
      "           345,  1612,   416,   366,  1169,  1204,    12, 29336,  2563,  1911,\n",
      "           314,   892,   345,  1612,   366,  1169,  1204,    12, 29336,  2563,\n",
      "          1911, 14883, 23520,   198,    40,  1101,   407,  1654,   644,   345,\n",
      "          1612,   416,   366,  1169,  1204,    12, 29336,  2563,  1911,   314,\n",
      "           892,   345,  1612,   366,  1169,  1204,    12, 29336,  2563,  1911,\n",
      "         14883, 23520,   198,    40,  1101,   407,  1654,   644,   345,  1612,\n",
      "           416,   366,  1169,  1204,    12, 29336,  2563,  1911,   314,   892,\n",
      "           345,  1612,   366,  1169,  1204,    12, 29336,  2563,  1911, 14883,\n",
      "         23520,   198,    40,  1101,   407,  1654,   644,   345,  1612,   416,\n",
      "           366,  1169,  1204,    12, 29336,  2563,  1911,   314,   892,   345,\n",
      "          1612,   366,  1169,  1204,    12, 29336,  2563,  1911, 14883, 23520,\n",
      "           198,    40,  1101,   407,  1654,   644,   345,  1612,   416,   366,\n",
      "          1169,  1204,    12, 29336,  2563,  1911,   314,   892,   345,  1612,\n",
      "           366,  1169,  1204,    12, 29336,  2563,  1911, 14883, 23520,   198,\n",
      "            40,  1101,   407,  1654,   644,   345,  1612,   416,   366,  1169,\n",
      "          1204,    12, 29336,  2563,  1911,   314,   892,   345,  1612,   366,\n",
      "          1169,  1204,    12, 29336,  2563,  1911, 14883, 23520,   198,    40,\n",
      "          1101,   407,  1654,   644,   345,  1612,   416,   366,  1169,  1204,\n",
      "            12, 29336,  2563,  1911,   314,   892,   345,  1612,   366,  1169,\n",
      "          1204,    12, 29336,  2563,  1911, 14883, 23520,   198,    40,  1101,\n",
      "           407,  1654,   644,   345,  1612,   416,   366,  1169,  1204,    12,\n",
      "         29336,  2563,  1911,   314,   892,   345,  1612,   366,  1169,  1204,\n",
      "            12, 29336,  2563,  1911, 14883, 23520,   198,    40,  1101,   407,\n",
      "          1654,   644],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         27914,  2173,   286,  1570,    25,  7776,  1080,    11,   281, 17963,\n",
      "          5933,  1080,   810, 14451,  4656,  2482,  5004,  1943,    13,   198,\n",
      "         31077,    25,   198,   464,   983,   318,   257,  1049,   835,   284,\n",
      "         10400,   262,  3721,   286, 17963,    13,   632,   318,   257,  1049,\n",
      "           835,   284, 10400,   262,  3721,   286, 17963, 20354,    13,   632,\n",
      "           318,   257,  1049,   835,   284, 10400,   262,  3721,   286, 17963,\n",
      "         20354,    13,   632,   318,   257,  1049,   835,   284, 10400,   262,\n",
      "          3721,   286, 17963, 20354,    13,   632,   318,   257,  1049,   835,\n",
      "           284, 10400,   262,  3721,   286, 17963, 20354,    13,   632,   318,\n",
      "           257,  1049,   835,   284, 10400,   262,  3721,   286, 17963, 20354,\n",
      "            13,   632,   318,   257,  1049,   835,   284, 10400,   262,  3721,\n",
      "           286, 17963, 20354,    13,   632,   318,   257,  1049,   835,   284,\n",
      "         10400,   262,  3721,   286, 17963, 20354,    13,   632,   318,   257,\n",
      "          1049,   835,   284, 10400,   262,  3721,   286, 17963, 20354,    13,\n",
      "           632,   318,   257,  1049,   835,   284, 10400,   262,  3721,   286,\n",
      "         17963, 20354,    13,   632,   318,   257,  1049,   835,   284, 10400,\n",
      "           262,  3721,   286, 17963, 20354,    13,   632,   318,   257,  1049,\n",
      "           835,   284, 10400,   262,  3721,   286, 17963, 20354,    13,   632,\n",
      "           318,   257,  1049,   835,   284, 10400,   262,  3721,   286, 17963,\n",
      "         20354,    13,   632,   318,   257,  1049,   835,   284, 10400,   262,\n",
      "          3721,   286, 17963, 20354,    13,   632,   318,   257,  1049,   835,\n",
      "           284, 10400,   262,  3721,   286, 17963, 20354,    13,   632,   318,\n",
      "           257,  1049,   835,   284, 10400,   262,  3721,   286, 17963, 20354,\n",
      "            13,   632,   318,   257,  1049,   835,   284, 10400,   262,  3721,\n",
      "           286, 17963, 20354,    13,   632,   318,   257,  1049,   835,   284,\n",
      "         10400,   262,  3721,   286, 17963, 20354,    13,   632,   318,   257,\n",
      "          1049,   835,   284, 10400,   262,  3721,   286, 17963, 20354,    13,\n",
      "           632,   318,   257,  1049,   835,   284, 10400,   262,  3721,   286,\n",
      "         17963, 20354,    13,   632,   318,   257,  1049,   835,   284, 10400,\n",
      "           262,  3721,   286, 17963, 20354,    13,   632,   318,   257,  1049,\n",
      "           835,   284, 10400,   262,   262,  3721,   286, 17963, 20354,    13,\n",
      "           632,   318,   257,  1049,   835,   284, 10400,   262,  3721,   286,\n",
      "         17963, 20354,    13,   632,   318,   257,  1049,   835,   284, 10400,\n",
      "           262,  3721,   286, 17963, 20354,    13,   632,   318,   257,  1049,\n",
      "           835,   284, 10400,   262,  3721,   286, 17963, 20354,    13,   632,\n",
      "           318,   257,  1049,   835,   284, 10400,   262,  3721,   286, 17963,\n",
      "         20354,    13,   632,   318,   257,  1049,   835,   284, 10400,   262,\n",
      "          3721,   286]], device='cuda:0')\n",
      "result: [' I\\'m not sure what you mean by \"the molecular lifesaver\". I think you mean \"the life-saving drug\".\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what you mean by \"the life-saving drug\". I think you mean \"the life-saving drug\". Reply Delete\\nI\\'m not sure what', '\\nThe game is a great way to introduce the concept of dice. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of dice pools. It is a great way to introduce the concept of']\n",
      "\n",
      ">> paraphrase\n",
      "request: ['Reword this sentence: Item 5.1.2 shall be amended to read:\\nResponse:', 'Reword this text: She stopped when she saw his expression.\\nResponse:']\n",
      "reference: ['Point 5.1.2 is replaced by the following:', 'Seeing the look on his face, she paused.']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = model.config.eos_token_id\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(**input_ids, max_new_tokens=128)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     **inputs,\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     do_sample=True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     max_length=256,\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1510\u001b[0m         input_ids,\n\u001b[1;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:876\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    873\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[1;32m    890\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    897\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:489\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[0;32m--> 489\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:312\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     query, key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    314\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    315\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:458\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m active_adapter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "max_batch = 2\n",
    "max_length = 350\n",
    "\n",
    "count = 0\n",
    "# for task, batch in test_dataset_dict.items():\n",
    "for task, batch in train_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    # print(f\"input: {input_batch['input']}\")\n",
    "    print(f\"request: {input_batch['request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch[\"request\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "    print(inputs)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    # model.config.pad_token_id = model.config.eos_token_id\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "    # outputs = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "    outputs = model.generate(**inputs, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs,\n",
    "    #     do_sample=True,\n",
    "    #     top_k=10,\n",
    "    #     num_return_sequences=1,\n",
    "    #     pad_token_id=tokenizer.eos_token_id,\n",
    "    #     # return_attention_mask=True,\n",
    "    #     max_length=256,\n",
    "    # )\n",
    "    print(outputs)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    count += 1\n",
    "    if count > 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
