{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"openai-community/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=False,\n",
    ")\n",
    "print(type(tokenizer))\n",
    "\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_id,\n",
    "#     # padding_side=\"right\",\n",
    "#     # model_max_length=512,\n",
    "#     # pad_token=\"!\",\n",
    "#     # add_eos_token=True,\n",
    "#     # add_bos_token=True,\n",
    "#     # padding='longest',\n",
    "#     use_fast=True,\n",
    "#     # trust_remote_code=True,\n",
    "# )\n",
    "# print(type(tokenizer))\n",
    "\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# print(tokenizer.eos_token_id)\n",
    "# print(tokenizer.pad_token_id)\n",
    "# print(tokenizer.special_tokens_map)\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens(\n",
    "#     {\n",
    "#         \"pad_token\": \"<|endoftext|>\",\n",
    "#         \"eos_token\": \"<|endoftext|>\",\n",
    "#         \"bos_token\": \"<|endoftext|>\",\n",
    "#         \"unk_token\": \"<|endoftext|>\",\n",
    "#     }\n",
    "# )\n",
    "\n",
    "input_text = \"\"\"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs\n",
    "Response:\n",
    "\"\"\".strip()\n",
    "tokens = tokenizer(input_text, add_special_tokens=True)\n",
    "print(tokens.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix grammatical errors in this sentence: I goes to school every day.\n",
      "\n",
      "I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day. I go to school every day.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Fix grammatical errors in this sentence: I goes to school every day.\"\n",
    "# input_text = \"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\"\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "# outputs = model.generate(\n",
    "#     **input_ids,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=False,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "full_ds = load_dataset(\"Abirate/english_quotes\")\n",
    "print(full_ds)\n",
    "train_ds = load_dataset(\"Abirate/english_quotes\", split=\"train[:30%]\")\n",
    "test_ds = load_dataset(\"Abirate/english_quotes\", split=\"train[-05%:]\")\n",
    "\n",
    "processed_train_ds = train_ds.map(\n",
    "    lambda samples: tokenizer(samples[\"quote\"], max_length=512), batched=True\n",
    ")\n",
    "processed_test_ds = test_ds.map(\n",
    "    lambda samples: tokenizer(samples[\"quote\"], max_length=512), batched=True\n",
    ")\n",
    "print(processed_train_ds)\n",
    "print(processed_test_ds)\n",
    "\n",
    "# test_data = train_test_split(\n",
    "#     test_size=0.1,\n",
    "# )\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "2\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 69071\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1712\n",
      "    })\n",
      "})\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_data = load_dataset(\"grammarly/coedit\", split=\"train[:1000]\")\n",
    "test_data = load_dataset(\"grammarly/coedit\", split=\"train[:100]\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_data, \"test\": test_data})\n",
    "print(dataset)\n",
    "print(len(dataset))\n",
    "\n",
    "full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "print(full_dataset)\n",
    "print(len(full_dataset))\n",
    "\n",
    "# data[\"test\"] = test_data\n",
    "# data = load_dataset(\"grammarly/coedit\")\n",
    "# data[\"test\"] = data[\"validation\"]\n",
    "# del data[\"validation\"]\n",
    "\n",
    "# print(len(train_data))\n",
    "# print(len(data))\n",
    "\n",
    "# print(data)\n",
    "# print(data[\"train\"])\n",
    "# print(data[\"train\"][0]['src'])\n",
    "# print(data[\"test\"])\n",
    "# print(data[\"test\"][0]['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src', 'tgt', 'text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src', 'tgt', 'text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "2\n",
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response: For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
      "Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\n",
      "Response: As the number of people grows, the need for a habitable environment is unquestionably increasing.\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "# Below is an original English text. Correct English Grammar and spelling mistakes in the text.\n",
    "# \"\"\".strip()\n",
    "\n",
    "\n",
    "# def generate_instruction_prompt(input: str, response: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "#     return f\"\"\"### Instruction: {system_prompt}\n",
    "# ### Input:\n",
    "# {input.strip()}\n",
    "# ### Response:\n",
    "# {response}\n",
    "# \"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_grammar_prompt(src: str, tgt: str) -> str:\n",
    "    return f\"\"\"{src}\n",
    "Response: {tgt}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def process_data(item):\n",
    "    return {\n",
    "        \"text\": generate_grammar_prompt(item[\"src\"], item[\"tgt\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "text_dataset = dataset.map(process_data, remove_columns=[\"task\", \"_id\"])\n",
    "print(text_dataset)\n",
    "print(len(text_dataset))\n",
    "print(text_dataset[\"train\"][0][\"text\"])\n",
    "print(text_dataset[\"train\"][1][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d26c0dee39248bcad157bda789387f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01b591c5e314abfbc02b6b97206b344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src', 'tgt', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src', 'tgt', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response: For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
      "[27914, 477, 14599, 44935, 8563, 422, 428, 2420, 25, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 1059, 430, 687, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 1262, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 198, 31077, 25, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 6121, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 779, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 50256]\n",
      "[47531, 262, 14599, 44935, 414, 25, 1081, 262, 1271, 286, 661, 13676, 11, 262, 761, 286, 49055, 2858, 318, 38766, 1346, 6393, 13, 198, 31077, 25, 1081, 262, 1271, 286, 661, 13676, 11, 262, 761, 329, 257, 49055, 2858, 318, 38766, 1346, 3649, 13, 50256]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"], max_length=2048)\n",
    "    new_input_ids = []\n",
    "    for input_ids in model_inputs.input_ids:\n",
    "        new_input_ids.append(input_ids.append(tokenizer.eos_token_id))\n",
    "\n",
    "    # labels = tokenizer(text_target=examples[\"tgt\"], max_length=1024, padding=True).input_ids\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs.input_ids = new_input_ids\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# processed_data = data.map(preprocess_function, batched=True, batch_size=10, remove_columns=data[\"train\"].column_names)\n",
    "processed_dataset = text_dataset.map(\n",
    "    # preprocess_function, batched=True, batch_size=100, remove_columns=text_dataset[\"train\"].column_names\n",
    "    preprocess_function, batched=True, batch_size=100\n",
    ")\n",
    "print(processed_dataset)\n",
    "print(processed_dataset[\"train\"][0][\"text\"])\n",
    "print(processed_dataset[\"train\"][0][\"input_ids\"])\n",
    "print(processed_dataset[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total/used/cuda/res/ram (Gb): 10.00/3.38/0.48/1.04/12.82\n",
      "total/used/available memory (Gb): 10.00/3.38/6.62\n",
      "recommended/actual fraction: 0.66/0.90\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "actual_fraction = 0.90\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "trainable params: 23592960 || all params: 443728640 || trainable%: 5.316979314204285\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=32,\n",
    "    # target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print(type(model))\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'outputs/runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m      6\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain-gpt2-test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m---> 28\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mprocessed_train_ds\u001b[49m,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# eval_dataset=processed_test_ds,\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     31\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(trainer))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed_train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"train-gpt2-test\",\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=2,\n",
    "    # save_steps=50,\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    # eval_steps=1,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    report_to='tensorboard',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=processed_train_ds,\n",
    "    # eval_dataset=processed_test_ds,\n",
    "    args=args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "print(type(trainer))\n",
    "\n",
    "# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "# trainer.save_model(\"model-gpt2-large-bnb4bit-test\")\n",
    "\n",
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "trained_model = trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response: Yes.\n",
      "Response: Yes.\n",
      "Response: Yes.\n",
      "Response: Yes.\n",
      "Response: Yes.\n",
      "Response: Yes. This question has been asked many times before. It has been answered in the form of a question and response with an emphasis on the word \"yes\". However, I think that this question needs a clarification. The first question, \"yes\", is the \"yes\" of the verb meaning \"\n"
     ]
    }
   ],
   "source": [
    "# input_text = \"Fix grammatical errors in this sentence: I goes to school every day.\"\n",
    "# input_text = \"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\"\n",
    "\n",
    "# input_text = f\"\"\"Fix grammar: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "# Response:\n",
    "# \"\"\".strip()\n",
    "# input_text = f\"\"\"Fix grammar: Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "# Response: For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
    "# \"\"\".strip()\n",
    "input_text = f\"\"\"Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "trained_model.config.pad_token_id = trained_model.config.eos_token_id\n",
    "# outputs = trained_model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = trained_model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "# outputs = trained_model.generate(**input_ids, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "outputs = model.generate(\n",
    "    **input_ids,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    # return_attention_mask=True,\n",
    "    max_length=128,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 10\n",
    "total_samples = 100\n",
    "samples_map = {\n",
    "    \"fluency\": get_iterater_samples_simplified(label=\"fluency\", num_samples=total_samples),\n",
    "    \"clarity\": get_iterater_samples_simplified(label=\"clarity\", num_samples=total_samples),\n",
    "    \"coherence\": get_iterater_samples_simplified(label=\"coherence\", num_samples=total_samples),\n",
    "}\n",
    "print(samples_map)\n",
    "print(samples_map[\"fluency\"][\"task\"][0])\n",
    "print(samples_map[\"fluency\"][\"source\"][0])\n",
    "print(samples_map[\"fluency\"][\"reference\"][0])\n",
    "print(samples_map[\"fluency\"][\"task\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%load_ext autoreload\n",
    "\n",
    "batch_size = 5\n",
    "total_samples = 10\n",
    "samples_map = {\n",
    "    \"fluency\": get_iterater_samples_simplified(label=\"fluency\", num_samples=total_samples),\n",
    "    \"clarity\": get_iterater_samples_simplified(label=\"clarity\", num_samples=total_samples),\n",
    "    \"coherence\": get_iterater_samples_simplified(label=\"coherence\", num_samples=total_samples),\n",
    "}\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch[\"task\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    input_ids = tokenizer(batch[\"task\"], padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(batch['task'], return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(item['task'], return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(input_ids, max_length=512)\n",
    "    outputs = model.generate(input_ids, max_length=312)\n",
    "    # print(f\"outputs: {outputs}\")\n",
    "    processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "processed_samples_map = {}\n",
    "\n",
    "for category, samples in samples_map.items():\n",
    "    total_samples = len(samples)\n",
    "\n",
    "    print(f\"Processing {total_samples} samples for {category}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    processed_samples = samples.map(\n",
    "        model_process,\n",
    "        fn_kwargs={\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"total_samples\": total_samples,\n",
    "        },\n",
    "        num_proc=1,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        with_indices=True,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = total_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    print(f\"Finished processing {total_samples} samples for {category}.\")\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"sps: {sps_str}\"\n",
    "    )\n",
    "\n",
    "    processed_samples_map[category] = {\n",
    "        # \"category\": category,\n",
    "        # \"samples\": samples,\n",
    "        \"samples\": processed_samples,\n",
    "        \"sps\": sps,\n",
    "        \"utilization\": utilization,\n",
    "    }\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "# processed_samples = samples.map(\n",
    "#     model_process,\n",
    "#     fn_kwargs={\n",
    "#         \"model\": model,\n",
    "#         \"tokenizer\": tokenizer,\n",
    "#         \"total_samples\": total_samples,\n",
    "#     },\n",
    "#     num_proc=1,\n",
    "#     batched=True,\n",
    "#     batch_size=batch_size,\n",
    "#     with_indices=True,\n",
    "# )\n",
    "\n",
    "processed_fluency_samples = processed_samples_map[\"fluency\"][\"samples\"]\n",
    "\n",
    "pprint(processed_fluency_samples)\n",
    "pprint(processed_fluency_samples[\"processed\"][:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
