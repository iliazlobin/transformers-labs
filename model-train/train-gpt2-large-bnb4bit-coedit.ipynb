{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n",
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "model_repo = f\"openai-community\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\", max_memory={0: \"10GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=0)\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # padding_side=\"right\",\n",
    "    # model_max_length=512,\n",
    "    # pad_token=\"!\",\n",
    "    # add_eos_token=True,\n",
    "    # add_bos_token=True,\n",
    "    # padding='longest',\n",
    "    use_fast=True,\n",
    "    # trust_remote_code=True,\n",
    ")\n",
    "print(type(tokenizer))\n",
    "\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# print(tokenizer.eos_token_id)\n",
    "# print(tokenizer.pad_token_id)\n",
    "# print(tokenizer.special_tokens_map)\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens(\n",
    "#     {\n",
    "#         \"pad_token\": \"<|endoftext|>\",\n",
    "#         \"eos_token\": \"<|endoftext|>\",\n",
    "#         \"bos_token\": \"<|endoftext|>\",\n",
    "#         \"unk_token\": \"<|endoftext|>\",\n",
    "#     }\n",
    "# )\n",
    "\n",
    "tokenizer.add_eos_token = True\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "input_text = \"\"\"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs\n",
    "Response:\n",
    "\"\"\".strip()\n",
    "tokens = tokenizer(input_text, add_special_tokens=True)\n",
    "print(tokens.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "input_text = \"Fix grammatical errors in this sentence: I goes to school every day.\"\n",
    "# input_text = \"Fix grammar: In the case of young people the best way is studying as hard as possible to get better grades so in the future they will have better chance to find better jobs .\"\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "# outputs = model.generate(\n",
    "#     **input_ids,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=False,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip\n"
     ]
    }
   ],
   "source": [
    "%%script echo skip\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "full_ds = load_dataset(\"Abirate/english_quotes\")\n",
    "print(full_ds)\n",
    "train_ds = load_dataset(\"Abirate/english_quotes\", split=\"train[:30%]\")\n",
    "test_ds = load_dataset(\"Abirate/english_quotes\", split=\"train[-05%:]\")\n",
    "\n",
    "processed_train_ds = train_ds.map(\n",
    "    lambda samples: tokenizer(samples[\"quote\"], max_length=512), batched=True\n",
    ")\n",
    "processed_test_ds = test_ds.map(\n",
    "    lambda samples: tokenizer(samples[\"quote\"], max_length=512), batched=True\n",
    ")\n",
    "print(processed_train_ds)\n",
    "print(processed_test_ds)\n",
    "\n",
    "# test_data = train_test_split(\n",
    "#     test_size=0.1,\n",
    "# )\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_data = load_dataset(\"grammarly/coedit\", split=\"train\")\n",
    "# test_data = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "train_data = load_dataset(\"grammarly/coedit\", split=\"train[:1000]\")\n",
    "test_data = load_dataset(\"grammarly/coedit\", split=\"train[:100]\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_data, \"test\": test_data})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src', 'tgt', 'prompt'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src', 'tgt', 'prompt'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "2\n",
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response: For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
      "Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\n",
      "Response: As the number of people grows, the need for a habitable environment is unquestionably increasing.\n"
     ]
    }
   ],
   "source": [
    "def generate_grammar_prompt(src: str, tgt: str) -> str:\n",
    "    return f\"\"\"{src}\n",
    "Response: {tgt}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def prompt_dataset(item):\n",
    "    return {\n",
    "        \"prompt\": generate_grammar_prompt(item[\"src\"], item[\"tgt\"]),\n",
    "        # \"labels\": item[\"tgt\"],\n",
    "    }\n",
    "\n",
    "\n",
    "prompted_dataset = dataset.map(prompt_dataset, remove_columns=[\"task\", \"_id\"])\n",
    "print(prompted_dataset)\n",
    "print(len(prompted_dataset))\n",
    "print(prompted_dataset[\"train\"][0][\"prompt\"])\n",
    "print(prompted_dataset[\"train\"][1][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "[27914, 477, 14599, 44935, 8563, 422, 428, 2420, 25, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 1059, 430, 687, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 1262, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 198, 31077, 25, 1114, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 6121, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 779, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 50256]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1672, 11, 2678, 351, 257, 1256, 286, 45288, 460, 6121, 511, 10326, 284, 2620, 511, 49055, 1956, 290, 779, 35425, 284, 2148, 3424, 1660, 284, 262, 10326, 13, 50256]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "def process_dataset(batch):\n",
    "    # model_input = tokenizer(examples[\"prompt\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs = tokenizer(batch[\"prompt\"])\n",
    "    model_reponses = tokenizer(batch[\"tgt\"])\n",
    "\n",
    "    new_input_ids = []\n",
    "    new_labels = []\n",
    "    for input_ids, response_ids in zip(model_inputs.input_ids, model_reponses.input_ids):\n",
    "        # debug_labels = input_ids[-len(response_ids) :]\n",
    "        # print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "        # print(tokenizer.decode(debug_labels, skip_special_tokens=False))\n",
    "\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "        new_input_ids.append(input_ids)\n",
    "\n",
    "        num_tokens_ignore = len(input_ids) - len(response_ids)\n",
    "        labels = [-100] * num_tokens_ignore + input_ids[-len(response_ids) :]\n",
    "        new_labels.append(labels)\n",
    "\n",
    "    new_attention_mask = []\n",
    "    for attention_mask in model_inputs.attention_mask:\n",
    "        attention_mask.append(1)\n",
    "        new_attention_mask.append(attention_mask)\n",
    "\n",
    "    # labels = tokenizer(text_target=examples[\"tgt\"], max_length=1024, padding=True).input_ids\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs[\"input_ids\"] = new_input_ids\n",
    "    model_inputs[\"attention_mask\"] = new_attention_mask\n",
    "    model_inputs[\"labels\"] = new_labels\n",
    "\n",
    "    # print(\n",
    "    #     f\">> input_ids: {len(model_inputs['input_ids'])},\"\n",
    "    #     \"attention_mask: {len(model_inputs['attention_mask'])},\"\n",
    "    #     \"labels: {len(model_inputs['labels'])}\"\n",
    "    # )\n",
    "    # count = 0\n",
    "    # for input_ids, labels, attention_masks in zip(new_input_ids, new_labels, new_attention_mask):\n",
    "    #     count += 1\n",
    "    #     if count > 3:\n",
    "    #         break\n",
    "    #     print(f\">> input_ids: {len(input_ids)}, attention_mask: {len(attention_masks)}, labels: {len(labels)}\")\n",
    "    #     print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "    #     print(labels)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "processed_dataset = prompted_dataset.map(process_dataset, batched=True, remove_columns=[\"tgt\", \"src\", \"prompt\"])\n",
    "print(processed_dataset)\n",
    "print(processed_dataset[\"train\"][\"input_ids\"][0])\n",
    "print(processed_dataset[\"train\"][\"attention_mask\"][0])\n",
    "print(processed_dataset[\"train\"][\"labels\"][0])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# dataloader = DataLoader(processed_dataset[\"train\"], batch_size=4, collate_fn=data_collator)\n",
    "# for batch in dataloader:\n",
    "# print(batch)\n",
    "# break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_model_name: gpt2-large-bnb8-coedit,training_model_id: iliazlobin/gpt2-large-bnb8-coedit,training_model_path: iliazlobin_gpt2-large-bnb8-coedit\n",
      "total/used/cuda/res/ram (Gb): 10.00/6.63/0.82/1.36/19.64\n",
      "total/used/available memory (Gb): 10.00/6.63/3.37\n",
      "recommended/actual fraction: 0.34/0.90\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "training_model_repo = f\"iliazlobin\"\n",
    "training_model_name: str = f\"{model_name}-bnb8-coedit\"\n",
    "training_model_id = f\"{training_model_repo}/{training_model_name}\"\n",
    "training_model_checkpoint = f\"{training_model_id}\"\n",
    "training_model_path = f\"{training_model_repo}_{training_model_name}\"\n",
    "print(\n",
    "    f\"training_model_name: {training_model_name}, \"\n",
    "    f\"training_model_id: {training_model_id}, \"\n",
    "    f\"training_model_path: {training_model_path}\"\n",
    ")\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "actual_fraction = 0.90\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "trainable params: 11796480 || all params: 431932160 || trainable%: 2.7310955498196754\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "perf_model = model\n",
    "perf_model = prepare_model_for_kbit_training(perf_model, use_gradient_checkpointing=True)\n",
    "# perf_model.gradient_checkpointing_enable()\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    # target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "perf_model = get_peft_model(perf_model, config)\n",
    "print(type(perf_model))\n",
    "print(perf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = perf_model\n",
    "print(type(training_model))\n",
    "print(training_model.config)\n",
    "\n",
    "total_params = sum(p.numel() for p in perf_model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in perf_model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"model-{training_model_path}-train/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 1000, per_epoch_steps: 125.0, max_steps: 125, epochs: 1.0\n",
      "train_path: model-iliazlobin_gpt2-large-bnb8-coedit-train\n",
      "<class 'transformers.trainer.Trainer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.314122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.286165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/izlobin/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.27462344932556154, metrics={'train_runtime': 103.2135, 'train_samples_per_second': 9.689, 'train_steps_per_second': 1.211, 'total_flos': 293214005575680.0, 'train_loss': 0.27462344932556154, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "train_path = f\"model-{training_model_path}-train\"\n",
    "print(f\"train_path: {train_path}\")\n",
    "\n",
    "train_size = len(processed_dataset[\"train\"])\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 4\n",
    "per_epoch_steps = train_size / (per_device_train_batch_size * gradient_accumulation_steps)\n",
    "max_steps = 125\n",
    "epochs = max_steps / per_epoch_steps\n",
    "print(f\"train_size: {train_size}, per_epoch_steps: {per_epoch_steps}, max_steps: {max_steps}, epochs: {epochs}\")\n",
    "\n",
    "if max_steps < per_epoch_steps:\n",
    "    raise Exception(f\"Training doesn't cover the entire training dataset with {train_size} samples\")\n",
    "\n",
    "# epochs = 2\n",
    "# epoch_total_steps = epochs * per_epoch_steps\n",
    "# print(f\"train_size: {train_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=train_path,\n",
    "    learning_rate=2e-4,\n",
    "    # warmup_ratio=0.05,\n",
    "    # optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    # num_train_epochs=2,\n",
    "    max_steps=max_steps,\n",
    "    warmup_steps=2,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=training_model,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(type(trainer))\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "trainer.save_model(f\"model-{training_model_path}\")\n",
    "# trainer.push_to_hub(training_model_id, token=os.getenv(\"HUGGING_FACE_TOKEN\"))\n",
    "trained_model = trainer.model\n",
    "model = trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
      "          1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
      "           430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
      "          1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
      "           198, 31077,    25],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 22743, 23491,    25,   679,  3708,   257,  4998,  1097,    13,\n",
      "           198, 31077,    25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "[' Countries with a lot of deserts can terraform their deserts to increase their habitable land and using irrigation to provide clean water to the desert.', ' He drives an amazing car.']\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\n",
    "    f\"\"\"Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
    "Response:\n",
    "\"\"\".strip(),\n",
    "    f\"\"\"Fix grammar: He drive a amazing car.\n",
    "Response:\n",
    "\"\"\".strip(),\n",
    "]\n",
    "# input_texts = [\n",
    "#     \"Fix grammar:  We don't have enough good Open Source games -- it's a waste to pour all the resources we have into one. :) Wesnoth has dwarves with guns, World of Warcraft'' has gnomes and goblins with explosives and flying machines -- where do you, personally, define the limits of the fantasy genre?\",\n",
    "#     \"Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian Hall, still a vibrant community center/beer garden started by Czech immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, continuously occupied Latin music store in New York City,  as census sites to the National Register of Historic Places.\",\n",
    "# ]\n",
    "\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "print(inputs)\n",
    "\n",
    "model.config.use_cache = False\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, num_return_sequences=1)\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=True,\n",
    "#     max_length=256,\n",
    "# )\n",
    "\n",
    "trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "trimmed_outputs = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "print(trimmed_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
