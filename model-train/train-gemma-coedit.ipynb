{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install transformers evaluate\n",
    "# %pip install nltk absl-py rouge_score\n",
    "# %pip install bleu sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install scipy\n",
    "# %pip install sentencepiece\n",
    "# %pip install optimum auto-gptq\n",
    "# %pip install scikit-learn\n",
    "# %pip install einops\n",
    "# %pip install bitsandbytes\n",
    "# %pip install accelerate\n",
    "# %pip install pynvml\n",
    "# %pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.dataset import get_iterater_samples_simplified, get_iterater_samples_with_instruction\n",
    "from utils.metric import calculate_scores\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gemma-2b,model_id: google/gemma-2b,model_path: google_gemma-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "1\n",
      "0\n",
      "left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1dc464796b47a79202818f6d6b84d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.modeling_gemma.GemmaForCausalLM'>\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gemma-2b\"\n",
    "# model_name = \"gemma-7b-it\"\n",
    "# model_name = \"gemma-7b\"\n",
    "model_repo = f\"google\"\n",
    "model_id = f\"{model_repo}/{model_name}\"\n",
    "model_checkpoint = f\"{model_repo}/{model_name}\"\n",
    "model_path = f\"{model_repo}_{model_name}\"\n",
    "print(f\"model_name: {model_name},\" f\"model_id: {model_id},\" f\"model_path: {model_path}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    ")\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", max_memory={0: \"80GiB\"})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0)\n",
    "print(type(model))\n",
    "\n",
    "# model.generation_config.max_new_tokens = 350\n",
    "# # model.generation_config.new_tokens = 350\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "# model.generation_config.padding_side = \"left\"\n",
    "\n",
    "print(model.generation_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammarly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set {'gec', 'coherence', 'clarity', 'paraphrase', 'simplification', 'neutralize'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 63703\n",
      "})\n",
      "test set {'gec', 'coherence', 'clarity', 'paraphrase', 'simplification', 'neutralize'}\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references'],\n",
      "    num_rows: 7080\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 63703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "        num_rows: 7080\n",
      "    })\n",
      "})\n",
      "{'task': 'gec', 'input': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'reference': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'references': ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'], 'request': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'prompt': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# full_dataset = load_dataset(\"grammarly/coedit\")\n",
    "# print(full_dataset)\n",
    "\n",
    "# train_dataset = load_dataset(\"grammarly/coedit\", split=\"train[:50000]\")\n",
    "# test_dataset = load_dataset(\"grammarly/coedit\", split=\"train[10000:]\")\n",
    "# # test_dataset = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "all_dataset = load_dataset(\"grammarly/coedit\", split=\"train+validation\")\n",
    "# print(all_dataset)\n",
    "\n",
    "# print()\n",
    "# print(f\"train set {set(all_dataset['task'])}\")\n",
    "# print(f\"total len: {len(all_dataset)}\")\n",
    "# print(f\"gec len: {len(all_dataset.filter(lambda x: x['task'] == 'gec'))}\")\n",
    "# print(f\"simplification len: {len(all_dataset.filter(lambda x: x['task'] == 'simplification'))}\")\n",
    "# print(f\"clarity len: {len(all_dataset.filter(lambda x: x['task'] == 'clarity'))}\")\n",
    "# print(f\"coherence len: {len(all_dataset.filter(lambda x: x['task'] == 'coherence'))}\")\n",
    "# print(f\"paraphrase len: {len(all_dataset.filter(lambda x: x['task'] == 'paraphrase'))}\")\n",
    "# print(f\"neutralize len: {len(all_dataset.filter(lambda x: x['task'] == 'neutralize'))}\")\n",
    "# print()\n",
    "\n",
    "# train_ratio = 0.01\n",
    "# test_ratio = 0.001\n",
    "# train_ratio = 0.1\n",
    "# test_ratio = 0.01\n",
    "train_ratio = 0.9\n",
    "test_ratio = 0.1\n",
    "\n",
    "gec_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"gec\")\n",
    "train_gec_dataset = gec_dataset.select(range(0, int(train_ratio * len(gec_dataset))))\n",
    "test_gec_dataset = gec_dataset.select(range(int((1 - test_ratio) * len(gec_dataset)), len(gec_dataset)))\n",
    "\n",
    "simplification_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"simplification\")\n",
    "train_simplification_dataset = simplification_dataset.select(range(0, int(train_ratio * len(simplification_dataset))))\n",
    "test_simplification_dataset = simplification_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(simplification_dataset)), len(simplification_dataset))\n",
    ")\n",
    "\n",
    "clarity_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"clarity\")\n",
    "train_clarity_dataset = clarity_dataset.select(range(0, int(train_ratio * len(clarity_dataset))))\n",
    "test_clarity_dataset = clarity_dataset.select(range(int((1 - test_ratio) * len(clarity_dataset)), len(clarity_dataset)))\n",
    "\n",
    "coherence_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"coherence\")\n",
    "train_coherence_dataset = coherence_dataset.select(range(0, int(train_ratio * len(coherence_dataset))))\n",
    "test_coherence_dataset = coherence_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(coherence_dataset)), len(coherence_dataset))\n",
    ")\n",
    "\n",
    "paraphrase_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"paraphrase\")\n",
    "train_paraphrase_dataset = paraphrase_dataset.select(range(0, int(train_ratio * len(paraphrase_dataset))))\n",
    "test_paraphrase_dataset = paraphrase_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(paraphrase_dataset)), len(paraphrase_dataset))\n",
    ")\n",
    "\n",
    "neutralize_dataset = all_dataset.filter(lambda x: x[\"task\"] == \"neutralize\")\n",
    "neutralize_dataset_split = int(train_ratio * len(neutralize_dataset))\n",
    "train_neutralize_dataset = neutralize_dataset.select(range(0, int(train_ratio * len(neutralize_dataset))))\n",
    "test_neutralize_dataset = neutralize_dataset.select(\n",
    "    range(int((1 - test_ratio) * len(neutralize_dataset)), len(neutralize_dataset))\n",
    ")\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets(\n",
    "    [\n",
    "        train_gec_dataset,\n",
    "        train_simplification_dataset,\n",
    "        train_clarity_dataset,\n",
    "        train_coherence_dataset,\n",
    "        train_paraphrase_dataset,\n",
    "        train_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"train set {set(train_dataset['task'])}\")\n",
    "print(train_dataset)\n",
    "\n",
    "test_dataset = concatenate_datasets(\n",
    "    [\n",
    "        test_gec_dataset,\n",
    "        test_simplification_dataset,\n",
    "        test_clarity_dataset,\n",
    "        test_coherence_dataset,\n",
    "        test_paraphrase_dataset,\n",
    "        test_neutralize_dataset,\n",
    "    ]\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda item: {\n",
    "        \"input\": item[\"src\"],\n",
    "        \"reference\": item[\"tgt\"],\n",
    "        \"references\": [item[\"tgt\"]],\n",
    "    },\n",
    "    remove_columns=[\"src\", \"tgt\", \"_id\"],\n",
    ")\n",
    "print(f\"test set {set(test_dataset['task'])}\")\n",
    "print(test_dataset)\n",
    "\n",
    "\n",
    "def add_prompt(item):\n",
    "    return {\n",
    "        \"request\": f\"{item['input']}\\nResponse:\",\n",
    "        \"prompt\": f\"{item['input']}\\nResponse:{item['reference']}\",\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = dataset.rename_column(\"task\", \"label\")\n",
    "dataset = dataset.map(add_prompt)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_input_length train: 342\n",
      "max_input_length test: 248\n"
     ]
    }
   ],
   "source": [
    "# find the longest sequence in the dataset\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"train\"])\n",
    "print(f\"max_input_length train: {max_input_length}\")\n",
    "max_input_length = max(len(tokenizer.encode(item[\"input\"])) for item in dataset[\"test\"])\n",
    "print(f\"max_input_length test: {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/gec: 18277\n",
      "train/coherence: 9554\n",
      "train/clarity: 1126\n",
      "train/paraphrase: 14307\n",
      "train/simplification: 10296\n",
      "train/neutralize: 10143\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    train_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    train_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "train_dataset_map = {}\n",
    "for task, list in train_lists_map.items():\n",
    "    train_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(train_dataset_map)\n",
    "\n",
    "train_dataset_dict = DatasetDict(train_dataset_map)\n",
    "# print(train_dataset_dict)\n",
    "\n",
    "# for task, ds in train_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"train/{task}: {len(train_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/gec: 2031\n",
      "test/coherence: 1062\n",
      "test/clarity: 126\n",
      "test/paraphrase: 1590\n",
      "test/simplification: 1144\n",
      "test/neutralize: 1127\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_lists_map = {}\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    test_lists_map[task] = []\n",
    "\n",
    "for item in dataset[\"test\"]:\n",
    "    test_lists_map[item[\"task\"]].append(item)\n",
    "\n",
    "test_dataset_map = {}\n",
    "for task, list in test_lists_map.items():\n",
    "    test_dataset_map[task] = Dataset.from_list(list)\n",
    "# print(test_dataset_map)\n",
    "\n",
    "test_dataset_dict = DatasetDict(test_dataset_map)\n",
    "# print(test_dataset_dict)\n",
    "\n",
    "# for task, ds in test_dataset_dict.items():\n",
    "#     print(f\"{task}: {ds}\")\n",
    "\n",
    "for task in set(train_dataset['task']):\n",
    "    print(f\"test/{task}: {len(test_lists_map[task])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "False\n",
      "1\n",
      "0\n",
      "left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0dd2e94b234d26a531ebf16cc3ef5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330b95a2e3544921ac97c3950549ecd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 63703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 7080\n",
      "    })\n",
      "})\n",
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response:For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n",
      "72 [2, 13681, 832, 92868, 10266, 774, 736, 2793, 235292, 1699, 3287, 235269, 5605, 675, 476, 2940, 576, 105502, 798, 16743, 961, 1024, 21459, 577, 4740, 1024, 177711, 2840, 578, 2177, 34167, 577, 3658, 3903, 2003, 577, 573, 21459, 235265, 108, 3943, 235292, 1636, 3287, 235269, 5605, 675, 476, 2940, 576, 105502, 798, 9315, 1024, 21459, 577, 4740, 1024, 177711, 2840, 578, 1281, 34167, 577, 3658, 3903, 2003, 577, 573, 21459, 235265, 1]\n",
      "72 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "72 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 235292, 1636, 3287, 235269, 5605, 675, 476, 2940, 576, 105502, 798, 9315, 1024, 21459, 577, 4740, 1024, 177711, 2840, 578, 1281, 34167, 577, 3658, 3903, 2003, 577, 573, 21459, 235265, 1]\n",
      "Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\n",
      "Response:As the number of people grows, the need for a habitable environment is unquestionably increasing.\n",
      "{'input_ids': tensor([[     2,  13681,    832,  92868,  10266,    774,    736,   2793, 235292,\n",
      "           1699,   3287, 235269,   5605,    675,    476,   2940,    576, 105502,\n",
      "            798,  16743,    961,   1024,  21459,    577,   4740,   1024, 177711,\n",
      "           2840,    578,   2177,  34167,    577,   3658,   3903,   2003,    577,\n",
      "            573,  21459, 235265,    108,   3943, 235292,   1636,   3287, 235269,\n",
      "           5605,    675,    476,   2940,    576, 105502,    798,   9315,   1024,\n",
      "          21459,    577,   4740,   1024, 177711,   2840,    578,   1281,  34167,\n",
      "            577,   3658,   3903,   2003,    577,    573,  21459, 235265,      1],\n",
      "        [     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      2,  90822,    573,  92868,    684, 235292,   1877,\n",
      "            573,   1758,    576,   1461,  26075, 235269,    573,   1476,    576,\n",
      "         177711,   4473,    603, 135072,   8727, 235265,    108,   3943, 235292,\n",
      "           2169,    573,   1758,    576,   1461,  26075, 235269,    573,   1476,\n",
      "            604,    476, 177711,   4473,    603, 135072,   8185, 235265,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100, 235292,   1636,   3287, 235269,\n",
      "           5605,    675,    476,   2940,    576, 105502,    798,   9315,   1024,\n",
      "          21459,    577,   4740,   1024, 177711,   2840,    578,   1281,  34167,\n",
      "            577,   3658,   3903,   2003,    577,    573,  21459, 235265,      1],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100, 235292,\n",
      "           2169,    573,   1758,    576,   1461,  26075, 235269,    573,   1476,\n",
      "            604,    476, 177711,   4473,    603, 135072,   8185, 235265,      1]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "max_length = 350\n",
    "\n",
    "\n",
    "def process_dataset(batch):\n",
    "    model_inputs = tokenizer(batch[\"prompt\"], max_length=max_length)\n",
    "    model_reponses = tokenizer(batch[\"reference\"], max_length=max_length)\n",
    "\n",
    "    new_input_ids = []\n",
    "    new_labels = []\n",
    "    for input_ids, response_ids in zip(model_inputs.input_ids, model_reponses.input_ids):\n",
    "        # debug_labels = input_ids[-len(response_ids) :]\n",
    "        # print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "        # print(tokenizer.decode(debug_labels, skip_special_tokens=False))\n",
    "\n",
    "        num_tokens_ignore = len(input_ids) - len(response_ids)\n",
    "        labels = [-100] * num_tokens_ignore + input_ids[-len(response_ids) :]\n",
    "        # labels.append(-100)\n",
    "        labels.append(tokenizer.eos_token_id)\n",
    "        new_labels.append(labels)\n",
    "\n",
    "        input_ids.append(tokenizer.eos_token_id)\n",
    "        new_input_ids.append(input_ids)\n",
    "\n",
    "    new_attention_mask = []\n",
    "    for attention_mask in model_inputs.attention_mask:\n",
    "        # attention_mask.append(0)\n",
    "        attention_mask.append(1)\n",
    "        new_attention_mask.append(attention_mask)\n",
    "\n",
    "    # labels = tokenizer(text_target=examples[\"tgt\"], max_length=1024, padding=True).input_ids\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs[\"input_ids\"] = new_input_ids\n",
    "    model_inputs[\"attention_mask\"] = new_attention_mask\n",
    "    model_inputs[\"labels\"] = new_labels\n",
    "\n",
    "    # print(\n",
    "    #     f\">> input_ids: {len(model_inputs['input_ids'])},\"\n",
    "    #     \"attention_mask: {len(model_inputs['attention_mask'])},\"\n",
    "    #     \"labels: {len(model_inputs['labels'])}\"\n",
    "    # )\n",
    "    # count = 0\n",
    "    # for input_ids, labels, attention_masks in zip(new_input_ids, new_labels, new_attention_mask):\n",
    "    #     count += 1\n",
    "    #     if count > 3:\n",
    "    #         break\n",
    "    #     print(f\">> input_ids: {len(input_ids)}, attention_mask: {len(attention_masks)}, labels: {len(labels)}\")\n",
    "    #     print(tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "    #     print(labels)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# processed_dataset = data.map(preprocess_function, batched=True, batch_size=10, remove_columns=data[\"train\"].column_names)\n",
    "processed_dataset = dataset.map(\n",
    "    process_dataset, batched=True, batch_size=10, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "# processed_dataset = dataset.map(process_dataset, batched=True, batch_size=10)\n",
    "print(processed_dataset)\n",
    "print(dataset[\"train\"][0][\"prompt\"])\n",
    "# print(dataset[\"train\"][0][\"reference\"])\n",
    "print(len(processed_dataset[\"train\"][\"input_ids\"][0]), processed_dataset[\"train\"][\"input_ids\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"attention_mask\"][0]), processed_dataset[\"train\"][\"attention_mask\"][0])\n",
    "print(len(processed_dataset[\"train\"][\"labels\"][0]), processed_dataset[\"train\"][\"labels\"][0])\n",
    "print(dataset[\"train\"][1][\"prompt\"])\n",
    "# print(dataset[\"train\"][1][\"reference\"])\n",
    "# print(dataset[\"train\"][2][\"prompt\"])\n",
    "# print(dataset[\"train\"][2][\"reference\"])\n",
    "\n",
    "# print(len(processed_dataset[\"test\"][\"input_ids\"][0]), processed_dataset[\"test\"][\"input_ids\"][0])\n",
    "# print(len(processed_dataset[\"test\"][\"attention_mask\"][0]), processed_dataset[\"test\"][\"attention_mask\"][0])\n",
    "# print(len(processed_dataset[\"test\"][\"labels\"][0]), processed_dataset[\"test\"][\"labels\"][0])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", model=model)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "dataloader = DataLoader(processed_dataset[\"train\"], batch_size=2, collate_fn=data_collator)\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.modeling_gemma.GemmaForCausalLM'>\n",
      "None\n",
      "\n",
      ">> clarity\n",
      "request: ['Write a better readable version of the sentence: The idea is distinct from measured plant perception and chemical communication.\\nResponse:', 'Clarify this text: Education Malhi received his primary and secondary education at G.H. (Ghulam Haider) Muslim High School, named after his father, who was encouraged by Maulana Muhammad Ali to found this school in 1918, to educate aspiring students.\\nResponse:']\n",
      "reference: ['Such paranormal claims are distinct from the ability of plants to sense and respond to the environment via chemical and related stimuli.', 'Education Naseer Ahmad Malhi received his primary and secondary education at G.H. (Ghulam Haider) Muslim High School, named after his father, who was encouraged by Maulana Muhammad Ali to found this school in 1918, to educate aspiring students.']\n",
      "result: ['The idea is distinct from measured plant perception and chemical communication.\\nResponse:The idea is distinct from measured plant perception and chemical communication.', 'Education Malhi received his primary and secondary education at G.H. (Ghulam Haider) Muslim High School, named after his father, who was encouraged by Maulana Muhammad Ali to found this school in 1918, to educate aspiring students.']\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skip\n",
    "\n",
    "max_batch = 2\n",
    "max_length = 350\n",
    "\n",
    "print(type(model))\n",
    "print(model.generation_config.max_new_tokens)\n",
    "\n",
    "for task, batch in test_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    # print(f\"input: {input_batch['input']}\")\n",
    "    # print(f\"prompt: {input_batch['prompt']}\")\n",
    "    print(f\"request: {input_batch['request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch[\"request\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(trimmed_output, max_length=max_length, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    # processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(f\"result: {processed}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_model_name: gemma-2b-coedit, training_model_id: iliazlobin/gemma-2b-coedit, training_model_path: iliazlobin_gemma-2b-coedit\n",
      "total/used/cuda/res/ram (Gb): 79.15/10.60/9.34/9.34/4.48\n",
      "total/used/available memory (Gb): 79.15/10.60/68.55\n",
      "recommended/actual fraction: 0.87/0.95\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "training_model_repo = f\"iliazlobin\"\n",
    "training_model_name: str = f\"{model_name}-coedit\"\n",
    "training_model_id = f\"{training_model_repo}/{training_model_name}\"\n",
    "training_model_checkpoint = f\"{training_model_id}\"\n",
    "training_model_path = f\"{training_model_repo}_{training_model_name}\"\n",
    "print(\n",
    "    f\"training_model_name: {training_model_name}, \"\n",
    "    f\"training_model_id: {training_model_id}, \"\n",
    "    f\"training_model_path: {training_model_path}\"\n",
    ")\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "\n",
    "actual_fraction = 0.95\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    f\"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f\"recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total/trainable params: 2506172416/2506172416\n"
     ]
    }
   ],
   "source": [
    "training_model = model\n",
    "# print(type(training_model))\n",
    "# print(training_model.config)\n",
    "# print(training_model.generation_config)\n",
    "\n",
    "total_params = sum(p.numel() for p in training_model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in training_model.parameters() if p.requires_grad)\n",
    "print(f\"Total/trainable params: {total_params}/{total_trainable_params}\")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "# batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "# input_batch = batch.select(range(batch_size))\n",
    "# input = tokenizer(input_batch[\"input\"], padding=True, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(input.input_ids, max_length=max_length)\n",
    "# processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # print(eval_pred)\n",
    "    preds = eval_pred.predictions\n",
    "    # print(preds)\n",
    "    labels = eval_pred.label_ids\n",
    "    # print(labels)\n",
    "\n",
    "    # preds = np.argmax(preds, axis=-1)\n",
    "    # print(preds)\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    # print(preds)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # print(labels)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # print(decoded_preds)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # print(decoded_labels)\n",
    "\n",
    "    rouge_score = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    sacreblue_score = sacreblue_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # sari_score = sari_metric.compute(\n",
    "    #     sources=processed_samples[\"input\"],\n",
    "    #     predictions=processed_samples[\"processed\"],\n",
    "    #     references=processed_samples[\"references\"],\n",
    "    # )\n",
    "    em_score = em_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "\n",
    "    # report = {\n",
    "    #     \"metric\": 0,\n",
    "    # }\n",
    "    report = {\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"rougeLsum\": rouge_score[\"rougeLsum\"],\n",
    "        \"sacreblue\": sacreblue_score[\"score\"],\n",
    "        \"memory_used\": utilization[\"memory_used\"] / 1024**2,\n",
    "        \"cuda_allocated\": utilization[\"cuda_allocated\"] / 1024**2,\n",
    "        \"cuda_reserved\": utilization[\"cuda_reserved\"] / 1024**2,\n",
    "        \"ram_usage\": utilization[\"ram_usage\"] / 1024**2,\n",
    "        # \"sari.sari\": 0,\n",
    "        \"em\": em_score[\"exact_match\"],\n",
    "    }\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    report[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in report.items()}\n",
    "\n",
    "\n",
    "def logits_argmax(logits: torch.Tensor, labels):\n",
    "    # if isinstance(logits, tuple):\n",
    "    #     # Depending on the model and config, logits may contain extra tensors,\n",
    "    #     # like past_key_values, but logits always come first\n",
    "    #     logits = logits[0]\n",
    "    # return logits.argmax(dim=-1), labels\n",
    "    # logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# debug\n",
    "# trainer = Trainer(\n",
    "#     model=training_model,\n",
    "#     train_dataset=processed_dataset[\"train\"],\n",
    "#     eval_dataset=processed_dataset[\"test\"],\n",
    "#     # eval_dataset=processed_dataset[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     preprocess_logits_for_metrics=logits_argmax,\n",
    "#     args=args,\n",
    "# )\n",
    "\n",
    "# predictions = trainer.predict(processed_dataset[\"train\"].select(range(2)))\n",
    "# print(type(predictions))\n",
    "# # print(predictions)\n",
    "# print(predictions.predictions)\n",
    "# print(predictions.label_ids)\n",
    "# metrics = compute_metrics((predictions.predictions, predictions.label_ids))\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"model-{training_model_path}-train/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: model-iliazlobin_gemma-2b-coedit-train\n",
      "train_size: 63703, batch_size: 35, per_epoch_steps: 455.02142857142854, max_steps: 100, epochs: 4.5502142857142855\n",
      "train_size: 63703, batch_size: 35, per_epoch_steps: 455.02142857142854, epochs: 2, epoch_total_steps: 910.0428571428571\n"
     ]
    }
   ],
   "source": [
    "train_path = f\"model-{training_model_path}-train\"\n",
    "print(f\"train_path: {train_path}\")\n",
    "\n",
    "train_size = len(processed_dataset[\"train\"])\n",
    "batch_size = 35\n",
    "gradient_accumulation_steps = 4\n",
    "eval_batch_size = 35\n",
    "eval_accumulation_steps = 4\n",
    "\n",
    "per_epoch_steps = train_size / (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "max_steps = 100\n",
    "epochs = per_epoch_steps / max_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, max_steps: {max_steps}, epochs: {epochs}\")\n",
    "\n",
    "# if epochs < 1:\n",
    "#     raise Exception(f\"Training doesn't cover the entire training dataset with {train_size} samples\")\n",
    "\n",
    "epochs = 2\n",
    "epoch_total_steps = epochs * per_epoch_steps\n",
    "print(f\"train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/training_args.py:1859: FutureWarning: `--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case iliazlobin/gemma-2b-coedit).\n",
      "  warnings.warn(\n",
      "/home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='910' max='910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [910/910 48:22, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Sacreblue</th>\n",
       "      <th>Memory Used</th>\n",
       "      <th>Cuda Allocated</th>\n",
       "      <th>Cuda Reserved</th>\n",
       "      <th>Ram Usage</th>\n",
       "      <th>Em</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.542600</td>\n",
       "      <td>0.707619</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.362300</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>18.851300</td>\n",
       "      <td>69159.500000</td>\n",
       "      <td>9625.143100</td>\n",
       "      <td>62980.000000</td>\n",
       "      <td>5073.785200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.684908</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.390700</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>21.117500</td>\n",
       "      <td>67317.500000</td>\n",
       "      <td>9625.119600</td>\n",
       "      <td>61138.000000</td>\n",
       "      <td>5067.132800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.490900</td>\n",
       "      <td>0.673507</td>\n",
       "      <td>0.494300</td>\n",
       "      <td>0.392600</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>11.097900</td>\n",
       "      <td>67319.500000</td>\n",
       "      <td>9625.118200</td>\n",
       "      <td>61138.000000</td>\n",
       "      <td>9820.371100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.667195</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.479500</td>\n",
       "      <td>24.146400</td>\n",
       "      <td>67319.500000</td>\n",
       "      <td>9625.107900</td>\n",
       "      <td>61138.000000</td>\n",
       "      <td>9803.617200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.284200</td>\n",
       "      <td>0.747512</td>\n",
       "      <td>0.501100</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.479200</td>\n",
       "      <td>0.479200</td>\n",
       "      <td>27.352100</td>\n",
       "      <td>79283.500000</td>\n",
       "      <td>9625.097700</td>\n",
       "      <td>73102.000000</td>\n",
       "      <td>9845.976600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.744655</td>\n",
       "      <td>0.490800</td>\n",
       "      <td>0.390600</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>0.469300</td>\n",
       "      <td>24.005800</td>\n",
       "      <td>79283.500000</td>\n",
       "      <td>9625.112300</td>\n",
       "      <td>73102.000000</td>\n",
       "      <td>9916.753900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.736088</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>21.451900</td>\n",
       "      <td>79283.500000</td>\n",
       "      <td>9625.119600</td>\n",
       "      <td>73102.000000</td>\n",
       "      <td>9910.269500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.744274</td>\n",
       "      <td>0.488200</td>\n",
       "      <td>0.388200</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>79283.500000</td>\n",
       "      <td>9625.124000</td>\n",
       "      <td>73102.000000</td>\n",
       "      <td>10050.582000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.745639</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>20.776400</td>\n",
       "      <td>79283.500000</td>\n",
       "      <td>9625.100600</td>\n",
       "      <td>73102.000000</td>\n",
       "      <td>10024.695300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.533300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=910, training_loss=0.3761023112705776, metrics={'train_runtime': 2907.3075, 'train_samples_per_second': 43.823, 'train_steps_per_second': 0.313, 'total_flos': 1.82698009221931e+17, 'train_loss': 0.3761023112705776, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=train_path,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    # warmup_ratio=0.05,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    eval_accumulation_steps=eval_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    # max_steps=max_steps,\n",
    "    warmup_steps=1,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub_model_id=training_model_name,\n",
    "    push_to_hub_organization=\"iliazlobin\",\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # Should be false for Lora (https://github.com/kohya-ss/sd-scripts/issues/323#issuecomment-1485073421)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=training_model,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=logits_argmax,\n",
    "    args=args,\n",
    ")\n",
    "print(type(trainer))\n",
    "\n",
    "model.config.use_cache = False\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 1: `huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")'\n",
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'transformers.models.gemma.modeling_gemma.GemmaForCausalLM'>\n",
      "pushing to hub: iliazlobin/gemma-2b-coedit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb96641fb6654f6fb0e8956ca073462b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aebd070001414eab7cc6173503c934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1713666689.workstation.30436.0:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f05245baffc431e8a5a93b3be68bf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1828d6166cb144788698f5e80e9df043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64764bfe8f5b41a78e11e3e0a8d0ec94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285b2b8ff10540f3b20f69490aeca003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c038363f3c46e9843a87a7e02ee189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7f3f2f6a384caaa54221a5d31b7f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/iliazlobin/gemma-2b-coedit/commit/6e16d2c55143c5578ea4fe99fb4ff5834a0b838f', commit_message='complete: train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}', commit_description='', oid='6e16d2c55143c5578ea4fe99fb4ff5834a0b838f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../.env\n",
    "\n",
    "!huggingface-cli login --token os.getenv(key=\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "print(type(trainer))\n",
    "print(type(trainer.model))\n",
    "\n",
    "# print(f\"saving locally: model-{training_model_path}\")\n",
    "# trainer.save_model(f\"model-{training_model_path}\")\n",
    "\n",
    "print(f\"pushing to hub: {training_model_id}\")\n",
    "trainer.push_to_hub(\n",
    "    commit_message=\"complete: train_size: {train_size}, batch_size: {batch_size}, per_epoch_steps: {per_epoch_steps}, epochs: {epochs}, epoch_total_steps: {epoch_total_steps}\",\n",
    "    model_name=training_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "1\n",
      "0\n",
      "left\n",
      "Dataset({\n",
      "    features: ['task', 'input', 'reference', 'references', 'request', 'prompt'],\n",
      "    num_rows: 63703\n",
      "})\n",
      "request: ['Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\\nResponse:']\n",
      "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.']\n",
      "{'input_ids': tensor([[     2,  13681,    832,  92868,  10266,    774,    736,   2793, 235292,\n",
      "           1699,   3287, 235269,   5605,    675,    476,   2940,    576, 105502,\n",
      "            798,  16743,    961,   1024,  21459,    577,   4740,   1024, 177711,\n",
      "           2840,    578,   2177,  34167,    577,   3658,   3903,   2003,    577,\n",
      "            573,  21459, 235265,    108,   3943, 235292],\n",
      "        [     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      2,\n",
      "          90822,    573,  92868,    684, 235292,   1877,    573,   1758,    576,\n",
      "           1461,  26075, 235269,    573,   1476,    576, 177711,   4473,    603,\n",
      "         135072,   8727, 235265,    108,   3943, 235292]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "tensor([[     2,  13681,    832,  92868,  10266,    774,    736,   2793, 235292,\n",
      "           1699,   3287, 235269,   5605,    675,    476,   2940,    576, 105502,\n",
      "            798,  16743,    961,   1024,  21459,    577,   4740,   1024, 177711,\n",
      "           2840,    578,   2177,  34167,    577,   3658,   3903,   2003,    577,\n",
      "            573,  21459, 235265,    108,   3943, 235292,   1636,   3287, 235269,\n",
      "           5605,    675,    476,   2940,    576, 105502,    798,   9315,   1024,\n",
      "          21459,    577,   4740,   1024, 177711,   2840,    578,   1281,  34167,\n",
      "            577,   3658,   3903,   2003,    577,    573,  21459, 235265,      1],\n",
      "        [     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      2,\n",
      "          90822,    573,  92868,    684, 235292,   1877,    573,   1758,    576,\n",
      "           1461,  26075, 235269,    573,   1476,    576, 177711,   4473,    603,\n",
      "         135072,   8727, 235265,    108,   3943, 235292,   2169,    573,   1758,\n",
      "            576,   1461,  26075, 235269,    573,   1476,    604,    476, 177711,\n",
      "           4473,    603, 135072,   8727, 235265,      1,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0]],\n",
      "       device='cuda:0')\n",
      "result: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably essential.']\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "print(type(tokenizer))\n",
    "# print(tokenizer.add_eos_token)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "\n",
    "\n",
    "max_length = 350\n",
    "max_new_tokens = 100\n",
    "max_batch = 2\n",
    "\n",
    "print(dataset[\"train\"])\n",
    "\n",
    "batch_size = len(dataset[\"train\"]) if len(dataset[\"train\"]) < max_batch else max_batch\n",
    "input_batch = dataset[\"train\"].select(range(batch_size))\n",
    "# print(f\"task: {input_batch['task']}\")\n",
    "# print(f\"input: {input_batch['input']}\")\n",
    "print(f\"request: {input_batch['request']}\")\n",
    "print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "inputs = tokenizer(input_batch[\"request\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "print(inputs)\n",
    "\n",
    "model.config.use_cache = False\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=128, num_return_sequences=1)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    # max_length=max_length,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    # pad_token_id=tokenizer.eos_token_id,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "#     # return_attention_mask=True,\n",
    "#     # max_length=max_length,\n",
    "#     max_new_tokens=max_new_tokens\n",
    "# )\n",
    "print(outputs)\n",
    "\n",
    "trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "# result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=False)\n",
    "# result = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\n",
      "Response:\n",
      "Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "sequence = [27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
    "          1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
    "           430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
    "          1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
    "           198, 31077,    25]\n",
    "print(tokenizer.decode(sequence))\n",
    "# input\n",
    "sequence = [47531,   262, 14599,\n",
    "         44935,   414,    25,  1081,   262,  1271,   286,   661, 13676,    11,\n",
    "           262,   761,   286, 49055,  2858,   318, 38766,  1346,  6393,    13,\n",
    "           198, 31077,    25]\n",
    "print(tokenizer.decode(sequence))\n",
    "# labels\n",
    "# sequence = [1114,  1672,    11,  2678,   351,   257,  1256,\n",
    "#            286, 45288,   460,  6121,   511, 10326,   284,  2620,   511, 49055,\n",
    "#           1956,   290,   779, 35425,   284,  2148,  3424,  1660,   284,   262,\n",
    "#          10326,    13]\n",
    "# print(tokenizer.decode(sequence))\n",
    "# sequence = [27914,   477, 14599, 44935,  8563,   422,   428,  2420,    25,  1114,\n",
    "#           1672,    11,  2678,   351,   257,  1256,   286, 45288,   460,  1059,\n",
    "#            430,   687,   511, 10326,   284,  2620,   511, 49055,  1956,   290,\n",
    "#           1262, 35425,   284,  2148,  3424,  1660,   284,   262, 10326,    13,\n",
    "#            198, 31077,    25,   220]\n",
    "# print(tokenizer.decode(token_ids=sequence))\n",
    "# sequence = [25, 220, 25]\n",
    "# print(tokenizer.decode(sequence))\n",
    "# sequence = [1114]\n",
    "# print(tokenizer.decode(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> gec\n",
      "request: ['Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\\nResponse:', 'Improve the grammaticality: As the number of people grows, the need of habitable environment is unquestionably essential.\\nResponse:']\n",
      "reference: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.']\n",
      "result: ['For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'As the number of people grows, the need for a habitable environment is unquestionably increasing.']\n",
      "\n",
      ">> coherence\n",
      "request: ['Fix coherence in this sentence: Guy Leech is still a celebrity today. Guy Leech regularly appears on television as an advocate for health and fitness.\\nResponse:', 'Fix coherence in this text: Injury, bad luck and inconsistency prevented him from ever winning the series. He was certainly good enough.\\nResponse:']\n",
      "reference: ['Guy Leech is still a celebrity today and regularly appears on television as an advocate for health and fitness.', 'Injury, bad luck and inconsistency prevented him from ever winning the series, although he was certainly good enough.']\n",
      "result: ['Guy Leech is still a celebrity today and regularly appears on television as an advocate for health and fitness.', 'Injury, bad luck and inconsistency prevented him from ever winning the series. Nevertheless, he was certainly good enough.']\n",
      "\n",
      ">> clarity\n",
      "request: ['Clarify: This has been widely demonstrated for English using contextualized word representations such as OpenAI GPT (Radford et al., 2018 ), BERT (Devlin et al., 2019 ), or XLNet (Yang et al., 2019b).\\nResponse:', 'Use clearer wording: We apply our French language models to complex NLP tasks (natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches.\\nResponse:']\n",
      "reference: ['This has been widely demonstrated for English using contextualized word representations such as OpenAI GPT (Radford et al., 2018 ), BERT (Devlin et al., 2019; Yang et al., 2019b).', 'We apply our French language models to diverse NLP tasks (natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches.']\n",
      "result: ['This has been widely demonstrated for English using contextualized word representations such as OpenAI GPT (Radford et al., 2018 ), BERT (Devlin et al., 2019; Yang et al., 2019b).', 'We apply our French language models to complex NLP tasks (natural language inference, parsing, word sense disambiguation) and demonstrate that they outperform other pre-training approaches.']\n",
      "\n",
      ">> paraphrase\n",
      "request: ['Reword this sentence: Item 5.1.2 shall be amended to read:\\nResponse:', 'Reword this text: She stopped when she saw his expression.\\nResponse:']\n",
      "reference: ['Point 5.1.2 is replaced by the following:', 'Seeing the look on his face, she paused.']\n",
      "result: ['Point 5.1.2 is replaced by the following:', 'Seeing his face, she stopped.']\n",
      "\n",
      ">> simplification\n",
      "request: [\"Simplify this text: For ourselves, we'll make a fresh one.\\nResponse:\", 'Simplify this sentence: He wants to fuck one of us.\\nResponse:']\n",
      "reference: ['We make a new one for ourselves.', 'One of us is about to get fucked.']\n",
      "result: ['We will make a new one for ourselves.', 'One of us is going to get fucked up.']\n",
      "\n",
      ">> neutralize\n",
      "request: ['Make this text more neutral: chloroform \"the molecular lifesaver\" an article at oxford university providing interesting facts about chloroform.\\nResponse:', 'Remove points of view: gaming system, an dice pool system where matched die results determine success.\\nResponse:']\n",
      "reference: ['chloroform \"the molecular lifesaver\" an article at oxford university providing facts about chloroform.', 'gaming system, a unique dice pool system where matched die results determine success.']\n",
      "result: ['chloroform \"the molecular lifesaver\" an article at oxford university providing facts about chloroform.', 'gaming system, a dice pool system where matched die results determine success.']\n"
     ]
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "max_batch = 2\n",
    "max_length = 350\n",
    "max_new_tokens = 350\n",
    "\n",
    "count = 0\n",
    "# for task, batch in test_dataset_dict.items():\n",
    "for task, batch in train_dataset_dict.items():\n",
    "    print()\n",
    "    print(f\">> {task}\")\n",
    "    batch_size = len(batch) if len(batch) < max_batch else max_batch\n",
    "    input_batch = batch.select(range(batch_size))\n",
    "    print(f\"request: {input_batch['request']}\")\n",
    "    print(f\"reference: {input_batch['reference']}\")\n",
    "\n",
    "    inputs = tokenizer(input_batch[\"request\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "    # print(inputs)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    # model.config.pad_token_id = model.config.eos_token_id\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128)\n",
    "    # outputs = model.generate(**input_ids, max_new_tokens=128, num_return_sequences=1)\n",
    "    # outputs = model.generate(\n",
    "    #     **inputs, max_new_tokens=max_new_tokens, num_return_sequences=1\n",
    "    # )\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        # pad_token_id=tokenizer.eos_token_id,\n",
    "        # return_attention_mask=True,\n",
    "        # max_length=256,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    trimmed_output = outputs[:, inputs.input_ids.shape[1] :]\n",
    "    result = tokenizer.batch_decode(trimmed_output, skip_special_tokens=True)\n",
    "    print(f\"result: {result}\")\n",
    "\n",
    "    # count += 1\n",
    "    # if count > 3:\n",
    "    #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
