{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/izlobin/.local/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: evaluate in /home/izlobin/.local/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: filelock in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/izlobin/.local/lib/python3.10/site-packages (from evaluate) (2.18.0)\n",
      "Requirement already satisfied: dill in /home/izlobin/.local/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/izlobin/.local/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/izlobin/.local/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/izlobin/.local/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/izlobin/.local/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/izlobin/.local/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/izlobin/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/izlobin/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/izlobin/.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/izlobin/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/izlobin/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /home/izlobin/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: absl-py in /home/izlobin/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: rouge_score in /home/izlobin/.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: click in /home/izlobin/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/izlobin/.local/lib/python3.10/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/izlobin/.local/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/izlobin/.local/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/izlobin/.local/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/izlobin/.local/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bleu in /home/izlobin/.local/lib/python3.10/site-packages (0.3)\n",
      "Requirement already satisfied: sacrebleu in /home/izlobin/.local/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: efficiency in /home/izlobin/.local/lib/python3.10/site-packages (from bleu) (2.0)\n",
      "Requirement already satisfied: portalocker in /home/izlobin/.local/lib/python3.10/site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: regex in /home/izlobin/.local/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/izlobin/.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/izlobin/.local/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /home/izlobin/.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/izlobin/.local/lib/python3.10/site-packages (from sacrebleu) (5.2.1)\n",
      "Requirement already satisfied: pandas in /home/izlobin/.local/lib/python3.10/site-packages (from efficiency->bleu) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->efficiency->bleu) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->efficiency->bleu) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->efficiency->bleu) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/izlobin/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->efficiency->bleu) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sacremoses in /home/izlobin/.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /home/izlobin/.local/lib/python3.10/site-packages (from sacremoses) (2023.12.25)\n",
      "Requirement already satisfied: click in /home/izlobin/.local/lib/python3.10/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/izlobin/.local/lib/python3.10/site-packages (from sacremoses) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /home/izlobin/.local/lib/python3.10/site-packages (from sacremoses) (4.66.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /home/izlobin/.local/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /home/izlobin/.local/lib/python3.10/site-packages (from scipy) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in /home/izlobin/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: optimum in /home/izlobin/.local/lib/python3.10/site-packages (1.18.1)\n",
      "Requirement already satisfied: auto-gptq in /home/izlobin/.local/lib/python3.10/site-packages (0.7.1)\n",
      "Requirement already satisfied: coloredlogs in /home/izlobin/.local/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/izlobin/.local/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.40.0,>=4.26.0 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (4.39.3)\n",
      "Requirement already satisfied: torch>=1.11 in /home/izlobin/.local/lib/python3.10/site-packages (from optimum) (2.2.2)\n",
      "Requirement already satisfied: packaging in /home/izlobin/.local/lib/python3.10/site-packages (from optimum) (24.0)\n",
      "Requirement already satisfied: numpy in /home/izlobin/.local/lib/python3.10/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /home/izlobin/.local/lib/python3.10/site-packages (from optimum) (0.22.2)\n",
      "Requirement already satisfied: datasets in /home/izlobin/.local/lib/python3.10/site-packages (from optimum) (2.18.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/izlobin/.local/lib/python3.10/site-packages (from auto-gptq) (0.28.0)\n",
      "Requirement already satisfied: sentencepiece in /home/izlobin/.local/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: rouge in /home/izlobin/.local/lib/python3.10/site-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: gekko in /home/izlobin/.local/lib/python3.10/site-packages (from auto-gptq) (1.1.1)\n",
      "Requirement already satisfied: safetensors in /home/izlobin/.local/lib/python3.10/site-packages (from auto-gptq) (0.4.2)\n",
      "Requirement already satisfied: peft>=0.5.0 in /home/izlobin/.local/lib/python3.10/site-packages (from auto-gptq) (0.10.0)\n",
      "Requirement already satisfied: tqdm in /home/izlobin/.local/lib/python3.10/site-packages (from auto-gptq) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/izlobin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/izlobin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.2.0)\n",
      "Requirement already satisfied: requests in /home/izlobin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/izlobin/.local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.11.0)\n",
      "Requirement already satisfied: networkx in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/izlobin/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.4.99)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/izlobin/.local/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (0.15.2)\n",
      "Requirement already satisfied: protobuf in /home/izlobin/.local/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum) (5.26.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/izlobin/.local/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/izlobin/.local/lib/python3.10/site-packages (from datasets->optimum) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/izlobin/.local/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/izlobin/.local/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/izlobin/.local/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/izlobin/.local/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/izlobin/.local/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/izlobin/.local/lib/python3.10/site-packages (from datasets->optimum) (3.9.4)\n",
      "Requirement already satisfied: six in /home/izlobin/.local/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/izlobin/.local/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/izlobin/.local/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/izlobin/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/izlobin/.local/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /home/izlobin/.local/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/izlobin/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/izlobin/.local/lib/python3.10/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/izlobin/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/izlobin/.local/lib/python3.10/site-packages (from scikit-learn) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages (0.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in /home/izlobin/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages (0.43.1)\n",
      "Requirement already satisfied: torch in /home/izlobin/.local/lib/python3.10/site-packages (from bitsandbytes) (2.2.2)\n",
      "Requirement already satisfied: numpy in /home/izlobin/.local/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/izlobin/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/izlobin/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/izlobin/.local/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/izlobin/.local/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /home/izlobin/.local/lib/python3.10/site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/izlobin/.local/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/izlobin/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/izlobin/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: requests in /home/izlobin/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/izlobin/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/izlobin/.local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/izlobin/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/izlobin/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# %%script echo skipping\n",
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%pip install transformers evaluate\n",
    "%pip install nltk absl-py rouge_score\n",
    "%pip install bleu sacrebleu\n",
    "%pip install sacremoses\n",
    "%pip install scipy\n",
    "%pip install sentencepiece\n",
    "%pip install optimum auto-gptq\n",
    "%pip install scikit-learn\n",
    "%pip install einops\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartForCausalLM,\n",
    "    BartModel,\n",
    "    BartTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "from utils.metric import calculate_scores\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pprint(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BART\n",
    "* https://huggingface.co/facebook/bart-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "# model = BartModel.from_pretrained('facebook/bart-large')\n",
    "model = BartForCausalLM.from_pretrained('facebook/bart-large', device_map=0)\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading t5\n",
    "* https://huggingface.co/google-t5/t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"grammarly/coedit-large\"\n",
    "model_name = \"google-t5/t5-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name, model_max_length=512)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading coedit / flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"grammarly/coedit-large\"\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # revision=\"float16\",\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, countries with a lot of deserts can fff their desert to increase their habitable land and use irrigation to provide clean water to the desert.\n"
     ]
    }
   ],
   "source": [
    "# input_text = \"Fix grammatical errors in this sentence: I goes to work every noning.\"\n",
    "input_text = \"Remove all grammatical errors from this text: For example, countries with a lot of deserts can fff their desert to increase their habitable land and using irrigation to provide clean water to the desert.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **input_ids,\n",
    "    do_sample=True,\n",
    "    top_k=100,\n",
    "    # top_p=0.9,\n",
    "    # num_return_sequences=2,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=256,\n",
    ")\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# output = \"For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\"\n",
    "# reference = \"For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/gemma-2b-it\"\n",
    "# model_name = \"google/gemma-7b-it\"\n",
    "# model_name = \"google/gemma-7b\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=0)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # revision=\"float16\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Phi-2\n",
    "* https://huggingface.co/microsoft/phi-2\n",
    "* https://huggingface.co/TheBloke/phi-2-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.eos_token: <|endoftext|>\n",
      "PhiConfig {\n",
      "  \"_name_or_path\": \"TheBloke/phi-2-GPTQ\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"TheBloke/phi-2-GPTQ--configuration_phi.PhiConfig\",\n",
      "    \"AutoModelForCausalLM\": \"TheBloke/phi-2-GPTQ--modeling_phi.PhiForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"flash_attn\": false,\n",
      "  \"flash_rotary\": false,\n",
      "  \"fused_dense\": false,\n",
      "  \"img_processor\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"phi-msft\",\n",
      "  \"n_embd\": 2560,\n",
      "  \"n_head\": 32,\n",
      "  \"n_head_kv\": null,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 32,\n",
      "  \"n_positions\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": true,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'microsoft/phi-2'\n",
    "model_name = 'TheBloke/phi-2-GPTQ'\n",
    "model_alias = model_name.replace('/', '_')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\n",
    "# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # revision=\"float16\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading mixtral-8x7B\n",
    "* https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n",
    "* https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ\n",
    "\n",
    "Quantization\n",
    "* https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from transformers import BitsAndBytesConfig\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     29\u001b[0m     model_name,\n\u001b[1;32m     30\u001b[0m     use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# tokenizer.pad_token = tokenizer.eos_token\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3039\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3039\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m   3041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3043\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/quantizers/auto.py:153\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    149\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m AutoQuantizationConfig\u001b[38;5;241m.\u001b[39mfrom_dict(quantization_config)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig)) \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ config collision\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m \u001b[43mquantization_config_from_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loading_attributes\u001b[49m()\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m loading_attr_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(quantization_config, attr, val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'"
     ]
    }
   ],
   "source": [
    "# model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "model_name = 'TheBloke/Mixtral-8x7B-v0.1-GPTQ'\n",
    "model_alias = model_name.replace('/', '_')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=4)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=0,\n",
    "#     # torch_dtype=torch.float16,\n",
    "#     # load_in_4bit=True,\n",
    "#     # quantization_config=quantization_config,\n",
    "# )\n",
    "\n",
    "# print(f\"model.config.eos_token_id: {model.config.eos_token_id}\")\n",
    "# eos_token_id = 50256 # https://huggingface.co/microsoft/phi-2/blob/main/config.json\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=0,\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# text = 'Hello my name is'\n",
    "# inputs = tokenizer(text, return_tensors='pt')\n",
    "# outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/izlobin/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"TheBloke/Llama-2-7B-GPTQ\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_length\": 4096,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"dataset\": null,\n",
      "    \"desc_act\": false,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 1\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading llama-2\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "# model_name = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
    "\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, device_map=0)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring memory (VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total/trainable params: 254084096/254084096\n",
      "{'total_memory': 10736893952, 'memory_used': 3005644800, 'cuda_allocated': 1016336384, 'cuda_reserved': 1031798784, 'ram_usage': 13295087616}\n",
      "{'total_memory': '10.00', 'memory_used': '2.80', 'cuda_allocated': '0.95', 'cuda_reserved': '0.96', 'ram_usage': '12.38'}\n",
      "total/used/cuda/res/ram(Gb): 10.00/2.80/0.95/0.96/12.38\n",
      "Total/used/available memory (Gb): 10.00/{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\n",
      "Recommended/actual fraction: 0.72/0.95\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total/trainable params: {total_params}/{total_trainable_params}')\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "print(utilization)\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(utilization_str)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram(Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "actual_fraction = 0.95\n",
    "available_memory = utilization['total_memory'] - utilization['memory_used']\n",
    "recommended_fraction = available_memory / utilization['total_memory']\n",
    "torch.cuda.set_per_process_memory_fraction(actual_fraction, 0)\n",
    "\n",
    "print(\n",
    "    f\"Total/used/available memory (Gb): {utilization['total_memory']/1024**3:.2f}/\"\n",
    "    \"{utilization['memory_used']/1024**3:.2f}/{available_memory/1024**3:.2f}\"\n",
    ")\n",
    "print(f'Recommended/actual fraction: {recommended_fraction:.2f}/{actual_fraction:.2f}')\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.empty(utilization['total_memory'] // 2, dtype=torch.int8, device='cuda')\n",
    "# print_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix grammatical errors in this sentence: I does work.FfefefeDife Flowers Flowers-feeaeaeainaBeie Erin SweermMEferiainaFME Love LoveFER Flame Flame Erin Erin Swe Flame Erin Flame Flame Love Flame Erin Sweria Flame Flameriaria Flame Love Love Flameria Flame Erin Love Love Brandsriariariaca Flame Erin LOVE Love Flame Love Brands Flame Flame Flameilaria Flameermriaria Erin Flame Erin Wave Flame Flame LOVE Flame Erinila Flame Erin Flowersria Flameila Flame Flame Flowers Flame\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# reference = \" Some even provided gateways, such as UFGATE, by which members could send / receive e-mail to and from the Internet via UUCP, and many FidoNet discussion groups were shared via gateway to Usenet.\"\n",
    "input_text = \"Fix grammatical errors in this sentence: I does work.\"\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix grammatical errors in this sentence: I goes to work every noning.\n",
      "Answer: I go to work every day.\n",
      "\n",
      "Exercise 3:\n",
      "Add a conjunction to combine these two sentences: \"I love to read.\" \"I also enjoy playing sports.\"\n",
      "Answer: \"I love to read, and I also enjoy playing sports.\"\n",
      "\n",
      "Exercise 4:\n",
      "Complete the sentence by adding appropriate prepositions. \"The cat is hiding __ the table.\"\n",
      "Answer: \"The cat is hiding under the table.\"\n",
      "\n",
      "Exercise 5:\n",
      "Rewrite the sentence by removing unnecessary words: \"I went to the store, and then I also went to the bank.\"\n",
      "Answer: \"I went to the store and the bank.\"\n",
      "\n",
      "I hope this comprehensive explanation, accompanied by real-world use cases and exercises, has piqued your interest and helped you understand the importance of grammar, conjunctions, and prepositions in the language arts. Remember, these elements are like the threads that hold our thoughts together, making communication clearer and more effective.\n",
      "\n",
      "Stay curious and keep exploring the fascinating world of grammar!\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n",
      "\n",
      "\n",
      "\n",
      "Exercise:\n",
      "A company is developing an\n"
     ]
    }
   ],
   "source": [
    "# reference = \" Some even provided gateways, such as UFGATE, by which members could send / receive e-mail to and from the Internet via UUCP, and many FidoNet discussion groups were shared via gateway to Usenet.\"\n",
    "input_text = 'Fix grammatical errors in this sentence: I goes to work every noning.'\n",
    "# input_ids = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "input_ids = tokenizer(input_text, return_tensors='pt')\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **input_ids,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    # return_attention_mask=False,\n",
    "    max_length=256,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 69071\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1712\n",
      "    })\n",
      "})\n",
      "{'validation', 'train'}\n",
      "{'clarity', 'neutralize', 'coherence', 'gec', 'paraphrase', 'simplification'}\n"
     ]
    }
   ],
   "source": [
    "# api = HfApi()\n",
    "# coedit_info = api.dataset_info(\"grammarly/coedit\")\n",
    "# pprint(coedit_info)\n",
    "\n",
    "grammarly_dataset = load_dataset(\"grammarly/coedit\")\n",
    "pprint(grammarly_dataset)\n",
    "\n",
    "unique_categories = set(grammarly_dataset)\n",
    "pprint(unique_categories)\n",
    "\n",
    "unique_tasks = set(grammarly_dataset[\"train\"][\"task\"])\n",
    "pprint(unique_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "[gec] Fix grammaticality in this sentence\n",
      "src: Dear friends, I hope you should correctly but I can gives you some opinion, I guess that is a good idea if you go to a small schools, under you can met a lot on people and there are more closed friend of course you cannot like that opcion if you like the biggest once, so in that ways you can go from the other school.\n",
      "tgt: Dear friend, I hope you choose correctly but I can give you my opinion. I guess that it's a good idea if you go to a small school, because you can meet a lot of people and make more close friends of course you won't like that option if you like the bigger one, so in that case you should go to the other school.\n"
     ]
    }
   ],
   "source": [
    "def get_samples(dataset, category=\"validation\", task=\"gec\", num_samples=1, seed=42):\n",
    "    return dataset[category].shuffle(seed=seed).filter(lambda item: item[\"task\"] == task).select(range(num_samples))\n",
    "\n",
    "def print_samples(samples) -> None:\n",
    "    for item in samples:\n",
    "        pfx, src = item[\"src\"].split(\": \", 1)\n",
    "        print(f\"[{item['task']}] {pfx}\")\n",
    "        print(f\"src: {src}\")\n",
    "        print(f\"tgt: {item['tgt']}\")\n",
    "\n",
    "\n",
    "print_samples(get_samples(grammarly_dataset, num_samples=2))\n",
    "\n",
    "# input_ids = tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "# outputs = model.generate(input_ids, max_length=256)\n",
    "# corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# return {\"processed\": corrected}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "em_metric = evaluate.load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'rouge1': 0.7150747966331101,\n",
      " 'rouge2': 0.5167050375929942,\n",
      " 'rougeL': 0.7005677840072502,\n",
      " 'rougeLsum': 0.7007083564381789}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = rouge_metric.compute(\n",
    "    predictions=samples['src'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _GLUE metric_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "tensor([14269, 19519,    16,    48,  7142,    10,   493,  6195,   388,    55,\n",
      "            1])\n",
      "tensor([ 493, 6195,    6,  388,   55,    1])\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=2)\n",
    "pprint(object=samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "src_input_ids = tokenizer(samples[\"src\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "tgt_input_ids = tokenizer(samples[\"tgt\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "pprint(src_input_ids[0])\n",
    "pprint(tgt_input_ids[0])\n",
    "\n",
    "# score = glue_metric.compute(predictions=src_input_ids[0], references=tgt_input_ids[0])\n",
    "# score = glue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "# pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SacreBLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'bp': 1.0,\n",
      " 'counts': [3886, 2743, 1965, 1419],\n",
      " 'precisions': [70.79613773000547,\n",
      "                50.899981443681575,\n",
      "                37.152580828133864,\n",
      "                27.346309500867218],\n",
      " 'ref_len': 5090,\n",
      " 'score': 43.74251258938969,\n",
      " 'sys_len': 5489,\n",
      " 'totals': [5489, 5389, 5289, 5189]}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = sacreblue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARI metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'sari': 52.48853096503606}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "new_samples = samples.map(lambda item: {\"tgts\": [item[\"tgt\"]]})\n",
    "new_samples[\"tgts\"][:5]\n",
    "\n",
    "# sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "# predictions=[\"About 95 you now get in.\",\"About 95 you now get in.\"]\n",
    "# references=[[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"],[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"]]\n",
    "\n",
    "score = sari_metric.compute(\n",
    "  sources=new_samples['src'],\n",
    "  predictions=new_samples['src'],\n",
    "  references=new_samples['tgts']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact match (EM) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'exact_match': 1.0}\n"
     ]
    }
   ],
   "source": [
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=samples['tgt'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IteraTeR\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_v2\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_full_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 157579\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 19705\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 19703\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "    num_rows: 19705\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# iterater_dataset = load_dataset(\"wanyu/IteraTeR_v2\") # human in the loop\n",
    "iterater_dataset = load_dataset(\"wanyu/IteraTeR_full_sent\")\n",
    "pprint(iterater_dataset)\n",
    "iterater_validation_dataset = load_dataset(\"wanyu/IteraTeR_full_sent\", split=\"validation\")\n",
    "pprint(iterater_validation_dataset)\n",
    "# pprint(iterater_validation_dataset['validation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbolizers = {\n",
    "    \"gce\": {\n",
    "        \"tokens\": [\"<fluency>\"],\n",
    "        \"verbs\": [\n",
    "            \"Fix grammar\",\n",
    "            \"Fix grammar in this sentence\",\n",
    "            \"Fix grammar in the sentence\",\n",
    "            \"Fix grammar errors\",\n",
    "            \"Fix grammatical errors\",\n",
    "            \"Fix grammaticality\",\n",
    "            \"Fix all grammatical errors\",\n",
    "            \"Fix grammatical errors in this sentence\",\n",
    "            \"Fix grammar errors in this sentence\",\n",
    "            \"Fix grammatical mistakes in this sentence\",\n",
    "            \"Fix grammaticality in this sentence\",\n",
    "            \"Fix grammaticality of the sentence\",\n",
    "            \"Fix disfluencies in the sentence\",\n",
    "            \"Make the sentence grammatical\",\n",
    "            \"Make the sentence fluent\",\n",
    "            \"Fix errors in this text\",\n",
    "            \"Update to remove grammar errors\",\n",
    "            \"Remove all grammatical errors from this text\",\n",
    "            \"Improve the grammar of this text\",\n",
    "            \"Improve the grammaticality\",\n",
    "            \"Improve the grammaticality of this text\",\n",
    "            \"Improve the grammaticality of this sentence,\",\n",
    "            \"Grammar improvements\",\n",
    "            \"Remove grammar mistakes\",\n",
    "            \"Remove grammatical mistakes\",\n",
    "            \"Fix the grammar mistakes\",\n",
    "            \"Fix grammatical mistakes\",\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5078, selected: 5, num_samples: 5\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 5\n",
      "})\n",
      "[\"Fix grammar:  We don't have enough good Open Source games -- it's a waste to \"\n",
      " 'pour all the resources we have into one. :) Wesnoth has dwarves with guns, '\n",
      " \"World of Warcraft'' has gnomes and goblins with explosives and flying \"\n",
      " 'machines -- where do you, personally, define the limits of the fantasy '\n",
      " 'genre?',\n",
      " 'Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian '\n",
      " 'Hall, still a vibrant community center/beer garden started by Czech '\n",
      " 'immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, '\n",
      " 'continuously occupied Latin music store in New York City,  as census sites '\n",
      " 'to the National Register of Historic Places.']\n"
     ]
    }
   ],
   "source": [
    "def substitute_verbolizer(text, verbolizer, count=[0]):\n",
    "    verbs = verbolizers[verbolizer][\"verbs\"]\n",
    "\n",
    "    verb = verbs[count[0]]\n",
    "    tokens = verbolizers[verbolizer][\"tokens\"]\n",
    "    replaced_text = text\n",
    "    for t in tokens:\n",
    "        replaced_text = text.replace(t, f\"{verb}:\")\n",
    "        # pprint(f\"> t: {t}, verb: {verb}, text: {text}, replaced_text: {replaced_text}\")\n",
    "\n",
    "    count[0] += 1\n",
    "    if count[0] >= len(verbs):\n",
    "        count[0] = 0\n",
    "\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "def get_iterater_samples(label, category=\"validation\", num_samples=0, seed=42, confidence_threshold=0.9):\n",
    "    filtered_samples = (\n",
    "        iterater_dataset[category]\n",
    "        .shuffle(seed=seed)\n",
    "        .filter(lambda item: item[\"labels\"] == label and float(item[\"confidence\"]) >= confidence_threshold)\n",
    "    )\n",
    "    max_samples = len(filtered_samples)\n",
    "    selected = max_samples if num_samples == 0 else num_samples\n",
    "    print(f\"max_samples: {max_samples}, selected: {selected}, num_samples: {num_samples}\")\n",
    "    samples = filtered_samples.select(range(selected))\n",
    "\n",
    "    return samples.map(\n",
    "        lambda item: {\n",
    "            \"task\": substitute_verbolizer(item[\"before_sent_with_intent\"], \"gce\"),\n",
    "            \"source\": item[\"before_sent\"],\n",
    "            \"reference\": item[\"after_sent\"],\n",
    "            \"references\": [item[\"after_sent\"]],\n",
    "        },\n",
    "        remove_columns=[\n",
    "            \"before_sent_with_intent\",\n",
    "            \"before_sent\",\n",
    "            \"after_sent\",\n",
    "            \"labels\",\n",
    "            \"confidence\",\n",
    "            \"doc_id\",\n",
    "            \"revision_depth\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=5)\n",
    "pprint(samples)\n",
    "pprint(samples[\"task\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5078, selected: 100, num_samples: 100\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "})\n",
      "total/used/cuda/res/ram (Gb): 10.00/3.72/2.10/2.84/10.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  10%|         | 10/100 [00:13<01:59,  1.33s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-9/100 | total/used/cuda/res/ram (Gb): 10.00/9.75/2.10/8.95/10.30 | batch/sps: 10/0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  20%|        | 20/100 [00:26<01:45,  1.32s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-19/100 | total/used/cuda/res/ram (Gb): 10.00/9.73/2.10/9.14/10.30 | batch/sps: 10/0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  30%|       | 30/100 [00:38<01:28,  1.27s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-29/100 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.10/9.14/10.30 | batch/sps: 10/0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  40%|      | 40/100 [00:51<01:16,  1.27s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-39/100 | total/used/cuda/res/ram (Gb): 10.00/9.86/2.10/9.14/10.32 | batch/sps: 10/0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  50%|     | 50/100 [01:03<01:02,  1.26s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40-49/100 | total/used/cuda/res/ram (Gb): 10.00/9.85/2.10/9.14/10.26 | batch/sps: 10/0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  60%|    | 60/100 [01:15<00:48,  1.22s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50-59/100 | total/used/cuda/res/ram (Gb): 10.00/9.86/2.10/9.14/10.25 | batch/sps: 10/0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  70%|   | 70/100 [01:21<00:30,  1.03s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60-69/100 | total/used/cuda/res/ram (Gb): 10.00/9.86/2.10/9.37/10.26 | batch/sps: 10/1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  80%|  | 80/100 [01:35<00:22,  1.13s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70-79/100 | total/used/cuda/res/ram (Gb): 10.00/9.84/2.10/9.37/10.22 | batch/sps: 10/0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  90%| | 90/100 [01:49<00:12,  1.22s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80-89/100 | total/used/cuda/res/ram (Gb): 10.00/9.94/2.10/9.37/10.23 | batch/sps: 10/0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 100/100 [02:01<00:00,  1.21s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90-99/100 | total/used/cuda/res/ram (Gb): 10.00/9.91/2.10/9.37/10.22 | batch/sps: 10/0.81\n",
      "processed_sps: 0.8039060963211832\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'processed'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[\"Fix grammar:  We don't have enough good Open Source games -- it's a waste to \"\n",
      " 'pour all the resources we have into one. :) Wesnoth has dwarves with guns, '\n",
      " \"World of Warcraft'' has gnomes and goblins with explosives and flying \"\n",
      " 'machines -- where do you, personally, define the limits of the fantasy '\n",
      " 'genre?The.\\n'\n",
      " 'The study, published in the journal Nature, found that the bacteria in the '\n",
      " 'gut of the mice were more diverse than those of the mice that were fed a '\n",
      " 'standard diet.\\n'\n",
      " 'The researchers also found that the mice that were fed the high-fiber diet '\n",
      " 'had a higher number of bacteria that were associated with the development of '\n",
      " 'obesity.\\n'\n",
      " 'The researchers also found that the mice that were fed the high-fiber diet '\n",
      " 'had a higher number of bacteria that were associated with the development of '\n",
      " 'obesity.\\n'\n",
      " 'The researchers also found that the mice that were fed the high-fats diet '\n",
      " 'had a higher number of bacteria that were associated with the development of '\n",
      " 'obesity.\\n'\n",
      " 'The researchers also found that the mice that were fed the high-protein diet '\n",
      " 'had a higher number of bacteria that were associated with the development of '\n",
      " 'obesity.\\n'\n",
      " 'The researchers also found that the mice that were fed the high-carbohydrate '\n",
      " 'diet had a higher number of bacteria that were associated with the '\n",
      " 'development in the development of obesity.\\n'\n",
      " 'The researchers also found that the mice that were fed the high-sugar diet '\n",
      " 'had a higher number of bacteria that were associated with the development in '\n",
      " 'the development in the development in',\n",
      " 'Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian '\n",
      " 'Hall, still a vibrant community center/beer garden started by Czech '\n",
      " 'immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, '\n",
      " 'continuously occupied Latin music store in New York City,  as census sites '\n",
      " 'to the National Register of Historic Places.The U.S. Department of Labor '\n",
      " '(DOL) has issued a final rule that will allow employers to use a new, '\n",
      " 'less-onerous method for determining whether an employee is an independent '\n",
      " 'contractor or an employee under the Fair Labor Standards Act (FLSA).\\n'\n",
      " 'The final rule, which takes effect on January 1, 2020, will allow employers '\n",
      " 'to use a \"economic reality\" test to determine whether a worker is an '\n",
      " 'employee or an independent contractor. The new rule will replace the current '\n",
      " '\"control\" test, which has been in place since the 1940s.\\n'\n",
      " 'The new rule will allow employers to consider the following factors in '\n",
      " 'determining whether a worker is an employee or an independent contractor:\\n'\n",
      " '- The extent to which the work performed is an integral part of the '\n",
      " \"employer's business;\\n\"\n",
      " '- The permanence of the working relationship;\\n'\n",
      " \"- The amount of the worker's investment in facilities and equipment;\\n\"\n",
      " \"- The worker's opportunity for profit or loss;\\n\"\n",
      " '- The amount of initiative, judgment, or foresight in open market '\n",
      " 'competition with others with respect to the work performed;\\n'\n",
      " \"- The worker's skill and initiative;\\n\"\n",
      " '- The degree of permanence of the']\n",
      "CPU times: user 1min 47s, sys: 17.1 s, total: 2min 4s\n",
      "Wall time: 2min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "batch_size = 10\n",
    "total_samples = 100\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=total_samples)\n",
    "# samples = get_iterater_samples(label=\"fluency\")\n",
    "pprint(samples)\n",
    "\n",
    "processed_samples = samples\n",
    "\n",
    "\n",
    "from auto_gptq import exllama_set_max_input_length\n",
    "model = exllama_set_max_input_length(model, max_input_length=2400)\n",
    "\n",
    "def model_process(batch, idx, **kwargs):\n",
    "    num_samples = len(batch['task'])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "    total_samples = kwargs.get(\"total_samples\")\n",
    "\n",
    "    input_ids = tokenizer(batch['task'], padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(batch['task'], return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(item['task'], return_tensors=\"pt\").input_ids\n",
    "    # outputs = model.generate(input_ids, max_length=512)\n",
    "    outputs = model.generate(input_ids, max_length=312)\n",
    "    # print(f\"outputs: {outputs}\")\n",
    "    processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"{idx[0]}-{idx[-1]}/{total_samples} | total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "        f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']} | \"\n",
    "        f\"batch/sps: {num_samples}/{sps_str}\"\n",
    "    )\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram (Gb): {utilization_str['total_memory']}/{utilization_str['memory_used']}/\"\n",
    "    f\"{utilization_str['cuda_allocated']}/{utilization_str['cuda_reserved']}/{utilization_str['ram_usage']}\"\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# processed_samples = samples.map(model_process, num_proc=torch.cuda.device_count())\n",
    "processed_samples = samples.map(\n",
    "    model_process,\n",
    "    fn_kwargs={\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"total_samples\": total_samples,\n",
    "    },\n",
    "    num_proc=1,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    with_indices=True,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "processed_sps = total_samples / elapsed_time\n",
    "print(f\"processed_sps: {processed_sps}\")\n",
    "\n",
    "pprint(processed_samples)\n",
    "pprint(processed_samples['processed'][:2])\n",
    "\n",
    "saved_samples = processed_samples.remove_columns(['references'])\n",
    "flat_df = pd.DataFrame.from_records(saved_samples)\n",
    "flat_df.to_json(f\"samples/{model_alias}_frames.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 model  total_samples       sps     task  total_params  \\\n",
      "0  TheBloke/phi-2-GPTQ            100  0.803906  fluency     262364160   \n",
      "\n",
      "                                              scores  \n",
      "0  {'rouge': {'rouge1': 0.3078833232553331, 'roug...  \n"
     ]
    }
   ],
   "source": [
    "scores = calculate_scores(processed_samples)\n",
    "# pprint(scores)\n",
    "\n",
    "score_paths = [\n",
    "    \"rouge.rouge1\",\n",
    "    # \"rouge.rouge2\",\n",
    "    # \"rouge.rougeL\",\n",
    "    # \"rouge.rougeLsum\",\n",
    "    \"sacreblue.score\",\n",
    "    \"sari.sari\",\n",
    "    \"em.exact_match\",\n",
    "]\n",
    "\n",
    "base_frame = {\n",
    "    \"model\": model_name,\n",
    "    \"total_samples\": total_samples,\n",
    "    \"sps\": processed_sps,\n",
    "    \"task\": \"fluency\",\n",
    "    \"total_params\": total_params,\n",
    "}\n",
    "\n",
    "normalized_scores = {}\n",
    "for k, v in scores.items():\n",
    "    for k2, v2 in v.items():\n",
    "        if not isinstance(v2, list):\n",
    "            # normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "            path = f\"{k}.{k2}\"\n",
    "            if path in score_paths:\n",
    "                normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "# pprint(normalized_scores)\n",
    "\n",
    "flat_frame = base_frame.copy()\n",
    "flat_frame.update(normalized_scores)\n",
    "# pprint(frame)\n",
    "\n",
    "all_flat_frames = []\n",
    "if os.path.exists(\"results/all-flat-frames.csv\"):\n",
    "    all_flat_frames = pd.read_csv(\"results/all-flat-frames.csv\").to_dict(\"records\")\n",
    "\n",
    "flat_df = pd.DataFrame.from_records([flat_frame])\n",
    "# pprint(df)\n",
    "# print(flat_df.head().to_markdown(index=True))\n",
    "# flat_df.to_csv(f\"results/{model_alias}_flat-frame.csv\", index=False, float_format=\"%.2f\")\n",
    "flat_df.to_csv(f\"results/{model_alias}_flat-frame.csv\", index=False)\n",
    "\n",
    "all_flat_frames.append(flat_frame)\n",
    "all_flat_dfs = pd.DataFrame.from_records(all_flat_frames)\n",
    "all_flat_dfs.to_csv(f\"results/all-flat-frames.csv\", index=False)\n",
    "\n",
    "\n",
    "full_frame = base_frame.copy()\n",
    "full_frame.update({\"scores\": scores})\n",
    "# pprint(full_frame)\n",
    "full_df = pd.DataFrame.from_records([full_frame])\n",
    "\n",
    "pprint(full_df)\n",
    "full_df.to_json(f\"results/{model_alias}_full-frame.json\", orient=\"records\")\n",
    "# full_df.to_json(\"results/frame.json\", index=False)\n",
    "\n",
    "# more_frames = [frame, frame, frame]\n",
    "# more_df = pd.DataFrame.from_records(more_frames)\n",
    "# more_df.to_csv(\"results/frames.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
