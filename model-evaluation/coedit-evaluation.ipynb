{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "%%capture\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%pip install transformers evaluate\n",
    "%pip install nltk absl-py rouge_score\n",
    "%pip install bleu sacrebleu\n",
    "%pip install bleu sacremoses\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, AutoTokenizer, T5Tokenizer\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lading coedit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 3.06 GB\n",
      "Reserved: 3.06 GB\n"
     ]
    }
   ],
   "source": [
    "coedit_large_tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-large\")\n",
    "# coedit_large_model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\", device_map=0)\n",
    "coedit_large_model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\")\n",
    "coedit_large_model=coedit_large_model.to(device)\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device)/1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(device)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you?\n",
      "|    total_memory |     memory_used |  cuda_allocated |   cuda_reserved |       ram_usage |\n",
      "|           10.00 |            4.58 |            3.07 |            3.08 |           15.91 |\n",
      "Recommended fraction: 0.54\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skipping\n",
    "\n",
    "import psutil\n",
    "from pynvml import nvmlInit\n",
    "\n",
    "prompt = \"fix grammar: How is are you?\"\n",
    "input_ids = coedit_large_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = coedit_large_model.generate(input_ids, max_new_tokens=200)\n",
    "print(coedit_large_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "def calculate_utilization():\n",
    "    nvmlInit()\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used = info.used\n",
    "    cuda_allocated = torch.cuda.memory_allocated(device)\n",
    "    cuda_reserved = torch.cuda.memory_reserved(device)\n",
    "    ram_usage = psutil.virtual_memory().used\n",
    "    return {\n",
    "        \"total_memory\": total_memory,\n",
    "        \"memory_used\": memory_used,\n",
    "        \"cuda_allocated\": cuda_allocated,\n",
    "        \"cuda_reserved\": cuda_reserved,\n",
    "        \"ram_usage\": ram_usage,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_utilization(utilization):\n",
    "    total_memory = f\"{utilization[\"total_memory\"]/1024**3:15.2f}\"\n",
    "    memory_used = f\"{utilization[\"memory_used\"]/1024**3:15.2f}\"\n",
    "    cuda_allocated = f\"{utilization[\"cuda_allocated\"]/1024**3:15.2f}\"\n",
    "    cuda_reserved = f\"{utilization[\"cuda_reserved\"]/1024**3:15.2f}\"\n",
    "    ram_usage = f\"{utilization[\"ram_usage\"]/(1024**3):15.2f}\"\n",
    "\n",
    "    return {\n",
    "        \"total_memory\": total_memory,\n",
    "        \"memory_used\": memory_used,\n",
    "        \"cuda_allocated\": cuda_allocated,\n",
    "        \"cuda_reserved\": cuda_reserved,\n",
    "        \"ram_usage\": ram_usage,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_utilization_header(utilization):\n",
    "    print(f\"|    total_memory |     memory_used |  cuda_allocated |   cuda_reserved |       ram_usage |\")\n",
    "\n",
    "\n",
    "def print_utilization(utilization):\n",
    "    utilization_str = format_utilization(utilization)\n",
    "    print(\n",
    "        f\"| {utilization_str[\"total_memory\"]} | {utilization_str[\"memory_used\"]} | {utilization_str[\"cuda_allocated\"]} | {utilization_str[\"cuda_reserved\"]} | {utilization_str[\"ram_usage\"]} |\"\n",
    "    )\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "utilization = calculate_utilization()\n",
    "print_utilization_header(utilization)\n",
    "print_utilization(utilization)\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "print(f\"Recommended fraction: {recommended_fraction:.2f}\")\n",
    "\n",
    "# torch.cuda.set_per_process_memory_fraction(recommended_fraction, 0)\n",
    "torch.cuda.set_per_process_memory_fraction(0.95, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "print_utilization_header(utilization)\n",
    "print_utilization(utilization)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.empty(utilization[\"total_memory\"] // 2, dtype=torch.int8, device='cuda')\n",
    "# print_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "flan_t5_large_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\", return_attention_mask=False)\n",
    "flan_t5_large_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n",
    "# flan_t5_large_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
    "# flan_t5_large_model = flan_t5_large_model.to(device)\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device)/1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(device)/1024**3:.2f} GB\")\n",
    "\n",
    "total_params = sum(p.numel() for p in flan_t5_large_model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "total_trainable_params = sum(p.numel() for p in flan_t5_large_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {total_trainable_params}\")\n",
    "# total_memory_GB = total_params * 4 / (1024**3)\n",
    "# print(f\"Estimated model memory: {total_memory_GB:.2f} GB\")\n",
    "# for param_tensor in flan_t5_large_model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", flan_t5_large_model.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "prompt = \"translate English to German: How old are you?\"\n",
    "input_ids = flan_t5_large_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = flan_t5_large_model.generate(input_ids, max_new_tokens=200)\n",
    "print(flan_t5_large_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 69071\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1712\n",
      "    })\n",
      "})\n",
      "{'validation', 'train'}\n",
      "{'coherence', 'clarity', 'neutralize', 'simplification', 'paraphrase', 'gec'}\n"
     ]
    }
   ],
   "source": [
    "# api = HfApi()\n",
    "# coedit_info = api.dataset_info(\"grammarly/coedit\")\n",
    "# pprint(coedit_info)\n",
    "\n",
    "grammarly_dataset = load_dataset(\"grammarly/coedit\")\n",
    "pprint(grammarly_dataset)\n",
    "\n",
    "unique_categories = set(grammarly_dataset)\n",
    "pprint(unique_categories)\n",
    "\n",
    "unique_tasks = set(grammarly_dataset[\"train\"][\"task\"])\n",
    "pprint(unique_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "[gec] Fix grammaticality in this sentence\n",
      "src: Dear friends, I hope you should correctly but I can gives you some opinion, I guess that is a good idea if you go to a small schools, under you can met a lot on people and there are more closed friend of course you cannot like that opcion if you like the biggest once, so in that ways you can go from the other school.\n",
      "tgt: Dear friend, I hope you choose correctly but I can give you my opinion. I guess that it's a good idea if you go to a small school, because you can meet a lot of people and make more close friends of course you won't like that option if you like the bigger one, so in that case you should go to the other school.\n"
     ]
    }
   ],
   "source": [
    "def get_samples(dataset, category=\"validation\", task=\"gec\", num_samples=1, seed=42):\n",
    "    return dataset[category].shuffle(seed=seed).filter(lambda item: item[\"task\"] == task).select(range(num_samples))\n",
    "\n",
    "def print_samples(samples) -> None:\n",
    "    for item in samples:\n",
    "        pfx, src = item[\"src\"].split(\": \", 1)\n",
    "        print(f\"[{item['task']}] {pfx}\")\n",
    "        print(f\"src: {src}\")\n",
    "        print(f\"tgt: {item['tgt']}\")\n",
    "\n",
    "\n",
    "print_samples(get_samples(grammarly_dataset, num_samples=2))\n",
    "\n",
    "# input_ids = coedit_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "# outputs = coedit_large_model.generate(input_ids, max_length=256)\n",
    "# corrected = coedit_large_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# return {\"processed\": corrected}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'rouge1': 0.7149005682143055,\n",
      " 'rouge2': 0.5169776674544456,\n",
      " 'rougeL': 0.6998778809041015,\n",
      " 'rougeLsum': 0.7008364594050545}\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = rouge_metric.compute(\n",
    "    predictions=samples['src'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _GLUE metric_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "tensor([14269, 19519,    16,    48,  7142,    10,   493,  6195,   388,    55,\n",
      "            1])\n",
      "tensor([ 493, 6195,    6,  388,   55,    1])\n"
     ]
    }
   ],
   "source": [
    "glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=2)\n",
    "pprint(object=samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "src_input_ids = coedit_large_tokenizer(samples[\"src\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "tgt_input_ids = coedit_large_tokenizer(samples[\"tgt\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "pprint(src_input_ids[0])\n",
    "pprint(tgt_input_ids[0])\n",
    "\n",
    "# score = glue_metric.compute(predictions=src_input_ids[0], references=tgt_input_ids[0])\n",
    "# score = glue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "# pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SacreBLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'bp': 1.0,\n",
      " 'counts': [3886, 2743, 1965, 1419],\n",
      " 'precisions': [70.79613773000547,\n",
      "                50.899981443681575,\n",
      "                37.152580828133864,\n",
      "                27.346309500867218],\n",
      " 'ref_len': 5090,\n",
      " 'score': 43.74251258938969,\n",
      " 'sys_len': 5489,\n",
      " 'totals': [5489, 5389, 5289, 5189]}\n"
     ]
    }
   ],
   "source": [
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = sacreblue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARI metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'sari': 52.48853096503606}\n"
     ]
    }
   ],
   "source": [
    "sari_metric = evaluate.load(\"sari\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "new_samples = samples.map(lambda item: {\"tgts\": [item[\"tgt\"]]})\n",
    "new_samples[\"tgts\"][:5]\n",
    "\n",
    "# sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "# predictions=[\"About 95 you now get in.\",\"About 95 you now get in.\"]\n",
    "# references=[[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"],[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"]]\n",
    "\n",
    "score = sari_metric.compute(\n",
    "  sources=new_samples['src'],\n",
    "  predictions=new_samples['src'],\n",
    "  references=new_samples['tgts']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact match (EM) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'exact_match': 1.0}\n"
     ]
    }
   ],
   "source": [
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=samples['tgt'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IteraTeR\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_v2\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_full_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 157579\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 19705\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 19703\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "    num_rows: 19705\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# iterater_dataset = load_dataset(\"wanyu/IteraTeR_v2\") # human in the loop\n",
    "iterater_dataset = load_dataset(\"wanyu/IteraTeR_full_sent\")\n",
    "pprint(iterater_dataset)\n",
    "iterater_validation_dataset = load_dataset(\"wanyu/IteraTeR_full_sent\", split=\"validation\")\n",
    "pprint(iterater_validation_dataset)\n",
    "# pprint(iterater_validation_dataset['validation'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbolizers = {\n",
    "    \"gce\": {\n",
    "        \"tokens\": [\"<fluency>\"],\n",
    "        \"verbs\": [\n",
    "            \"Fix grammar\",\n",
    "            \"Fix grammar in this sentence\",\n",
    "            \"Fix grammar in the sentence\",\n",
    "            \"Fix grammar errors\",\n",
    "            \"Fix grammatical errors\",\n",
    "            \"Fix grammaticality\",\n",
    "            \"Fix all grammatical errors\",\n",
    "            \"Fix grammatical errors in this sentence\",\n",
    "            \"Fix grammar errors in this sentence\",\n",
    "            \"Fix grammatical mistakes in this sentence\",\n",
    "            \"Fix grammaticality in this sentence\",\n",
    "            \"Fix grammaticality of the sentence\",\n",
    "            \"Fix disfluencies in the sentence\",\n",
    "            \"Make the sentence grammatical\",\n",
    "            \"Make the sentence fluent\",\n",
    "            \"Fix errors in this text\",\n",
    "            \"Update to remove grammar errors\",\n",
    "            \"Remove all grammatical errors from this text\",\n",
    "            \"Improve the grammar of this text\",\n",
    "            \"Improve the grammaticality\",\n",
    "            \"Improve the grammaticality of this text\",\n",
    "            \"Improve the grammaticality of this sentence,\",\n",
    "            \"Grammar improvements\",\n",
    "            \"Remove grammar mistakes\",\n",
    "            \"Remove grammatical mistakes\",\n",
    "            \"Fix the grammar mistakes\",\n",
    "            \"Fix grammatical mistakes\",\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5078, selected: 5, num_samples: 5\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 5\n",
      "})\n",
      "[\"Fix grammar:  We don't have enough good Open Source games -- it's a waste to \"\n",
      " 'pour all the resources we have into one. :) Wesnoth has dwarves with guns, '\n",
      " \"World of Warcraft'' has gnomes and goblins with explosives and flying \"\n",
      " 'machines -- where do you, personally, define the limits of the fantasy '\n",
      " 'genre?',\n",
      " 'Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian '\n",
      " 'Hall, still a vibrant community center/beer garden started by Czech '\n",
      " 'immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, '\n",
      " 'continuously occupied Latin music store in New York City,  as census sites '\n",
      " 'to the National Register of Historic Places.']\n"
     ]
    }
   ],
   "source": [
    "def substitute_verbolizer(text, verbolizer, count=[0]):\n",
    "    verbs = verbolizers[verbolizer][\"verbs\"]\n",
    "\n",
    "    verb = verbs[count[0]]\n",
    "    tokens = verbolizers[verbolizer][\"tokens\"]\n",
    "    replaced_text = text\n",
    "    for t in tokens:\n",
    "        replaced_text = text.replace(t, f\"{verb}:\")\n",
    "        # pprint(f\"> t: {t}, verb: {verb}, text: {text}, replaced_text: {replaced_text}\")\n",
    "\n",
    "    count[0] += 1\n",
    "    if count[0] >= len(verbs):\n",
    "        count[0] = 0\n",
    "\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "def get_iterater_samples(label, category=\"validation\", num_samples=0, seed=42, confidence_threshold=0.9):\n",
    "    filtered_samples = (\n",
    "        iterater_dataset[category]\n",
    "        .shuffle(seed=seed)\n",
    "        .filter(lambda item: item[\"labels\"] == label and float(item[\"confidence\"]) >= confidence_threshold)\n",
    "    )\n",
    "    max_samples = len(filtered_samples)\n",
    "    selected = max_samples if num_samples == 0 else num_samples\n",
    "    print(f\"max_samples: {max_samples}, selected: {selected}, num_samples: {num_samples}\")\n",
    "    samples = filtered_samples.select(range(selected))\n",
    "\n",
    "    return samples.map(\n",
    "        lambda item: {\n",
    "            \"task\": substitute_verbolizer(item[\"before_sent_with_intent\"], \"gce\"),\n",
    "            \"source\": item[\"before_sent\"],\n",
    "            \"reference\": item[\"after_sent\"],\n",
    "            \"references\": [item[\"after_sent\"]],\n",
    "        },\n",
    "        remove_columns=[\n",
    "            \"before_sent_with_intent\",\n",
    "            \"before_sent\",\n",
    "            \"after_sent\",\n",
    "            \"labels\",\n",
    "            \"confidence\",\n",
    "            \"doc_id\",\n",
    "            \"revision_depth\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=5)\n",
    "pprint(samples)\n",
    "pprint(samples[\"task\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5078, selected: 1000, num_samples: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaeb9f19d78b45a4a3e2360d7d11bf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "|    total_memory |     memory_used |  cuda_allocated |   cuda_reserved |       ram_usage |\n",
      "|           10.00 |            4.56 |            3.07 |            3.08 |           15.82 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11be7613bd441e88fdfdbe8a475644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|           10.00 |            6.13 |            3.07 |            4.65 |           15.81 |\n",
      "|           10.00 |            7.24 |            3.07 |            5.80 |           15.80 |\n",
      "|           10.00 |            7.24 |            3.07 |            5.80 |           15.80 |\n",
      "|           10.00 |            8.91 |            3.07 |            7.47 |           15.81 |\n",
      "|           10.00 |            8.91 |            3.07 |            7.47 |           15.82 |\n",
      "|           10.00 |            8.91 |            3.07 |            7.47 |           15.81 |\n",
      "|           10.00 |            8.91 |            3.07 |            7.47 |           15.81 |\n",
      "|           10.00 |            8.91 |            3.07 |            7.47 |           15.81 |\n",
      "|           10.00 |            8.91 |            3.07 |            7.47 |           15.81 |\n",
      "|           10.00 |            9.22 |            3.07 |            7.77 |           15.80 |\n",
      "|           10.00 |            9.20 |            3.07 |            7.77 |           15.81 |\n",
      "|           10.00 |            9.21 |            3.07 |            7.77 |           15.82 |\n",
      "|           10.00 |            9.22 |            3.07 |            7.77 |           15.82 |\n",
      "|           10.00 |            9.22 |            3.07 |            7.77 |           15.82 |\n",
      "|           10.00 |            9.24 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.23 |            3.07 |            7.77 |           15.83 |\n",
      "|           10.00 |            9.23 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.24 |            3.07 |            7.77 |           15.83 |\n",
      "|           10.00 |            9.22 |            3.07 |            7.77 |           15.82 |\n",
      "|           10.00 |            9.21 |            3.07 |            7.77 |           15.83 |\n",
      "|           10.00 |            9.21 |            3.07 |            7.77 |           15.83 |\n",
      "|           10.00 |            9.30 |            3.07 |            7.77 |           15.82 |\n",
      "|           10.00 |            9.24 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.32 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.32 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.34 |            3.07 |            7.77 |           15.85 |\n",
      "|           10.00 |            9.21 |            3.07 |            7.77 |           15.82 |\n",
      "|           10.00 |            9.29 |            3.07 |            7.77 |           15.81 |\n",
      "|           10.00 |            9.31 |            3.07 |            7.77 |           15.82 |\n",
      "|           10.00 |            9.29 |            3.07 |            7.77 |           15.80 |\n",
      "|           10.00 |            9.28 |            3.07 |            7.77 |           15.81 |\n",
      "|           10.00 |            9.29 |            3.07 |            7.77 |           15.83 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.28 |            3.07 |            7.77 |           15.85 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.28 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.83 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.26 |            3.07 |            7.77 |           15.85 |\n",
      "|           10.00 |            9.26 |            3.07 |            7.77 |           15.85 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.26 |            3.07 |            7.77 |           15.85 |\n",
      "|           10.00 |            9.29 |            3.07 |            7.77 |           15.86 |\n",
      "|           10.00 |            9.30 |            3.07 |            7.77 |           15.84 |\n",
      "|           10.00 |            9.31 |            3.07 |            7.77 |           15.85 |\n",
      "|           10.00 |            9.27 |            3.07 |            7.77 |           15.84 |\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'processed'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "[\"We don't have enough good Open Source games -- it's a waste to pour all the \"\n",
      " 'resources we have into one. :) Wesnoth has dwarves with guns, World of '\n",
      " \"Warcraft'' has gnomes and goblins with explosives, and flying machines -- \"\n",
      " 'where do you, personally, define the limits of the fantasy genre?',\n",
      " 'In 2001, they successfully nominated Bohemian Hall, still a vibrant '\n",
      " 'community center/beer garden started by Czech immigrants in Astoria, Queens, '\n",
      " 'and the Casa Amadeo Music Store, the oldest, continuously occupied Latin '\n",
      " 'music store in New York City, as census sites to the National Register of '\n",
      " 'Historic Places.']\n",
      "CPU times: user 2min 50s, sys: 21.9 s, total: 3min 12s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=1000)\n",
    "# samples = get_iterater_samples(label=\"fluency\")\n",
    "pprint(samples)\n",
    "\n",
    "process_samples = samples\n",
    "\n",
    "\n",
    "def coedit_large_model_process(batch):\n",
    "    input_ids = coedit_large_tokenizer(batch[\"task\"], padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = coedit_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids\n",
    "    outputs = coedit_large_model.generate(input_ids, max_length=256)\n",
    "    # print(f\"outputs: {outputs}\")\n",
    "    processed = coedit_large_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    print_utilization(utilization)\n",
    "\n",
    "    return {\"processed\": processed}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "utilization = calculate_utilization()\n",
    "print_utilization_header(utilization)\n",
    "print_utilization(utilization)\n",
    "\n",
    "# process_samples = samples.map(coedit_large_model_process, num_proc=torch.cuda.device_count())\n",
    "process_samples = samples.map(coedit_large_model_process, num_proc=1, batched=True, batch_size=20)\n",
    "pprint(process_samples)\n",
    "pprint(process_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function flan_t5_large_model_process at 0x7fd4d1463c40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function flan_t5_large_model_process at 0x7fd4d1463c40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe3177ce6f8457f8b7b47610cf732dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'coedit_large_processed', 'flan_t5_large_processed'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "def flan_t5_large_model_process(item):\n",
    "    input_ids = flan_t5_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = flan_t5_large_model.generate(input_ids, max_length=256)\n",
    "    processed = flan_t5_large_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"flan_t5_large_processed\": processed}\n",
    "\n",
    "\n",
    "process_samples = process_samples.map(flan_t5_large_model_process, num_proc=torch.cuda.device_count())\n",
    "pprint(process_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_samples' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "process_samples[:2]\n",
    "\n",
    "rouge_score = rouge_metric.compute(\n",
    "    predictions=process_samples['coedit_large_processed'], references=process_samples['references']\n",
    ")\n",
    "pprint(rouge_score)\n",
    "# rouge_score = rouge_metric.compute(\n",
    "#     predictions=process_samples['flan_t5_large_processed'], references=process_samples['references']\n",
    "# )\n",
    "# pprint(rouge_score)\n",
    "\n",
    "sacreblue_score = sacreblue_metric.compute(predictions=process_samples['coedit_large_processed'], references=process_samples['references'])\n",
    "pprint(sacreblue_score)\n",
    "# sacreblue_score = sacreblue_metric.compute(predictions=process_samples['flan_t5_large_processed'], references=process_samples['references'])\n",
    "# pprint(sacreblue_score)\n",
    "\n",
    "sari_score = sari_metric.compute(\n",
    "  sources=process_samples['source'],\n",
    "  predictions=process_samples['coedit_large_processed'],\n",
    "  references=process_samples['references']\n",
    ")\n",
    "pprint(sari_score)\n",
    "# sari_score = sari_metric.compute(\n",
    "#   sources=process_samples['source'],\n",
    "#   predictions=process_samples['flan_t5_large_processed'],\n",
    "#   references=process_samples['references']\n",
    "# )\n",
    "# pprint(sari_score)\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=process_samples['coedit_large_processed'], references=process_samples['reference']\n",
    ")\n",
    "pprint(score)\n",
    "# score = em_metric.compute(\n",
    "#     predictions=process_samples['flan_t5_large_processed'], references=process_samples['reference']\n",
    "# )\n",
    "# pprint(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
