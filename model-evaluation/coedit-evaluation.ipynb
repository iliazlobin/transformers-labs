{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (4.39.2)\n",
      "Requirement already satisfied: evaluate in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: filelock in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from evaluate) (2.18.0)\n",
      "Requirement already satisfied: dill in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from evaluate) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: absl-py in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (2.1.0)\n",
      "Requirement already satisfied: rouge_score in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: click in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from rouge_score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from rouge_score) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bleu in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (0.3)\n",
      "Requirement already satisfied: sacrebleu in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: efficiency in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from bleu) (2.0)\n",
      "Requirement already satisfied: portalocker in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: regex in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacrebleu) (1.26.3)\n",
      "Requirement already satisfied: colorama in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacrebleu) (5.2.1)\n",
      "Requirement already satisfied: pandas in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from efficiency->bleu) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->efficiency->bleu) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->efficiency->bleu) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->efficiency->bleu) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->efficiency->bleu) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bleu in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (0.3)\n",
      "Requirement already satisfied: sacremoses in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: efficiency in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from bleu) (2.0)\n",
      "Requirement already satisfied: regex in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacremoses) (2023.12.25)\n",
      "Requirement already satisfied: click in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacremoses) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from sacremoses) (4.66.2)\n",
      "Requirement already satisfied: pandas in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from efficiency->bleu) (2.2.1)\n",
      "Requirement already satisfied: numpy in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from efficiency->bleu) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->efficiency->bleu) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->efficiency->bleu) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from pandas->efficiency->bleu) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->efficiency->bleu) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/izlobin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# %%script echo skipping\n",
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%pip install transformers evaluate\n",
    "%pip install nltk absl-py rouge_score\n",
    "%pip install bleu sacrebleu\n",
    "%pip install bleu sacremoses\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, AutoTokenizer, T5Tokenizer\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lading coedit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 3.06 GB\n",
      "Reserved: 3.06 GB\n"
     ]
    }
   ],
   "source": [
    "coedit_large_tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-large\")\n",
    "# coedit_large_model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\", device_map=0)\n",
    "coedit_large_model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\")\n",
    "coedit_large_model=coedit_large_model.to(device)\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device)/1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(device)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total/used/cuda/res/ram(Gb): 10.00/4.88/3.06/3.06/18.03\n",
      "Available memory: 5492998144.00 GB\n",
      "Recommended fraction: 0.51\n",
      "Set memory fraction: 0.95\n"
     ]
    }
   ],
   "source": [
    "from utils.monitoring import calculate_utilization, format_utilization_narrow, print_utilization\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram(Gb): {utilization_str[\"total_memory\"]}/{utilization_str[\"memory_used\"]}/\"\n",
    "    f\"{utilization_str[\"cuda_allocated\"]}/{utilization_str[\"cuda_reserved\"]}/{utilization_str[\"ram_usage\"]}\"\n",
    ")\n",
    "\n",
    "available_memory = utilization[\"total_memory\"] - utilization[\"memory_used\"]\n",
    "recommended_fraction = available_memory / utilization[\"total_memory\"]\n",
    "print(f\"Available memory: {available_memory:.2f} GB\")\n",
    "print(f\"Recommended fraction: {recommended_fraction:.2f}\")\n",
    "# print(f\"Set memory fraction: {recommended_fraction:.2f}\")\n",
    "# torch.cuda.set_per_process_memory_fraction(recommended_fraction, 0)\n",
    "print(f\"Set memory fraction: 0.95\")\n",
    "torch.cuda.set_per_process_memory_fraction(0.95, 0)\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.empty(utilization[\"total_memory\"] // 2, dtype=torch.int8, device='cuda')\n",
    "# print_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "flan_t5_large_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\", return_attention_mask=False)\n",
    "flan_t5_large_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n",
    "# flan_t5_large_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
    "# flan_t5_large_model = flan_t5_large_model.to(device)\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device)/1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(device)/1024**3:.2f} GB\")\n",
    "\n",
    "total_params = sum(p.numel() for p in flan_t5_large_model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "total_trainable_params = sum(p.numel() for p in flan_t5_large_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {total_trainable_params}\")\n",
    "# total_memory_GB = total_params * 4 / (1024**3)\n",
    "# print(f\"Estimated model memory: {total_memory_GB:.2f} GB\")\n",
    "# for param_tensor in flan_t5_large_model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", flan_t5_large_model.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "prompt = \"translate English to German: How old are you?\"\n",
    "input_ids = flan_t5_large_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = flan_t5_large_model.generate(input_ids, max_new_tokens=200)\n",
    "print(flan_t5_large_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 69071\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1712\n",
      "    })\n",
      "})\n",
      "{'train', 'validation'}\n",
      "{'neutralize', 'clarity', 'coherence', 'paraphrase', 'gec', 'simplification'}\n"
     ]
    }
   ],
   "source": [
    "# api = HfApi()\n",
    "# coedit_info = api.dataset_info(\"grammarly/coedit\")\n",
    "# pprint(coedit_info)\n",
    "\n",
    "grammarly_dataset = load_dataset(\"grammarly/coedit\")\n",
    "pprint(grammarly_dataset)\n",
    "\n",
    "unique_categories = set(grammarly_dataset)\n",
    "pprint(unique_categories)\n",
    "\n",
    "unique_tasks = set(grammarly_dataset[\"train\"][\"task\"])\n",
    "pprint(unique_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "[gec] Fix grammaticality in this sentence\n",
      "src: Dear friends, I hope you should correctly but I can gives you some opinion, I guess that is a good idea if you go to a small schools, under you can met a lot on people and there are more closed friend of course you cannot like that opcion if you like the biggest once, so in that ways you can go from the other school.\n",
      "tgt: Dear friend, I hope you choose correctly but I can give you my opinion. I guess that it's a good idea if you go to a small school, because you can meet a lot of people and make more close friends of course you won't like that option if you like the bigger one, so in that case you should go to the other school.\n"
     ]
    }
   ],
   "source": [
    "def get_samples(dataset, category=\"validation\", task=\"gec\", num_samples=1, seed=42):\n",
    "    return dataset[category].shuffle(seed=seed).filter(lambda item: item[\"task\"] == task).select(range(num_samples))\n",
    "\n",
    "def print_samples(samples) -> None:\n",
    "    for item in samples:\n",
    "        pfx, src = item[\"src\"].split(\": \", 1)\n",
    "        print(f\"[{item['task']}] {pfx}\")\n",
    "        print(f\"src: {src}\")\n",
    "        print(f\"tgt: {item['tgt']}\")\n",
    "\n",
    "\n",
    "print_samples(get_samples(grammarly_dataset, num_samples=2))\n",
    "\n",
    "# input_ids = coedit_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "# outputs = coedit_large_model.generate(input_ids, max_length=256)\n",
    "# corrected = coedit_large_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# return {\"processed\": corrected}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'rouge1': 0.7140847931715504,\n",
      " 'rouge2': 0.5156415559967662,\n",
      " 'rougeL': 0.7001989590321291,\n",
      " 'rougeLsum': 0.7000037328452997}\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = rouge_metric.compute(\n",
    "    predictions=samples['src'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _GLUE metric_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "tensor([14269, 19519,    16,    48,  7142,    10,   493,  6195,   388,    55,\n",
      "            1])\n",
      "tensor([ 493, 6195,    6,  388,   55,    1])\n"
     ]
    }
   ],
   "source": [
    "glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=2)\n",
    "pprint(object=samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "src_input_ids = coedit_large_tokenizer(samples[\"src\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "tgt_input_ids = coedit_large_tokenizer(samples[\"tgt\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "pprint(src_input_ids[0])\n",
    "pprint(tgt_input_ids[0])\n",
    "\n",
    "# score = glue_metric.compute(predictions=src_input_ids[0], references=tgt_input_ids[0])\n",
    "# score = glue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "# pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SacreBLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'bp': 1.0,\n",
      " 'counts': [3886, 2743, 1965, 1419],\n",
      " 'precisions': [70.79613773000547,\n",
      "                50.899981443681575,\n",
      "                37.152580828133864,\n",
      "                27.346309500867218],\n",
      " 'ref_len': 5090,\n",
      " 'score': 43.74251258938969,\n",
      " 'sys_len': 5489,\n",
      " 'totals': [5489, 5389, 5289, 5189]}\n"
     ]
    }
   ],
   "source": [
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = sacreblue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARI metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'sari': 52.48853096503606}\n"
     ]
    }
   ],
   "source": [
    "sari_metric = evaluate.load(\"sari\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "new_samples = samples.map(lambda item: {\"tgts\": [item[\"tgt\"]]})\n",
    "new_samples[\"tgts\"][:5]\n",
    "\n",
    "# sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "# predictions=[\"About 95 you now get in.\",\"About 95 you now get in.\"]\n",
    "# references=[[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"],[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"]]\n",
    "\n",
    "score = sari_metric.compute(\n",
    "  sources=new_samples['src'],\n",
    "  predictions=new_samples['src'],\n",
    "  references=new_samples['tgts']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact match (EM) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'exact_match': 1.0}\n"
     ]
    }
   ],
   "source": [
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=samples['tgt'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IteraTeR\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_v2\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_full_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 157579\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 19705\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 19703\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "    num_rows: 19705\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# iterater_dataset = load_dataset(\"wanyu/IteraTeR_v2\") # human in the loop\n",
    "iterater_dataset = load_dataset(\"wanyu/IteraTeR_full_sent\")\n",
    "pprint(iterater_dataset)\n",
    "iterater_validation_dataset = load_dataset(\"wanyu/IteraTeR_full_sent\", split=\"validation\")\n",
    "pprint(iterater_validation_dataset)\n",
    "# pprint(iterater_validation_dataset['validation'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbolizers = {\n",
    "    \"gce\": {\n",
    "        \"tokens\": [\"<fluency>\"],\n",
    "        \"verbs\": [\n",
    "            \"Fix grammar\",\n",
    "            \"Fix grammar in this sentence\",\n",
    "            \"Fix grammar in the sentence\",\n",
    "            \"Fix grammar errors\",\n",
    "            \"Fix grammatical errors\",\n",
    "            \"Fix grammaticality\",\n",
    "            \"Fix all grammatical errors\",\n",
    "            \"Fix grammatical errors in this sentence\",\n",
    "            \"Fix grammar errors in this sentence\",\n",
    "            \"Fix grammatical mistakes in this sentence\",\n",
    "            \"Fix grammaticality in this sentence\",\n",
    "            \"Fix grammaticality of the sentence\",\n",
    "            \"Fix disfluencies in the sentence\",\n",
    "            \"Make the sentence grammatical\",\n",
    "            \"Make the sentence fluent\",\n",
    "            \"Fix errors in this text\",\n",
    "            \"Update to remove grammar errors\",\n",
    "            \"Remove all grammatical errors from this text\",\n",
    "            \"Improve the grammar of this text\",\n",
    "            \"Improve the grammaticality\",\n",
    "            \"Improve the grammaticality of this text\",\n",
    "            \"Improve the grammaticality of this sentence,\",\n",
    "            \"Grammar improvements\",\n",
    "            \"Remove grammar mistakes\",\n",
    "            \"Remove grammatical mistakes\",\n",
    "            \"Fix the grammar mistakes\",\n",
    "            \"Fix grammatical mistakes\",\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5078, selected: 5, num_samples: 5\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 5\n",
      "})\n",
      "[\"Fix grammar:  We don't have enough good Open Source games -- it's a waste to \"\n",
      " 'pour all the resources we have into one. :) Wesnoth has dwarves with guns, '\n",
      " \"World of Warcraft'' has gnomes and goblins with explosives and flying \"\n",
      " 'machines -- where do you, personally, define the limits of the fantasy '\n",
      " 'genre?',\n",
      " 'Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian '\n",
      " 'Hall, still a vibrant community center/beer garden started by Czech '\n",
      " 'immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, '\n",
      " 'continuously occupied Latin music store in New York City,  as census sites '\n",
      " 'to the National Register of Historic Places.']\n"
     ]
    }
   ],
   "source": [
    "def substitute_verbolizer(text, verbolizer, count=[0]):\n",
    "    verbs = verbolizers[verbolizer][\"verbs\"]\n",
    "\n",
    "    verb = verbs[count[0]]\n",
    "    tokens = verbolizers[verbolizer][\"tokens\"]\n",
    "    replaced_text = text\n",
    "    for t in tokens:\n",
    "        replaced_text = text.replace(t, f\"{verb}:\")\n",
    "        # pprint(f\"> t: {t}, verb: {verb}, text: {text}, replaced_text: {replaced_text}\")\n",
    "\n",
    "    count[0] += 1\n",
    "    if count[0] >= len(verbs):\n",
    "        count[0] = 0\n",
    "\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "def get_iterater_samples(label, category=\"validation\", num_samples=0, seed=42, confidence_threshold=0.9):\n",
    "    filtered_samples = (\n",
    "        iterater_dataset[category]\n",
    "        .shuffle(seed=seed)\n",
    "        .filter(lambda item: item[\"labels\"] == label and float(item[\"confidence\"]) >= confidence_threshold)\n",
    "    )\n",
    "    max_samples = len(filtered_samples)\n",
    "    selected = max_samples if num_samples == 0 else num_samples\n",
    "    print(f\"max_samples: {max_samples}, selected: {selected}, num_samples: {num_samples}\")\n",
    "    samples = filtered_samples.select(range(selected))\n",
    "\n",
    "    return samples.map(\n",
    "        lambda item: {\n",
    "            \"task\": substitute_verbolizer(item[\"before_sent_with_intent\"], \"gce\"),\n",
    "            \"source\": item[\"before_sent\"],\n",
    "            \"reference\": item[\"after_sent\"],\n",
    "            \"references\": [item[\"after_sent\"]],\n",
    "        },\n",
    "        remove_columns=[\n",
    "            \"before_sent_with_intent\",\n",
    "            \"before_sent\",\n",
    "            \"after_sent\",\n",
    "            \"labels\",\n",
    "            \"confidence\",\n",
    "            \"doc_id\",\n",
    "            \"revision_depth\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=5)\n",
    "pprint(samples)\n",
    "pprint(samples[\"task\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_samples: 5078, selected: 100, num_samples: 100\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 100\n",
      "})\n",
      "total/used/cuda/res/ram(Gb): 10.00/9.43/3.07/7.42/18.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cab15afabf473293e3c18c87e49c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total/used/cuda/res/ram(Gb): 10.00/9.42/3.07/7.42/18.44\n",
      "batch/sps: 20/6.92\n",
      "total/used/cuda/res/ram(Gb): 10.00/9.41/3.07/7.42/18.41\n",
      "batch/sps: 20/5.21\n",
      "total/used/cuda/res/ram(Gb): 10.00/9.44/3.07/7.42/18.41\n",
      "batch/sps: 20/5.47\n",
      "total/used/cuda/res/ram(Gb): 10.00/9.42/3.07/7.42/18.41\n",
      "batch/sps: 20/3.29\n",
      "total/used/cuda/res/ram(Gb): 10.00/9.44/3.07/7.42/18.41\n",
      "batch/sps: 20/7.62\n",
      "processed_sps: 4.241775491066228\n",
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'processed'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[\"We don't have enough good Open Source games -- it's a waste to pour all the \"\n",
      " 'resources we have into one. :) Wesnoth has dwarves with guns, World of '\n",
      " \"Warcraft'' has gnomes and goblins with explosives, and flying machines -- \"\n",
      " 'where do you, personally, define the limits of the fantasy genre?',\n",
      " 'In 2001, they successfully nominated Bohemian Hall, still a vibrant '\n",
      " 'community center/beer garden started by Czech immigrants in Astoria, Queens, '\n",
      " 'and the Casa Amadeo Music Store, the oldest, continuously occupied Latin '\n",
      " 'music store in New York City, as census sites to the National Register of '\n",
      " 'Historic Places.']\n",
      "CPU times: user 19.9 s, sys: 3.72 s, total: 23.6 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "total_samples = 100\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=total_samples)\n",
    "# samples = get_iterater_samples(label=\"fluency\")\n",
    "pprint(samples)\n",
    "\n",
    "processed_samples = samples\n",
    "\n",
    "\n",
    "def coedit_large_model_process(batch, **kwargs):\n",
    "    num_samples = len(batch[\"task\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = kwargs.get(\"model\")\n",
    "    tokenizer = kwargs.get(\"tokenizer\")\n",
    "\n",
    "    input_ids = tokenizer(batch[\"task\"], padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_length=256)\n",
    "    # print(f\"outputs: {outputs}\")\n",
    "    processed = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    sps = num_samples / elapsed_time\n",
    "    sps_str = f\"{sps:.2f}\"\n",
    "\n",
    "    utilization = calculate_utilization()\n",
    "    utilization_str = format_utilization_narrow(utilization)\n",
    "    print(\n",
    "        f\"total/used/cuda/res/ram(Gb): {utilization_str[\"total_memory\"]}/{utilization_str[\"memory_used\"]}/\"\n",
    "        f\"{utilization_str[\"cuda_allocated\"]}/{utilization_str[\"cuda_reserved\"]}/{utilization_str[\"ram_usage\"]}\"\n",
    "    )\n",
    "    print(f\"batch/sps: {num_samples}/{sps_str}\")\n",
    "\n",
    "    # return {\"processed\": processed}\n",
    "    return {\"processed\": processed}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization, \"tps\": tps}\n",
    "    # return {\"processed\": processed, \"utilization\": utilization}\n",
    "\n",
    "\n",
    "utilization = calculate_utilization()\n",
    "utilization_str = format_utilization_narrow(utilization)\n",
    "print(\n",
    "    f\"total/used/cuda/res/ram(Gb): {utilization_str[\"total_memory\"]}/{utilization_str[\"memory_used\"]}/\"\n",
    "    f\"{utilization_str[\"cuda_allocated\"]}/{utilization_str[\"cuda_reserved\"]}/{utilization_str[\"ram_usage\"]}\"\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# processed_samples = samples.map(coedit_large_model_process, num_proc=torch.cuda.device_count())\n",
    "processed_samples = samples.map(\n",
    "    coedit_large_model_process,\n",
    "    fn_kwargs={\n",
    "        \"model\": coedit_large_model,\n",
    "        \"tokenizer\": coedit_large_tokenizer,\n",
    "    },\n",
    "    num_proc=1,\n",
    "    batched=True,\n",
    "    batch_size=20,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "processed_sps = total_samples / elapsed_time\n",
    "print(f\"processed_sps: {processed_sps}\")\n",
    "\n",
    "pprint(processed_samples)\n",
    "pprint(processed_samples[\"processed\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em': {'exact_match': 0.0},\n",
      " 'rouge': {'rouge1': 0.9650770044749972,\n",
      "           'rouge2': 0.9390297768784843,\n",
      "           'rougeL': 0.9646291951226759,\n",
      "           'rougeLsum': 0.9649260185181687},\n",
      " 'sacreblue': {'bp': 0.9770939403744792,\n",
      "               'counts': [3998, 3678, 3396, 3134],\n",
      "               'precisions': [95.50883898709985,\n",
      "                              90.01468428781205,\n",
      "                              85.19819367787255,\n",
      "                              80.64848172928461],\n",
      "               'ref_len': 4283,\n",
      "               'score': 85.66081415729968,\n",
      "               'sys_len': 4186,\n",
      "               'totals': [4186, 4086, 3986, 3886]},\n",
      " 'sari': {'sari': 52.33387425863478}}\n",
      "|    | model                  |   total_samples |     sps | task    |   score.rouge.rouge1 |   score.sacreblue.score |   score.sari.sari |   score.em.exact_match |\n",
      "|---:|:-----------------------|----------------:|--------:|:--------|---------------------:|------------------------:|------------------:|-----------------------:|\n",
      "|  0 | grammarly/coedit-large |             100 | 3.49321 | fluency |             0.965077 |                 85.6608 |           52.3339 |                      0 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.metric import calculate_scores\n",
    "\n",
    "model_name = \"grammarly/coedit-large\"\n",
    "model_alias = model_name.replace(\"/\", \"_\")\n",
    "\n",
    "scores = calculate_scores(processed_samples)\n",
    "pprint(scores)\n",
    "\n",
    "score_paths = [\n",
    "    \"rouge.rouge1\",\n",
    "    # \"rouge.rouge2\",\n",
    "    # \"rouge.rougeL\",\n",
    "    # \"rouge.rougeLsum\",\n",
    "    \"sacreblue.score\",\n",
    "    \"sari.sari\",\n",
    "    \"em.exact_match\",\n",
    "]\n",
    "\n",
    "base_frame = {\n",
    "    \"model\": model_name,\n",
    "    \"total_samples\": total_samples,\n",
    "    \"sps\": processed_sps,\n",
    "    \"task\": \"fluency\",\n",
    "}\n",
    "\n",
    "normalized_scores = {}\n",
    "for k, v in scores.items():\n",
    "    for k2, v2 in v.items():\n",
    "        if not isinstance(v2, list):\n",
    "            # normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "            path = f\"{k}.{k2}\"\n",
    "            if path in score_paths:\n",
    "                normalized_scores[f\"score.{k}.{k2}\"] = v2\n",
    "# pprint(normalized_scores)\n",
    "\n",
    "flat_frame = base_frame.copy()\n",
    "flat_frame.update(normalized_scores)\n",
    "# pprint(frame)\n",
    "\n",
    "all_flat_frames = []\n",
    "\n",
    "flat_df = pd.DataFrame.from_records([flat_frame])\n",
    "# pprint(df)\n",
    "print(flat_df.head().to_markdown(index=True))\n",
    "flat_df.to_csv(f\"results/{model_alias}_flat-frame.csv\", index=False)\n",
    "\n",
    "all_flat_frames.append(flat_frame)\n",
    "all_flat_frames.append(flat_frame)\n",
    "all_flat_dfs = pd.DataFrame.from_records(all_flat_frames)\n",
    "all_flat_dfs.to_csv(f\"results/all-flat-frames.csv\", index=False)\n",
    "all_flat_dfs.to_csv(f\"results/all-flat-frames.csv\", index=False)\n",
    "\n",
    "\n",
    "full_frame = base_frame.copy()\n",
    "full_frame.update({\"scores\": scores})\n",
    "full_df = pd.DataFrame.from_records(full_frame)\n",
    "\n",
    "full_df.to_json(f\"results/{model_alias}_full-frame.json\", orient=\"index\")\n",
    "# full_df.to_json(\"results/frame.json\", index=False)\n",
    "\n",
    "# more_frames = [frame, frame, frame]\n",
    "# more_df = pd.DataFrame.from_records(more_frames)\n",
    "# more_df.to_csv(\"results/frames.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_samples' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "process_samples[:2]\n",
    "\n",
    "rouge_score = rouge_metric.compute(\n",
    "    predictions=process_samples['coedit_large_processed'], references=process_samples['references']\n",
    ")\n",
    "pprint(rouge_score)\n",
    "# rouge_score = rouge_metric.compute(\n",
    "#     predictions=process_samples['flan_t5_large_processed'], references=process_samples['references']\n",
    "# )\n",
    "# pprint(rouge_score)\n",
    "\n",
    "sacreblue_score = sacreblue_metric.compute(predictions=process_samples['coedit_large_processed'], references=process_samples['references'])\n",
    "pprint(sacreblue_score)\n",
    "# sacreblue_score = sacreblue_metric.compute(predictions=process_samples['flan_t5_large_processed'], references=process_samples['references'])\n",
    "# pprint(sacreblue_score)\n",
    "\n",
    "sari_score = sari_metric.compute(\n",
    "  sources=process_samples['source'],\n",
    "  predictions=process_samples['coedit_large_processed'],\n",
    "  references=process_samples['references']\n",
    ")\n",
    "pprint(sari_score)\n",
    "# sari_score = sari_metric.compute(\n",
    "#   sources=process_samples['source'],\n",
    "#   predictions=process_samples['flan_t5_large_processed'],\n",
    "#   references=process_samples['references']\n",
    "# )\n",
    "# pprint(sari_score)\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=process_samples['coedit_large_processed'], references=process_samples['reference']\n",
    ")\n",
    "pprint(score)\n",
    "# score = em_metric.compute(\n",
    "#     predictions=process_samples['flan_t5_large_processed'], references=process_samples['reference']\n",
    "# )\n",
    "# pprint(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
