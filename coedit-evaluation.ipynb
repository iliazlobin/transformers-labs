{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%pip install transformers evaluate\n",
    "%pip install nltk absl-py rouge_score\n",
    "%pip install bleu sacrebleu\n",
    "%pip install bleu sacremoses\n",
    "!huggingface-cli login --token $HUGGING_FACE_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Device: cuda'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM, AutoTokenizer, T5Tokenizer\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lading coedit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 7.47 GB\n",
      "Reserved: 7.47 GB\n"
     ]
    }
   ],
   "source": [
    "coedit_large_tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-large\")\n",
    "# coedit_large_model=coedit_large_model.to(device)\n",
    "coedit_large_model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\")\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device)/1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(device)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 3.67 GB\n",
      "Reserved: 8.35 GB\n"
     ]
    }
   ],
   "source": [
    "coedit_large_tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-large\")\n",
    "coedit_large_model=coedit_large_model.to(device)\n",
    "# coedit_large_model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\")\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device)/1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(device)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you?\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skipping\n",
    "\n",
    "prompt = \"fix grammar: How is are you?\"\n",
    "input_ids = coedit_large_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = coedit_large_model.generate(input_ids, max_new_tokens=200)\n",
    "print(coedit_large_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e67462cbeef457d8476461288628ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 8.07 GB\n",
      "Reserved: 8.07 GB\n",
      "Total Parameters: 2849757184\n",
      "Trainable Parameters: 2849757184\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skipping\n",
    "\n",
    "flan_t5_large_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\", return_attention_mask=False)\n",
    "flan_t5_large_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")\n",
    "# flan_t5_large_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
    "# flan_t5_large_model = flan_t5_large_model.to(device)\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device)/1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(device)/1024**3:.2f} GB\")\n",
    "\n",
    "total_params = sum(p.numel() for p in flan_t5_large_model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "total_trainable_params = sum(p.numel() for p in flan_t5_large_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {total_trainable_params}\")\n",
    "# total_memory_GB = total_params * 4 / (1024**3)\n",
    "# print(f\"Estimated model memory: {total_memory_GB:.2f} GB\")\n",
    "# for param_tensor in flan_t5_large_model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", flan_t5_large_model.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie alt sind Sie?\n"
     ]
    }
   ],
   "source": [
    "# %%script echo skipping\n",
    "\n",
    "prompt = \"translate English to German: How old are you?\"\n",
    "input_ids = flan_t5_large_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = flan_t5_large_model.generate(input_ids, max_new_tokens=200)\n",
    "print(flan_t5_large_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 69071\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['_id', 'task', 'src', 'tgt'],\n",
      "        num_rows: 1712\n",
      "    })\n",
      "})\n",
      "{'validation', 'train'}\n",
      "{'gec', 'clarity', 'simplification', 'paraphrase', 'neutralize', 'coherence'}\n"
     ]
    }
   ],
   "source": [
    "# api = HfApi()\n",
    "# coedit_info = api.dataset_info(\"grammarly/coedit\")\n",
    "# pprint(coedit_info)\n",
    "\n",
    "grammarly_dataset = load_dataset(\"grammarly/coedit\")\n",
    "pprint(grammarly_dataset)\n",
    "\n",
    "unique_categories = set(grammarly_dataset)\n",
    "pprint(unique_categories)\n",
    "\n",
    "unique_tasks = set(grammarly_dataset[\"train\"][\"task\"])\n",
    "pprint(unique_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "[gec] Fix grammaticality in this sentence\n",
      "src: Dear friends, I hope you should correctly but I can gives you some opinion, I guess that is a good idea if you go to a small schools, under you can met a lot on people and there are more closed friend of course you cannot like that opcion if you like the biggest once, so in that ways you can go from the other school.\n",
      "tgt: Dear friend, I hope you choose correctly but I can give you my opinion. I guess that it's a good idea if you go to a small school, because you can meet a lot of people and make more close friends of course you won't like that option if you like the bigger one, so in that case you should go to the other school.\n"
     ]
    }
   ],
   "source": [
    "def get_samples(dataset, category=\"validation\", task=\"gec\", num_samples=1, seed=42):\n",
    "    return dataset[category].shuffle(seed=seed).filter(lambda item: item[\"task\"] == task).select(range(num_samples))\n",
    "\n",
    "def print_samples(samples) -> None:\n",
    "    for item in samples:\n",
    "        pfx, src = item[\"src\"].split(\": \", 1)\n",
    "        print(f\"[{item['task']}] {pfx}\")\n",
    "        print(f\"src: {src}\")\n",
    "        print(f\"tgt: {item['tgt']}\")\n",
    "\n",
    "\n",
    "print_samples(get_samples(grammarly_dataset, num_samples=2))\n",
    "\n",
    "# input_ids = coedit_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "# outputs = coedit_large_model.generate(input_ids, max_length=256)\n",
    "# corrected = coedit_large_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# return {\"processed\": corrected}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rouge metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'rouge1': 0.714011352170972,\n",
      " 'rouge2': 0.5157414622968349,\n",
      " 'rougeL': 0.7009413705046074,\n",
      " 'rougeLsum': 0.7006342681879616}\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = rouge_metric.compute(\n",
    "    predictions=samples['src'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _GLUE metric_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "tensor([14269, 19519,    16,    48,  7142,    10,   493,  6195,   388,    55,\n",
      "            1])\n",
      "tensor([ 493, 6195,    6,  388,   55,    1])\n"
     ]
    }
   ],
   "source": [
    "glue_metric = evaluate.load(\"glue\", \"stsb\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=2)\n",
    "pprint(object=samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "src_input_ids = coedit_large_tokenizer(samples[\"src\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "tgt_input_ids = coedit_large_tokenizer(samples[\"tgt\"][0], return_tensors=\"pt\", padding=True).input_ids\n",
    "pprint(src_input_ids[0])\n",
    "pprint(tgt_input_ids[0])\n",
    "\n",
    "# score = glue_metric.compute(predictions=src_input_ids[0], references=tgt_input_ids[0])\n",
    "# score = glue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "# pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SacreBLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'bp': 1.0,\n",
      " 'counts': [3886, 2743, 1965, 1419],\n",
      " 'precisions': [70.79613773000547,\n",
      "                50.899981443681575,\n",
      "                37.152580828133864,\n",
      "                27.346309500867218],\n",
      " 'ref_len': 5090,\n",
      " 'score': 43.74251258938969,\n",
      " 'sys_len': 5489,\n",
      " 'totals': [5489, 5389, 5289, 5189]}\n"
     ]
    }
   ],
   "source": [
    "sacreblue_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = sacreblue_metric.compute(predictions=samples[\"src\"], references=samples[\"tgt\"])\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARI metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'sari': 52.48853096503606}\n"
     ]
    }
   ],
   "source": [
    "sari_metric = evaluate.load(\"sari\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "new_samples = samples.map(lambda item: {\"tgts\": [item[\"tgt\"]]})\n",
    "new_samples[\"tgts\"][:5]\n",
    "\n",
    "# sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "# predictions=[\"About 95 you now get in.\",\"About 95 you now get in.\"]\n",
    "# references=[[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"],[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"]]\n",
    "\n",
    "score = sari_metric.compute(\n",
    "  sources=new_samples['src'],\n",
    "  predictions=new_samples['src'],\n",
    "  references=new_samples['tgts']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact match (EM) metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['_id', 'task', 'src', 'tgt'],\n",
      "    num_rows: 100\n",
      "})\n",
      "[gec] Fix grammar in this sentence\n",
      "src: Be careful man!\n",
      "tgt: Be careful, man!\n",
      "{'exact_match': 1.0}\n"
     ]
    }
   ],
   "source": [
    "em_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "samples = get_samples(grammarly_dataset, task=\"gec\", num_samples=100)\n",
    "pprint(samples)\n",
    "print_samples([samples[0]])\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=samples['tgt'], references=samples['tgt']\n",
    ")\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IteraTeR\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_v2\n",
    "* https://huggingface.co/datasets/wanyu/IteraTeR_full_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 157579\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 19705\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "        num_rows: 19703\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['before_sent', 'before_sent_with_intent', 'after_sent', 'labels', 'confidence', 'doc_id', 'revision_depth'],\n",
      "    num_rows: 19705\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# iterater_dataset = load_dataset(\"wanyu/IteraTeR_v2\") # human in the loop\n",
    "iterater_dataset = load_dataset(\"wanyu/IteraTeR_full_sent\")\n",
    "pprint(iterater_dataset)\n",
    "iterater_validation_dataset = load_dataset(\"wanyu/IteraTeR_full_sent\", split=\"validation\")\n",
    "pprint(iterater_validation_dataset)\n",
    "# pprint(iterater_validation_dataset['validation'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbolizers = {\n",
    "    \"gce\": {\n",
    "        \"tokens\": [\"<fluency>\"],\n",
    "        \"verbs\": [\n",
    "            \"Fix grammar\",\n",
    "            \"Fix grammar in this sentence\",\n",
    "            \"Fix grammar in the sentence\",\n",
    "            \"Fix grammar errors\",\n",
    "            \"Fix grammatical errors\",\n",
    "            \"Fix grammaticality\",\n",
    "            \"Fix all grammatical errors\",\n",
    "            \"Fix grammatical errors in this sentence\",\n",
    "            \"Fix grammar errors in this sentence\",\n",
    "            \"Fix grammatical mistakes in this sentence\",\n",
    "            \"Fix grammaticality in this sentence\",\n",
    "            \"Fix grammaticality of the sentence\",\n",
    "            \"Fix disfluencies in the sentence\",\n",
    "            \"Make the sentence grammatical\",\n",
    "            \"Make the sentence fluent\",\n",
    "            \"Fix errors in this text\",\n",
    "            \"Update to remove grammar errors\",\n",
    "            \"Remove all grammatical errors from this text\",\n",
    "            \"Improve the grammar of this text\",\n",
    "            \"Improve the grammaticality\",\n",
    "            \"Improve the grammaticality of this text\",\n",
    "            \"Improve the grammaticality of this sentence,\",\n",
    "            \"Grammar improvements\",\n",
    "            \"Remove grammar mistakes\",\n",
    "            \"Remove grammatical mistakes\",\n",
    "            \"Fix the grammar mistakes\",\n",
    "            \"Fix grammatical mistakes\",\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 5\n",
      "})\n",
      "[\"Fix grammar:  We don't have enough good Open Source games -- it's a waste to \"\n",
      " 'pour all the resources we have into one. :) Wesnoth has dwarves with guns, '\n",
      " \"World of Warcraft'' has gnomes and goblins with explosives and flying \"\n",
      " 'machines -- where do you, personally, define the limits of the fantasy '\n",
      " 'genre?',\n",
      " 'Fix grammar in this sentence:  In 2001, they successfully nominated Bohemian '\n",
      " 'Hall, still a vibrant community center/beer garden started by Czech '\n",
      " 'immigrants in Astoria, Queens, and the Casa Amadeo Music Store, the oldest, '\n",
      " 'continuously occupied Latin music store in New York City,  as census sites '\n",
      " 'to the National Register of Historic Places.']\n"
     ]
    }
   ],
   "source": [
    "def substitute_verbolizer(text, verbolizer, count=[0]):\n",
    "    verbs = verbolizers[verbolizer][\"verbs\"]\n",
    "\n",
    "    verb = verbs[count[0]]\n",
    "    tokens = verbolizers[verbolizer][\"tokens\"]\n",
    "    replaced_text = text\n",
    "    for t in tokens:\n",
    "        replaced_text = text.replace(t, f\"{verb}:\")\n",
    "        # pprint(f\"> t: {t}, verb: {verb}, text: {text}, replaced_text: {replaced_text}\")\n",
    "\n",
    "    count[0] += 1\n",
    "    if count[0] >= len(verbs):\n",
    "        count[0] = 0\n",
    "\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "def get_iterater_samples(label, category=\"validation\", num_samples=1, seed=42, confidence_threshold=0.9):\n",
    "    samples = (\n",
    "        iterater_dataset[category]\n",
    "        .shuffle(seed=seed)\n",
    "        .filter(lambda item: item[\"labels\"] == label and float(item[\"confidence\"]) >= confidence_threshold)\n",
    "        .select(range(num_samples))\n",
    "    )\n",
    "    return samples.map(\n",
    "        lambda item: {\n",
    "            \"task\": substitute_verbolizer(item[\"before_sent_with_intent\"], \"gce\"),\n",
    "            \"source\": item[\"before_sent\"],\n",
    "            \"reference\": item[\"after_sent\"],\n",
    "            \"references\": [item[\"after_sent\"]],\n",
    "        },\n",
    "        remove_columns=[\n",
    "            \"before_sent_with_intent\",\n",
    "            \"before_sent\",\n",
    "            \"after_sent\",\n",
    "            \"labels\",\n",
    "            \"confidence\",\n",
    "            \"doc_id\",\n",
    "            \"revision_depth\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=5)\n",
    "pprint(samples)\n",
    "pprint(samples[\"task\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6531d4b2f244951a5d3989f8d33351c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebacef509784365a5c45ab534312146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'coedit_large_processed'],\n",
      "    num_rows: 10\n",
      "})\n",
      "CPU times: user 4min 58s, sys: 516 ms, total: 4min 58s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=10)\n",
    "pprint(samples)\n",
    "\n",
    "process_samples = samples\n",
    "def coedit_large_model_process(item):\n",
    "    # input_ids = coedit_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids = coedit_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids\n",
    "    outputs = coedit_large_model.generate(input_ids, max_length=256)\n",
    "    processed = coedit_large_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"coedit_large_processed\": processed}\n",
    "\n",
    "\n",
    "# process_samples = samples.map(coedit_large_model_process, num_proc=torch.cuda.device_count())\n",
    "process_samples = samples.map(coedit_large_model_process)\n",
    "pprint(process_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da45720055144144b9996d6c5d67c9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2a3aff025144838e985a786599c887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'coedit_large_processed'],\n",
      "    num_rows: 10\n",
      "})\n",
      "CPU times: user 12.4 s, sys: 2.15 s, total: 14.6 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "samples = get_iterater_samples(label=\"fluency\", num_samples=10)\n",
    "pprint(samples)\n",
    "\n",
    "process_samples = samples\n",
    "def coedit_large_model_process(item):\n",
    "    input_ids = coedit_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "    # input_ids = coedit_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids\n",
    "    outputs = coedit_large_model.generate(input_ids, max_length=256)\n",
    "    processed = coedit_large_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"coedit_large_processed\": processed}\n",
    "\n",
    "\n",
    "process_samples = samples.map(coedit_large_model_process, num_proc=torch.cuda.device_count())\n",
    "# process_samples = samples.map(coedit_large_model_process)\n",
    "pprint(process_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function flan_t5_large_model_process at 0x7fd4d1463c40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function flan_t5_large_model_process at 0x7fd4d1463c40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe3177ce6f8457f8b7b47610cf732dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izlobin/miniconda3/envs/pytorch/lib/python3.12/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task', 'source', 'reference', 'references', 'coedit_large_processed', 'flan_t5_large_processed'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "def flan_t5_large_model_process(item):\n",
    "    input_ids = flan_t5_large_tokenizer(item[\"task\"], return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = flan_t5_large_model.generate(input_ids, max_length=256)\n",
    "    processed = flan_t5_large_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"flan_t5_large_processed\": processed}\n",
    "\n",
    "\n",
    "process_samples = process_samples.map(flan_t5_large_model_process, num_proc=torch.cuda.device_count())\n",
    "pprint(process_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.9667188632738873,\n",
      " 'rouge2': 0.946638373091172,\n",
      " 'rougeL': 0.9651674641148326,\n",
      " 'rougeLsum': 0.9657184283021604}\n",
      "{'bp': 1.0,\n",
      " 'counts': [362, 340, 319, 298],\n",
      " 'precisions': [96.53333333333333,\n",
      "                93.15068493150685,\n",
      "                89.85915492957747,\n",
      "                86.3768115942029],\n",
      " 'ref_len': 369,\n",
      " 'score': 91.40200311643846,\n",
      " 'sys_len': 375,\n",
      " 'totals': [375, 365, 355, 345]}\n",
      "{'sari': 57.07432552086183}\n",
      "{'exact_match': 0.0}\n",
      "CPU times: user 66 ms, sys: 0 ns, total: 66 ms\n",
      "Wall time: 66.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "process_samples[:2]\n",
    "\n",
    "rouge_score = rouge_metric.compute(\n",
    "    predictions=process_samples['coedit_large_processed'], references=process_samples['references']\n",
    ")\n",
    "pprint(rouge_score)\n",
    "# rouge_score = rouge_metric.compute(\n",
    "#     predictions=process_samples['flan_t5_large_processed'], references=process_samples['references']\n",
    "# )\n",
    "# pprint(rouge_score)\n",
    "\n",
    "sacreblue_score = sacreblue_metric.compute(predictions=process_samples['coedit_large_processed'], references=process_samples['references'])\n",
    "pprint(sacreblue_score)\n",
    "# sacreblue_score = sacreblue_metric.compute(predictions=process_samples['flan_t5_large_processed'], references=process_samples['references'])\n",
    "# pprint(sacreblue_score)\n",
    "\n",
    "sari_score = sari_metric.compute(\n",
    "  sources=process_samples['source'],\n",
    "  predictions=process_samples['coedit_large_processed'],\n",
    "  references=process_samples['references']\n",
    ")\n",
    "pprint(sari_score)\n",
    "# sari_score = sari_metric.compute(\n",
    "#   sources=process_samples['source'],\n",
    "#   predictions=process_samples['flan_t5_large_processed'],\n",
    "#   references=process_samples['references']\n",
    "# )\n",
    "# pprint(sari_score)\n",
    "\n",
    "score = em_metric.compute(\n",
    "    predictions=process_samples['coedit_large_processed'], references=process_samples['reference']\n",
    ")\n",
    "pprint(score)\n",
    "# score = em_metric.compute(\n",
    "#     predictions=process_samples['flan_t5_large_processed'], references=process_samples['reference']\n",
    "# )\n",
    "# pprint(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
